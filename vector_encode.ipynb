{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd9b7fd-20e7-4aa0-965d-68c8c2e40a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-02c167b4-fd2c-4231-b6f4-d4b05ec6bc70;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 196ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-02c167b4-fd2c-4231-b6f4-d4b05ec6bc70\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 03:05:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/09 03:05:13 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/03/09 03:05:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e63425-7456-49b1-9416-cdedaad017b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:12AM UTC on Mar 08, 2023 --- read and cache time: 0.4459545612335205 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_eps = readCheckpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfb8520-fd5a-42ad-a3a7-204a870db9b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- event_minute: integer (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_eps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f18d26-e723-473e-9745-c2ebdaf8eb2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "+-------+\n",
      "| object|\n",
      "+-------+\n",
      "|  SHELL|\n",
      "|   FLOW|\n",
      "|   FILE|\n",
      "|PROCESS|\n",
      "+-------+\n",
      "\n",
      "+---------+\n",
      "|event_day|\n",
      "+---------+\n",
      "|       23|\n",
      "+---------+\n",
      "\n",
      "+----------+\n",
      "|event_hour|\n",
      "+----------+\n",
      "|        16|\n",
      "|        15|\n",
      "|        17|\n",
      "|        18|\n",
      "|        13|\n",
      "|        14|\n",
      "|        19|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_eps.count())\n",
    "df_eps.select(\"object\").distinct().show()\n",
    "df_eps.select(\"event_day\").distinct().show()\n",
    "df_eps.select(\"event_hour\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073fb40f-444a-409f-ab88-68b61629cfcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a PySxpark DataFrame\n",
    "df = spark.createDataFrame([(0, \"category1\"), (1, \"category2\"), (2, \"category3\"), (3, \"category1\")], [\"id\", \"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff63a587-cfab-45e6-8115-36ff27b13b67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>category1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>category2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>category3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>category1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   category\n",
       "0   0  category1\n",
       "1   1  category2\n",
       "2   2  category3\n",
       "3   3  category1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d82b066-8262-4804-847e-428dade4a4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+\n",
      "| id| category|class_numeric|\n",
      "+---+---------+-------------+\n",
      "|  0|category1|          0.0|\n",
      "|  1|category2|          1.0|\n",
      "|  2|category3|          2.0|\n",
      "|  3|category1|          0.0|\n",
      "+---+---------+-------------+\n",
      "\n",
      "+---+---------+-------------+-------------+\n",
      "| id| category|class_numeric| class_onehot|\n",
      "+---+---------+-------------+-------------+\n",
      "|  0|category1|          0.0|(3,[0],[1.0])|\n",
      "|  1|category2|          1.0|(3,[1],[1.0])|\n",
      "|  2|category3|          2.0|(3,[2],[1.0])|\n",
      "|  3|category1|          0.0|(3,[0],[1.0])|\n",
      "+---+---------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#turn into numeric index before encoding\n",
    "indexer = StringIndexer(inputCol='category', outputCol='class_numeric')\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df_indexed = indexer_fitted.transform(df)\n",
    "df_indexed.show()\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['class_numeric'], outputCols=['class_onehot'],dropLast=False)\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22703a05-edb9-4ff1-b909-b25e5bcbb331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+-------------+---------------+\n",
      "| id| category|class_numeric| class_onehot|     col_onehot|\n",
      "+---+---------+-------------+-------------+---------------+\n",
      "|  0|category1|          0.0|(3,[0],[1.0])|[1.0, 0.0, 0.0]|\n",
      "|  1|category2|          1.0|(3,[1],[1.0])|[0.0, 1.0, 0.0]|\n",
      "|  2|category3|          2.0|(3,[2],[1.0])|[0.0, 0.0, 1.0]|\n",
      "|  3|category1|          0.0|(3,[0],[1.0])|[1.0, 0.0, 0.0]|\n",
      "+---+---------+-------------+-------------+---------------+\n",
      "\n",
      "+---+---------+-------------+-------------+-------------+\n",
      "| id| category|col_onehot[0]|col_onehot[1]|col_onehot[2]|\n",
      "+---+---------+-------------+-------------+-------------+\n",
      "|  0|category1|          1.0|          0.0|          0.0|\n",
      "|  1|category2|          0.0|          1.0|          0.0|\n",
      "|  2|category3|          0.0|          0.0|          1.0|\n",
      "|  3|category1|          1.0|          0.0|          0.0|\n",
      "+---+---------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now turn sparse array into columns\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "df_col_onehot = df_onehot.select('*', vector_to_array('class_onehot').alias('col_onehot'))\n",
    "df_col_onehot.show()\n",
    "\n",
    "#and then unpack vectors into columns\n",
    "num_categories = len(df_col_onehot.first()['col_onehot'])   # 3\n",
    "cols_expanded = [(col('col_onehot')[i]) for i in range(num_categories)]\n",
    "df_cols_onehot = df_col_onehot.select('id', 'category', *cols_expanded)\n",
    "df_cols_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0122624c-7d07-4460-bd8c-5455dd574ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------+---------+\n",
      "| id| category|category1|category2|category3|\n",
      "+---+---------+---------+---------+---------+\n",
      "|  0|category1|      1.0|      0.0|      0.0|\n",
      "|  1|category2|      0.0|      1.0|      0.0|\n",
      "|  2|category3|      0.0|      0.0|      1.0|\n",
      "|  3|category1|      1.0|      0.0|      0.0|\n",
      "+---+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now rename the categories\n",
    "cols_expanded = [col('col_onehot')[i].alias(f'{indexer_fitted.labels[i]}') for i in range(num_categories)]\n",
    "df_cols_onehot = df_col_onehot.select('id', 'category', *cols_expanded)\n",
    "df_cols_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6656ad34-453c-416c-9d43-f3910822dc04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+-------------+--------------------+\n",
      "| id| category|class_numeric| class_onehot|      HashedFeatures|\n",
      "+---+---------+-------------+-------------+--------------------+\n",
      "|  0|category1|          0.0|(3,[0],[1.0])|[[1.0], [-4.0], [...|\n",
      "|  1|category2|          1.0|(3,[1],[1.0])|[[8.0], [-7.0], [...|\n",
      "|  2|category3|          2.0|(3,[2],[1.0])|[[4.0], [6.0], [-...|\n",
      "|  3|category1|          0.0|(3,[0],[1.0])|[[1.0], [-4.0], [...|\n",
      "+---+---------+-------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH, VectorAssembler\n",
    "\n",
    "#now create an index based on the sparse vector\n",
    "# create a BucketedRandomProjectionLSH instance to hash the sparse vector column\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"class_onehot\", outputCol=\"HashedFeatures\", numHashTables=3, bucketLength=0.1)\n",
    "# fit the transformer on the DataFrame and transform it\n",
    "brp_model = brp.fit(df_onehot)\n",
    "df_hashed = brp_model.transform(df_onehot)\n",
    "# show the resulting DataFrame\n",
    "df_hashed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d002a09f-570a-4fc4-8058-1dc917148f6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column HashedFeatures must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m vi \u001b[38;5;241m=\u001b[39m VectorIndexer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHashedFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexedFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxCategories\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# transform the DataFrame using the VectorIndexer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df_indexed \u001b[38;5;241m=\u001b[39m \u001b[43mvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_hashed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df_hashed)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# show the resulting DataFrame\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df_indexed\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column HashedFeatures must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# create a VectorIndexer instance to assign integers to the unique hashed values\n",
    "vi = VectorIndexer(inputCol=\"HashedFeatures\", outputCol=\"IndexedFeatures\", maxCategories=3)\n",
    "\n",
    "# transform the DataFrame using the VectorIndexer\n",
    "df_indexed = vi.fit(df_hashed).transform(df_hashed)\n",
    "\n",
    "# show the resulting DataFrame\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bfcaab9-a2a4-4811-a646-1f7519f6d46f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: The feature is not supported: literal for '[r, g, r, r]' of class java.util.ArrayList.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:283)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:125)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_hashed \u001b[38;5;241m=\u001b[39m df_hashed\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/functions.py:137\u001b[0m, in \u001b[0;36mlit\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlit\u001b[39m(col: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Creates a :class:`~pyspark.sql.Column` of literal value.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    [Row(height=5, spark_user=True)]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m col \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/functions.py:85\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     84\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: The feature is not supported: literal for '[r, g, r, r]' of class java.util.ArrayList.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:283)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:125)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df_hashed = df_hashed.withColumn('color',lit(['r','g','r','r']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3520d2e6-691c-49e8-9087-81915fe5da62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- second_color: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- second_color: string (nullable = true)\n",
      " |-- sha2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# create sample data\n",
    "data = [(0, \"red\",'g'), (1, \"green\",'blue'), (2, \"red\",'g')]\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data, [\"id\", \"color\",'second_color'])\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "#now create a hash\n",
    "df = df.withColumn(\"sha2\",sha2(concat_ws('||',col('color'),col('second_color')),256))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d3e4efd-cdb9-43f7-ad1c-4af8c7e51348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------+--------------------+-------------+\n",
      "| id|color|second_color|                sha2|class_numeric|\n",
      "+---+-----+------------+--------------------+-------------+\n",
      "|  0|  red|           g|1edcfa2380771ecfb...|          0.0|\n",
      "|  1|green|        blue|50e3429807908a500...|          1.0|\n",
      "|  2|  red|           g|1edcfa2380771ecfb...|          0.0|\n",
      "+---+-----+------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#turn into numeric index before encoding\n",
    "indexer = StringIndexer(inputCol='sha2', outputCol='class_numeric')\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df_indexed = indexer_fitted.transform(df)\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f190b96-cc5a-42c5-bd1f-44115cfa4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create binary columns and then drop categorical column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
