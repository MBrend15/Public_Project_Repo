{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd9b7fd-20e7-4aa0-965d-68c8c2e40a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e68b2573-b5f6-4b7a-84f0-e3641f093703;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.2-spark3.2-s_2.12!graphframes.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (33ms)\n",
      ":: resolution report :: resolve 698ms :: artifacts dl 111ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e68b2573-b5f6-4b7a-84f0-e3641f093703\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (281kB/8ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 03:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 03:00:34 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c07d1ab-383e-4b1a-8108-e11e9ce33a68",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:06AM UTC on Mar 08, 2023 --- read and cache time: 1.71661376953125e-05 seconds ---\n",
      "root\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run build_adjacency.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b1283b-347b-4556-aa01-0a509ff6fcb6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_labels.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360a133e-c035-402f-94ea-6a551e14e08b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:08AM UTC on Mar 08, 2023 --- read and cache time: 33.56131649017334 seconds ---\n",
      "000401b2-4413-4f26-9943-75877fd855df:3b7351a9-abe1-4e29-a7dc-18c0aafdb68c\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "test_dict = build_adjaceny(df_labels, 'labels')\n",
    "\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- \" % (time.time() - start_time))\n",
    "\n",
    "print(test_dict['d5846376-9596-4283-8505-8e6112abf1f2:e15537fe-fdb0-49d4-a1fd-de34e8215617'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e63425-7456-49b1-9416-cdedaad017b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:12AM UTC on Mar 08, 2023 --- read and cache time: 0.4459545612335205 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_eps = readCheckpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfb8520-fd5a-42ad-a3a7-204a870db9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- event_minute: integer (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_eps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f18d26-e723-473e-9745-c2ebdaf8eb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "+-------+\n",
      "| object|\n",
      "+-------+\n",
      "|  SHELL|\n",
      "|   FLOW|\n",
      "|   FILE|\n",
      "|PROCESS|\n",
      "+-------+\n",
      "\n",
      "+---------+\n",
      "|event_day|\n",
      "+---------+\n",
      "|       23|\n",
      "+---------+\n",
      "\n",
      "+----------+\n",
      "|event_hour|\n",
      "+----------+\n",
      "|        16|\n",
      "|        15|\n",
      "|        17|\n",
      "|        18|\n",
      "|        13|\n",
      "|        14|\n",
      "|        19|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_eps.count())\n",
    "df_eps.select(\"object\").distinct().show()\n",
    "df_eps.select(\"event_day\").distinct().show()\n",
    "df_eps.select(\"event_hour\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "073fb40f-444a-409f-ab88-68b61629cfcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame([(0, \"category1\"), (1, \"category2\"), (2, \"category3\"), (3, \"category1\")], [\"id\", \"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff63a587-cfab-45e6-8115-36ff27b13b67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>category1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>category2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>category3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>category1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   category\n",
       "0   0  category1\n",
       "1   1  category2\n",
       "2   2  category3\n",
       "3   3  category1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3520d2e6-691c-49e8-9087-81915fe5da62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------------+\n",
      "| id|color|       id_vec|\n",
      "+---+-----+-------------+\n",
      "|  0|  red|(2,[0],[1.0])|\n",
      "|  1|green|(2,[1],[1.0])|\n",
      "|  2| blue|    (2,[],[])|\n",
      "+---+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# create sample data\n",
    "data = [(0, \"red\"), (1, \"green\"), (2, \"blue\")]\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data, [\"id\", \"color\"])\n",
    "\n",
    "# create OneHotEncoder object\n",
    "encoder = OneHotEncoder(inputCols=[\"id\"], outputCols=[\"id_vec\"])\n",
    "\n",
    "# fit and transform dataframe\n",
    "df_encoded = encoder.fit(df).transform(df)\n",
    "\n",
    "# show the encoded dataframe\n",
    "df_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f190b96-cc5a-42c5-bd1f-44115cfa4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create binary columns and then drop categorical column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
