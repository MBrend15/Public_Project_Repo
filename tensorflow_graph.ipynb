{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509d01c3-ad52-45ec-8c7b-865fccc42229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/15 07:19:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/15 07:19:45 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %run ./etl_trusted_features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42f0df4-2e40-4ba1-b377-67488d80a85f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.0 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\n",
      "multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow --quiet\n",
    "%pip install tensorflow_gnn --quiet\n",
    "%pip install tensorflow_io --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3aecd8-3df6-4dfd-b6ac-bf81e2ede490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 07:21:19.976067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow import keras\n",
    "import tensorflow_gnn as tfgnn\n",
    "from tensorflow_gnn import runner\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb861b-fe2e-42a1-a3c3-0191049e77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_tensor(df):\n",
    "    graph_tensor = tfgnn.GraphTensor.from_pieces(\n",
    "    context_spec = tfgnn.ContextSpec.from_field_specs(\n",
    "        features_spec ={\n",
    "            \"malicious\": np.array(df['malicious'],\n",
    "                                dtype='string').reshape(len(df),1)\n",
    "        }),\n",
    "    node_sets = {\n",
    "        \"actorID\": tfgnn.NodeSet.from_fields(\n",
    "            sizes = [len(df)],\n",
    "            features ={\n",
    "                'parent_image_path': np.array(df['parent_image_path'],\n",
    "                                dtype='string').reshape(len(df),1)\n",
    "            }),\n",
    "        \"objectID\": tfgnn.NodeSet.from_fields(\n",
    "            sizes = [len(df)],\n",
    "            features ={\n",
    "                'base_address': np.array(df['base_address'],\n",
    "                                 dtype='string').reshape(len(df),1),\n",
    "                'file_path': np.array(df['file_path'],\n",
    "                                   dtype='string').reshape(len(df),1),\n",
    "                'image_path': np.array(df['image_path'],\n",
    "                                   dtype='string').reshape(len(df),1),\n",
    "            })\n",
    "    },\n",
    "    edge_sets ={\n",
    "        \"id\": tfgnn.EdgeSet.from_fields(\n",
    "            sizes = [len(df)],\n",
    "            features = {\n",
    "                'id': np.array(df['id'],\n",
    "                                      dtype='string').reshape(len(df),1)},\n",
    "            adjacency = tfgnn.Adjacency.from_indices(\n",
    "                source = (\"actorID\", np.array(df['actorID'], dtype='string')),\n",
    "                target = (\"objectID\", np.array(df['objectID'], dtype='string')))),\n",
    "  })\n",
    "\n",
    "    return graph_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423cfb7a-c798-4ce5-99db-f8732847b8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_schema = tfgnn.read_schema(\"./graph_schema.pbtxt\")\n",
    "gtspec = tfgnn.create_graph_spec_from_schema_pb(graph_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4911158e-1c45-49a9-a822-59439d33611c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphTensorSpec({'context': ContextSpec({'features': {'malicious': RaggedTensorSpec(TensorShape([1, None]), tf.string, 1, tf.int32)}, 'sizes': TensorSpec(shape=(1,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, None), 'node_sets': {'objectID': NodeSetSpec({'features': {'file_path': RaggedTensorSpec(TensorShape([None, None]), tf.string, 1, tf.int32), 'base_address': RaggedTensorSpec(TensorShape([None, None]), tf.string, 1, tf.int32), 'image_path': RaggedTensorSpec(TensorShape([None, None]), tf.string, 1, tf.int32)}, 'sizes': TensorSpec(shape=(1,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, None), 'actorID': NodeSetSpec({'features': {'parent_image_path': RaggedTensorSpec(TensorShape([None, None]), tf.string, 1, tf.int32)}, 'sizes': TensorSpec(shape=(1,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, None)}, 'edge_sets': {'id': EdgeSetSpec({'features': {'event_minute': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'event_hour': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'event_day': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, 'sizes': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'adjacency': AdjacencySpec({'#index.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '#index.1': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, {'#index.0': 'actorID', '#index.1': 'objectID'})}, TensorShape([]), tf.int32, None)}}, TensorShape([]), tf.int32, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6f430d1-f9e9-436b-b0c9-06a21cc82c17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 07:39:38.920187: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-03-15 07:39:38.920236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-16-10-147.us-west-2.compute.internal): /proc/driver/nvidia/version does not exist\n",
      "2023-03-15 07:39:38.921158: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "gt = tfgnn.random_graph_tensor(gtspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884dbf2-3dcb-413d-acb1-6a170fd40458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0180519-a136-4283-84f6-8b960081fd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cb85d-d704-4752-93d6-838863eb1b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d598878b-28df-48f0-96e8-17193bc877a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_provider = runner.TFRecordDatasetProvider(file_pattern=\"s3://sapient-bucket-trusted/prod/graph/first_events/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee5c54d-29ed-486c-a6ef-87f7fef7969a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_ds_provider = runner.TFRecordDatasetProvider(file_pattern=\"s3://sapient-bucket-trusted/prod/graph/first_events/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942989a2-0afb-4154-86e7-2ba5010b79c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b86c1a-6604-4182-a151-566e11914ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m graph_tensor_spec \u001b[38;5;241m=\u001b[39m tfgnn\u001b[38;5;241m.\u001b[39mcreate_graph_spec_from_schema_pb(graph_schema)\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m serialized: tfgnn\u001b[38;5;241m.\u001b[39mparse_single_example(graph_tensor_spec, serialized))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, graph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m3\u001b[39m)):\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "graph_tensor_spec = tfgnn.create_graph_spec_from_schema_pb(graph_schema)\n",
    "dataset = dataset.map(\n",
    "    lambda serialized: tfgnn.parse_single_example(graph_tensor_spec, serialized))\n",
    "\n",
    "for i, graph in enumerate(dataset.take(3)):\n",
    "  print(f\"Input {i}: {graph}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a004c8f0-5c1c-4463-b54c-927aab7493e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getFirstEventFiles():\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket = 'sapient-bucket-trusted',\n",
    "        Prefix = f'prod/')\n",
    "    all_files = []\n",
    "    for content in response.get('Contents', []):\n",
    "        all_files.append(content['Key'])\n",
    "        # print(content['Key'])\n",
    "    files = [f\"sapient-bucket-trusted/\" + f for f in all_files if 'parquet' in f]\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fcec98b-a836-4111-89a7-11e8444059b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_event_files = getFirstEventFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e12534-0bcf-4ca7-ba02-769bd75158d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_fn(file_location):\n",
    "    return tfio.IODataset.from_parquet(file_location, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f44b4a7-744b-451d-b1bb-1e9f4d8b3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [ s3bucket + \"/path/file_{}.parquet\".format(i) for i in range(train_size) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5947801-4aea-47ad-a83b-1622e476bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 640\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbefa1-852f-42c8-949b-453f0912e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785d219-a663-42d6-9384-5d930aded069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf2f89-68a9-4a57-a925-f375bf6424de",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [ s3bucket + \"/path/file_{}.parquet\".format(i) for i in range(5) ]\n",
    "ds = tf.data.Dataset.from_tensor_slices(files).map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2dfffb-736e-4be7-851d-3105e9d56186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf26b43-e2db-4852-8281-7963110dbcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0fd74-44fc-44df-8906-c2afbb9123a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
