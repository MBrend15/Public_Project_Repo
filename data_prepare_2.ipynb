{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861e74cd-997b-4f7f-9503-8d31085f4726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5230c441-0590-4bc1-8e9c-ba4aad13b691;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.2-spark3.2-s_2.12!graphframes.jar (41ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (23ms)\n",
      ":: resolution report :: resolve 668ms :: artifacts dl 68ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5230c441-0590-4bc1-8e9c-ba4aad13b691\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (281kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 01:41:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 01:41:27 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0ddd8d-73f2-4471-8669-fdaeee193da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748a82bc-e9f2-4db1-9edd-849f73909ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:17AM UTC on Mar 30, 2023 --- read time: 4.651973247528076 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_all = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f2f125-b5cf-40c5-9f6b-129221793d85",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- event_minute: integer (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- image_path_ext: string (nullable = true)\n",
      " |-- parent_path_ext: string (nullable = true)\n",
      " |-- new_path_ext: string (nullable = true)\n",
      " |-- file_path_ext: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d8e5c7-2199-48f6-b720-8b21e0527280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot = df_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d78aaadd-a938-48d8-bc5a-d380920b4813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "|event_day|malicious|  count|\n",
      "+---------+---------+-------+\n",
      "|       25|        1|    343|\n",
      "|       23|        0|1206627|\n",
      "|       24|        1|   4362|\n",
      "|       23|        1|   3307|\n",
      "|       24|        0|1503638|\n",
      "|       25|        0|1038587|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check days and malicious counts\n",
    "df_all.groupBy(\"event_day\", \"malicious\") \\\n",
    "              .count().cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9adf3a0c-4831-427d-8232-5d6a9205a6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = df_all.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76baa3-d6a6-48ee-9e45-21140f872a37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------+\n",
      "|image_path_ext|parent_path_ext|  count|\n",
      "+--------------+---------------+-------+\n",
      "|              |               |  27514|\n",
      "|              |           .exe|  12340|\n",
      "|          null|           null| 334817|\n",
      "|          .exe|           .exe| 899444|\n",
      "|          .exe|           null|1875351|\n",
      "|              |           null| 596814|\n",
      "|          .exe|               |  10580|\n",
      "|          .com|           .exe|      1|\n",
      "|          .com|           null|      3|\n",
      "+--------------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how many image paths are there\n",
    "df_all.groupBy('image_path_ext','parent_path_ext').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "608c0926-20bc-4a46-8bc9-e06a3ee5cd23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_img = ['None','empty','.exe','.com']\n",
    "list_par_img = ['None','empty','.exe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba90e32d-879a-422e-9872-a758b0b32887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many new paths are there\n",
    "df_all.select('new_path_ext').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5da573d-cba5-42c6-ae3c-4053d90357ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|new_path_ext|  count|\n",
      "+------------+-------+\n",
      "|        .log|    954|\n",
      "|        null|3734496|\n",
      "|        .dat|    500|\n",
      "|        .etl|  19061|\n",
      "|        .bak|    197|\n",
      "|        .tmp|    343|\n",
      "|    .jsonlz4|    156|\n",
      "|        .odl|    134|\n",
      "|            |    565|\n",
      "|        .cvr|    241|\n",
      "|        .jtx|     15|\n",
      "|     .mozlz4|     16|\n",
      "|         .js|     19|\n",
      "|       .json|     72|\n",
      "|        .yml|     65|\n",
      "|       .pptx|      2|\n",
      "|        .dll|      4|\n",
      "|        .bin|      8|\n",
      "|        .zip|      1|\n",
      "|        .ini|      3|\n",
      "+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#what are the new path counts\n",
    "df_all.groupBy('new_path_ext').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3327b1df-7e22-4de3-a5da-20af704d2748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many file paths are there\n",
    "df_all.select('file_path_ext').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541406b7-8675-4586-aa57-34866aee06c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#how many file paths are there\n",
    "ext_ratios = df_all.groupBy('file_path_ext').count().sort(col('count').desc()).withColumn('perc_of_count_total', (col('count') / tot) * 100).limit(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3e2dd4f-7fac-4ecb-975a-85b7cd8a928d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:==========================================>          (162 + 16) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-------------------+\n",
      "|file_path_ext|  count|perc_of_count_total|\n",
      "+-------------+-------+-------------------+\n",
      "|         null|1294015|  34.44402033185124|\n",
      "|             |1093356| 29.102890070015842|\n",
      "|         .dll| 421606| 11.222285395478783|\n",
      "|         .pyc| 271542|  7.227890070015842|\n",
      "|         .bat|  48676| 1.2956550995724092|\n",
      "|          .pf|  46930| 1.2491801672884617|\n",
      "|         .ini|  41568| 1.1064547452343232|\n",
      "|         .exe|  37894| 1.0086604146437028|\n",
      "|         .tmp|  35275| 0.9389480162177817|\n",
      "|         .cat|  33596| 0.8942564862608814|\n",
      "|         .dat|  29786|  0.792842115125807|\n",
      "|         .etl|  28047| 0.7465535084581183|\n",
      "|         .xml|  26723| 0.7113113490400504|\n",
      "|         .log|  25180| 0.6702398596276044|\n",
      "|         .mum|  23850| 0.6348379925384575|\n",
      "|        .evtx|  22010|  0.585860973407608|\n",
      "|     .sbstore|  18984| 0.5053150712935044|\n",
      "|        .pset|  18841| 0.5015087051327917|\n",
      "|         .egg|  14665| 0.3903521660619069|\n",
      "|         .mui|  13433| 0.3575588576003816|\n",
      "+-------------+-------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ext_ratios.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c921200-e1e3-47fc-afa6-334464879462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#create a list of top 20 file extensions\n",
    "file_ext = list(df_all.groupBy('file_path_ext').count().sort(col('count').desc()).limit(9).toPandas()['file_path_ext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2288f8-1c25-4d26-ac22-ee6b9b75ceb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23 = df_all.where(col('event_day') == 23).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52034829-befb-4a5f-88aa-78228ed7acb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_1516 = df_23.where((col('event_hour') == 16) | (col('event_hour') == 15)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d967636-a44c-422a-91f0-72f176639672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "329305"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_23_1516.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35450811-3973-440c-ae78-fad4b6ad93b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_15 = df_23.where((col('event_hour') == 15)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365d51a3-d80c-428c-8307-1adf0651c361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "168873"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_23_15.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e73835f-1078-4394-8dee-d6e6c32a9bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.1743764877319336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 245:==========================================>         (163 + 16) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 358853\n",
      "create graph: 13.880214214324951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#now create graph for event traces of 2 events, 4 events, and 6 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c)\") #; (c)-[e3]->(d); (d)-[e4]->(e); (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_2 = motifs6.filter('''e1.timestamp <= e2.timestamp''').cache()# and e2.timestamp <= e3.timestamp and \n",
    "                                    #e3.timestamp <= e4.timestamp and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_2.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "530cc343-e705-4340-a80b-c9132a3bf1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.4217662811279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 257923798\n",
      "create graph: 1037.19176030159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#now create graph for event traces of 2 events, 3 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('actorID as src', 'objectID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_3 = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                    #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_3.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dcdb176-39c4-45a1-ae4a-d44e7765492e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.24465680122375488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 1108974\n",
      "create graph: 12.938036918640137\n"
     ]
    }
   ],
   "source": [
    "#now create graph for event traces of 2 events, 3 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('actorID as src', 'objectID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_3 = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                    #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_3.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c907eb93-5169-4d18-af78-e21fc879a5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.21571063995361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 466933\n",
      "create graph: 12.180915594100952\n"
     ]
    }
   ],
   "source": [
    "#now create graph for event traces of 2 events, 3 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_3oa = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                    #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_3oa.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5d1245a6-7832-429b-8e6e-c60daf902d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transp = cp_2315_3oa.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "df_transp = df_transp.select(\"Trace\", \n",
    "                             *[col for col in df_transp.columns if col != \"Trace\"])\n",
    "\n",
    "#drop all vertices\n",
    "df_transp = df_transp.drop('a','b','c','d')\n",
    "\n",
    "#transpose rows \n",
    "stacked_df = df_transp.selectExpr(\n",
    "    \"Trace\", \n",
    "    \"posexplode(array(e1, e2, e3)) as (pos, col)\"\n",
    ").select(\n",
    "    \"Trace\", \n",
    "    expr('''CASE pos \n",
    "    WHEN 0 THEN 'e1' \n",
    "    WHEN 1 THEN 'e2'\n",
    "    ELSE 'e3' END''').alias(\"event\"),\n",
    "    \"col\"\n",
    ").orderBy(\"Trace\",\"event\")\n",
    "\n",
    "#explode columns\n",
    "df_onehot_oa = stacked_df.select(*stacked_df.columns, \"col.*\").drop('col').cache()\n",
    "\n",
    "df_transp = cp_2315_3.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "df_transp = df_transp.select(\"Trace\", \n",
    "                             *[col for col in df_transp.columns if col != \"Trace\"])\n",
    "\n",
    "#drop all vertices\n",
    "df_transp = df_transp.drop('a','b','c','d')\n",
    "\n",
    "#transpose rows \n",
    "stacked_df = df_transp.selectExpr(\n",
    "    \"Trace\", \n",
    "    \"posexplode(array(e1, e2, e3)) as (pos, col)\"\n",
    ").select(\n",
    "    \"Trace\", \n",
    "    expr('''CASE pos \n",
    "    WHEN 0 THEN 'e1' \n",
    "    WHEN 1 THEN 'e2'\n",
    "    ELSE 'e3' END''').alias(\"event\"),\n",
    "    \"col\"\n",
    ").orderBy(\"Trace\",\"event\")\n",
    "\n",
    "#explode columns\n",
    "df_onehot = stacked_df.select(*stacked_df.columns, \"col.*\").drop('col').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "55d6237c-24e1-477c-b86a-c8d4b7cd7b43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>object</th>\n",
       "      <th>action</th>\n",
       "      <th>hostname</th>\n",
       "      <th>user_name</th>\n",
       "      <th>privileges</th>\n",
       "      <th>image_path</th>\n",
       "      <th>parent_image_path</th>\n",
       "      <th>new_path</th>\n",
       "      <th>file_path</th>\n",
       "      <th>direction</th>\n",
       "      <th>logon_id</th>\n",
       "      <th>requesting_domain</th>\n",
       "      <th>requesting_user</th>\n",
       "      <th>malicious</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e1</td>\n",
       "      <td>2b2a9a10-bcc0-413e-b6f1-8237c4579166</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>2019-09-23 15:04:40.021</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\SysWOW64\\adsld...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>e2</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>2019-09-23 15:09:54.088</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>e3</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>43a5e405-a143-4f3a-8954-0df7cc9abd87</td>\n",
       "      <td>2019-09-23 15:10:09.743</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>%SystemRoot%\\system32\\csrss.exe</td>\n",
       "      <td>%SystemRoot%\\system32\\csrss.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>e1</td>\n",
       "      <td>2b2a9a10-bcc0-413e-b6f1-8237c4579166</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>2019-09-23 15:04:40.021</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\SysWOW64\\adsld...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>e2</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>2019-09-23 15:09:54.088</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>e3</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>a059d617-993c-4772-8a96-803a2fc0eef2</td>\n",
       "      <td>2019-09-23 15:10:09.721</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trace event                                   src  \\\n",
       "0      1    e1  2b2a9a10-bcc0-413e-b6f1-8237c4579166   \n",
       "1      1    e2  9274fa54-92f9-4b36-84c8-4b103c3e931a   \n",
       "2      1    e3  97fc76f2-2e7c-4c21-8a82-ae191c19a33f   \n",
       "3      2    e1  2b2a9a10-bcc0-413e-b6f1-8237c4579166   \n",
       "4      2    e2  9274fa54-92f9-4b36-84c8-4b103c3e931a   \n",
       "5      2    e3  97fc76f2-2e7c-4c21-8a82-ae191c19a33f   \n",
       "\n",
       "                                    dst               timestamp   object  \\\n",
       "0  9274fa54-92f9-4b36-84c8-4b103c3e931a 2019-09-23 15:04:40.021     FILE   \n",
       "1  97fc76f2-2e7c-4c21-8a82-ae191c19a33f 2019-09-23 15:09:54.088  PROCESS   \n",
       "2  43a5e405-a143-4f3a-8954-0df7cc9abd87 2019-09-23 15:10:09.743  PROCESS   \n",
       "3  9274fa54-92f9-4b36-84c8-4b103c3e931a 2019-09-23 15:04:40.021     FILE   \n",
       "4  97fc76f2-2e7c-4c21-8a82-ae191c19a33f 2019-09-23 15:09:54.088  PROCESS   \n",
       "5  a059d617-993c-4772-8a96-803a2fc0eef2 2019-09-23 15:10:09.721  PROCESS   \n",
       "\n",
       "  action                    hostname user_name privileges  \\\n",
       "0   READ  SysClient0321.systemia.com      None       None   \n",
       "1   OPEN  SysClient0321.systemia.com      None       None   \n",
       "2   OPEN  SysClient0321.systemia.com      None       None   \n",
       "3   READ  SysClient0321.systemia.com      None       None   \n",
       "4   OPEN  SysClient0321.systemia.com      None       None   \n",
       "5   OPEN  SysClient0321.systemia.com      None       None   \n",
       "\n",
       "                                          image_path  \\\n",
       "0        \\Device\\HarddiskVolume1\\Python27\\python.exe   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...   \n",
       "2                    %SystemRoot%\\system32\\csrss.exe   \n",
       "3        \\Device\\HarddiskVolume1\\Python27\\python.exe   \n",
       "4  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...   \n",
       "5  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...   \n",
       "\n",
       "                                   parent_image_path new_path  \\\n",
       "0                                               None     None   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...     None   \n",
       "2                    %SystemRoot%\\system32\\csrss.exe     None   \n",
       "3                                               None     None   \n",
       "4  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...     None   \n",
       "5  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...     None   \n",
       "\n",
       "                                           file_path direction logon_id  \\\n",
       "0  \\Device\\HarddiskVolume1\\Windows\\SysWOW64\\adsld...      None     None   \n",
       "1                                               None      None     None   \n",
       "2                                               None      None     None   \n",
       "3  \\Device\\HarddiskVolume1\\Windows\\SysWOW64\\adsld...      None     None   \n",
       "4                                               None      None     None   \n",
       "5                                               None      None     None   \n",
       "\n",
       "  requesting_domain requesting_user  malicious  \n",
       "0              None            None          0  \n",
       "1              None            None          0  \n",
       "2              None            None          0  \n",
       "3              None            None          0  \n",
       "4              None            None          0  \n",
       "5              None            None          0  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot_oa.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c35f5cc-f954-40e5-9688-db72ec3681dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>object</th>\n",
       "      <th>action</th>\n",
       "      <th>hostname</th>\n",
       "      <th>user_name</th>\n",
       "      <th>privileges</th>\n",
       "      <th>image_path</th>\n",
       "      <th>parent_image_path</th>\n",
       "      <th>new_path</th>\n",
       "      <th>file_path</th>\n",
       "      <th>direction</th>\n",
       "      <th>logon_id</th>\n",
       "      <th>requesting_domain</th>\n",
       "      <th>requesting_user</th>\n",
       "      <th>malicious</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e1</td>\n",
       "      <td>addd4dd5-e6b8-471e-94b7-c391ea02da0c</td>\n",
       "      <td>27c3c561-4929-4241-97c2-e7fd688a7951</td>\n",
       "      <td>2019-09-23 15:17:56.543</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>CREATE</td>\n",
       "      <td>SysClient0358.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>e2</td>\n",
       "      <td>27c3c561-4929-4241-97c2-e7fd688a7951</td>\n",
       "      <td>06ca5760-85df-4718-b2c3-77f76c3a3e2d</td>\n",
       "      <td>2019-09-23 15:17:56.573</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>CREATE</td>\n",
       "      <td>SysClient0358.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>taskkill.exe</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>e3</td>\n",
       "      <td>06ca5760-85df-4718-b2c3-77f76c3a3e2d</td>\n",
       "      <td>52fa575b-8385-4ded-b607-93b404eb367d</td>\n",
       "      <td>2019-09-23 15:17:56.692</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0358.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>taskkill.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\SysWOW64\\Winst...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>e1</td>\n",
       "      <td>addd4dd5-e6b8-471e-94b7-c391ea02da0c</td>\n",
       "      <td>27c3c561-4929-4241-97c2-e7fd688a7951</td>\n",
       "      <td>2019-09-23 15:17:56.543</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>CREATE</td>\n",
       "      <td>SysClient0358.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>e2</td>\n",
       "      <td>27c3c561-4929-4241-97c2-e7fd688a7951</td>\n",
       "      <td>06ca5760-85df-4718-b2c3-77f76c3a3e2d</td>\n",
       "      <td>2019-09-23 15:17:56.573</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>CREATE</td>\n",
       "      <td>SysClient0358.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>taskkill.exe</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>e3</td>\n",
       "      <td>06ca5760-85df-4718-b2c3-77f76c3a3e2d</td>\n",
       "      <td>976fa881-0178-4a57-b129-c97d5bec03b8</td>\n",
       "      <td>2019-09-23 15:17:56.652</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0358.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>taskkill.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\SysWOW64\\netut...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trace event                                   src  \\\n",
       "0      1    e1  addd4dd5-e6b8-471e-94b7-c391ea02da0c   \n",
       "1      1    e2  27c3c561-4929-4241-97c2-e7fd688a7951   \n",
       "2      1    e3  06ca5760-85df-4718-b2c3-77f76c3a3e2d   \n",
       "3      2    e1  addd4dd5-e6b8-471e-94b7-c391ea02da0c   \n",
       "4      2    e2  27c3c561-4929-4241-97c2-e7fd688a7951   \n",
       "5      2    e3  06ca5760-85df-4718-b2c3-77f76c3a3e2d   \n",
       "\n",
       "                                    dst               timestamp   object  \\\n",
       "0  27c3c561-4929-4241-97c2-e7fd688a7951 2019-09-23 15:17:56.543  PROCESS   \n",
       "1  06ca5760-85df-4718-b2c3-77f76c3a3e2d 2019-09-23 15:17:56.573  PROCESS   \n",
       "2  52fa575b-8385-4ded-b607-93b404eb367d 2019-09-23 15:17:56.692     FILE   \n",
       "3  27c3c561-4929-4241-97c2-e7fd688a7951 2019-09-23 15:17:56.543  PROCESS   \n",
       "4  06ca5760-85df-4718-b2c3-77f76c3a3e2d 2019-09-23 15:17:56.573  PROCESS   \n",
       "5  976fa881-0178-4a57-b129-c97d5bec03b8 2019-09-23 15:17:56.652     FILE   \n",
       "\n",
       "   action                    hostname user_name privileges  \\\n",
       "0  CREATE  SysClient0358.systemia.com      None       None   \n",
       "1  CREATE  SysClient0358.systemia.com      None       None   \n",
       "2    READ  SysClient0358.systemia.com      None       None   \n",
       "3  CREATE  SysClient0358.systemia.com      None       None   \n",
       "4  CREATE  SysClient0358.systemia.com      None       None   \n",
       "5    READ  SysClient0358.systemia.com      None       None   \n",
       "\n",
       "                                         image_path  \\\n",
       "0  \\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe   \n",
       "1                                      taskkill.exe   \n",
       "2                                      taskkill.exe   \n",
       "3  \\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe   \n",
       "4                                      taskkill.exe   \n",
       "5                                      taskkill.exe   \n",
       "\n",
       "                                  parent_image_path new_path  \\\n",
       "0       \\Device\\HarddiskVolume1\\Python27\\python.exe     None   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe     None   \n",
       "2                                              None     None   \n",
       "3       \\Device\\HarddiskVolume1\\Python27\\python.exe     None   \n",
       "4  \\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe     None   \n",
       "5                                              None     None   \n",
       "\n",
       "                                           file_path direction logon_id  \\\n",
       "0                                               None      None     None   \n",
       "1                                               None      None     None   \n",
       "2  \\Device\\HarddiskVolume1\\Windows\\SysWOW64\\Winst...      None     None   \n",
       "3                                               None      None     None   \n",
       "4                                               None      None     None   \n",
       "5  \\Device\\HarddiskVolume1\\Windows\\SysWOW64\\netut...      None     None   \n",
       "\n",
       "  requesting_domain requesting_user  malicious  \n",
       "0              None            None          0  \n",
       "1              None            None          0  \n",
       "2              None            None          0  \n",
       "3              None            None          0  \n",
       "4              None            None          0  \n",
       "5              None            None          0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673cc76-b71a-4e44-8d10-b19aa1ef30d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39d846f5-2544-4c91-ac00-272e1c17353f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#write both samples to parquet\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "cp_2315_2.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/test/2315_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ae9b087-4ee7-40fa-bcd2-34d4a1c0bc4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.             (18 + 16) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "[Stage 95:=====>                                                (19 + 16) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#write both samples to parquet\u001b[39;00m\n\u001b[1;32m      2\u001b[0m s3_url_trusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://sapient-bucket-trusted/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcp_2315_4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxRecordsPerFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms3_url_trusted\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/prod/graph/test/2315_4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#write both samples to parquet\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "cp_2315_4.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/test/2315_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59d1cc-2d23-4757-b951-2f627a261f30",
   "metadata": {},
   "source": [
    "## So now let's work on filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c1ce59a6-2fe3-4c1d-8e32-4272a1d1977e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the necessary file extension filtering\n",
    "df = df_onehot_oa.withColumn(\"image_path_ext\", regexp_extract(\"image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"parent_path_ext\", regexp_extract(\"parent_image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"new_path_ext\", regexp_extract(\"new_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"file_path_ext\", regexp_extract(\"file_path\", \"\\.[0-9a-z]+$\", 0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c5111bb-2e74-40b8-ba07-23c5ada1e951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bdd47db-fd13-43a2-8200-d8012f4b8088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-------------------+\n",
      "|file_path_ext|  count|perc_of_count_total|\n",
      "+-------------+-------+-------------------+\n",
      "|         null|1017821|   72.6600318818046|\n",
      "|             | 151294| 10.800550257388819|\n",
      "|         .dll| 136872|  9.770994982149475|\n",
      "|         .pyc|  20031| 1.4299696102010353|\n",
      "|         .cat|  13868| 0.9900064177658608|\n",
      "|         .ini|   9862| 0.7040267732915286|\n",
      "|          .pf|   4175| 0.2980441876386263|\n",
      "|         .dat|   3756| 0.2681326871307018|\n",
      "|         .tmp|   3466| 0.2474302166120907|\n",
      "|         .exe|   2504|0.17875512475380123|\n",
      "|         .egg|   2403|0.17154495398697456|\n",
      "|     .sbstore|   2234| 0.1594804108226805|\n",
      "|        .pset|   2210|0.15776710291769197|\n",
      "|         .mui|   2117| 0.1511280347858615|\n",
      "|         .txt|   1769| 0.1262850701635281|\n",
      "|          .js|   1759|0.12557119186978288|\n",
      "|      .sqlite|   1584|0.11307832172924168|\n",
      "|         .etl|   1529|  0.109151991113643|\n",
      "|          .db|   1496|0.10679619274428379|\n",
      "|         .log|   1452|0.10365512825180485|\n",
      "+-------------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('file_path_ext').count().sort(col('count').desc()).withColumn('perc_of_count_total', (col('count') / tot) * 100).limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cd6748d-54a7-48ea-8ff0-3015f84759b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_ext = [str(i) for i in file_ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f9c34539-a319-4b2e-8e0a-a26874854ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_ext = ['None', 'empty', '.dll', '.pyc', '.bat', '.pf', '.ini', '.exe', '.tmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "caed313e-7331-4493-a38b-f93a754d2e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"file_path_ext\", when(col(\"file_path_ext\").isNull(),\"None\").otherwise(col(\"file_path_ext\"))) \\\n",
    ".withColumn(\"file_path_ext\", when(col(\"file_path_ext\") == '', 'empty').otherwise(col(\"file_path_ext\"))) \\\n",
    ".withColumn(\"file_path_ext\", when(col(\"file_path_ext\").isin(file_ext), col('file_path_ext')).otherwise('other')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f7b4bf4-4d5e-49ac-9d0b-eb21245292a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_img = ['None','empty','.exe','.com']\n",
    "list_par_img = ['None','empty','.exe']\n",
    "\n",
    "df = df.withColumn(\"image_path_ext\", when(col(\"image_path_ext\").isNull(),\"None\").otherwise(col(\"image_path_ext\"))) \\\n",
    ".withColumn(\"image_path_ext\", when(col(\"image_path_ext\") == '', 'empty').otherwise(col(\"image_path_ext\"))) \\\n",
    ".withColumn(\"image_path_ext\", when(col(\"image_path_ext\").isin(list_img), col('image_path_ext')).otherwise('other')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d839b21f-1fa5-4e3d-b830-1035586248c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"parent_path_ext\", when(col(\"parent_path_ext\").isNull(),\"None\").otherwise(col(\"parent_path_ext\"))) \\\n",
    ".withColumn(\"parent_path_ext\", when(col(\"parent_path_ext\") == '', 'empty').otherwise(col(\"parent_path_ext\"))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f5c8d407-9fff-4f74-a21c-3a014f24b647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 677:===================================================> (193 + 7) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|image_path_ext|\n",
      "+--------------+\n",
      "|          None|\n",
      "|          .exe|\n",
      "|         empty|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('image_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0191b9b3-94c8-4087-9265-b99420396e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|parent_path_ext|\n",
      "+---------------+\n",
      "|           None|\n",
      "|           .exe|\n",
      "|          empty|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('parent_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eaefd101-40d5-4825-b5e8-4b08b612d47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now filter the file paths based on the 9 we selected above\n",
    "df = df.withColumn(\"file_path_ext\",\n",
    "                             when(col(\"file_path_ext\").isin(file_ext), \n",
    "                                  col('file_path_ext')).otherwise('other')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b6fd2a10-3be3-4e1e-b0c6-fe085a35e7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|file_path_ext|\n",
      "+-------------+\n",
      "|         .ini|\n",
      "|         None|\n",
      "|          .pf|\n",
      "|        other|\n",
      "|         .pyc|\n",
      "|         .tmp|\n",
      "|         .exe|\n",
      "|         .dll|\n",
      "|        empty|\n",
      "|         .bat|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check results\n",
    "df.select('file_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f24d707-7a10-420e-8450-69c71d798a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|image_path_ext|\n",
      "+--------------+\n",
      "|          null|\n",
      "|          .exe|\n",
      "|              |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('image_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcd34ac7-1369-41f5-90c7-6b762f74b12d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|parent_path_ext|\n",
      "+---------------+\n",
      "|           null|\n",
      "|           .exe|\n",
      "|               |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('parent_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5ee3cd2-a269-4143-b978-f6affa4d2a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now work on labeling entire malicious trace\n",
    "# create a sample dataframe that aligns closer with what you see above. \n",
    "#I think the trick is going to be partitioning based on object action and timestamp.\n",
    "#if those three things are the same then pull the first one out\n",
    "data = [(\"A\", \"B\",1, 1,'process','create',0), \n",
    "        (\"B\", \"C\",1, 2,'process','create',0), \n",
    "        (\"C\", \"D\",1, 3,'process','open',0),\n",
    "        (\"D\", \"E\",2,4,'process','open',1),\n",
    "        (\"E\", \"F\",2, 5,'file','read',0),\n",
    "       (\"F\", \"G\",2, 6,'process','create',0), \n",
    "        (\"G\", \"H\",3, 7,'process','create',0), \n",
    "        (\"H\", \"I\",3, 8,'process','open',1),\n",
    "        (\"I\", \"J\" ,3,9,'process','open',0),\n",
    "        (\"J\", \"K\",3, 10,'file','read',1),\n",
    "       (\"K\", \"L\",4, 8,'process','open',1),\n",
    "        (\"M\", \"N\",4,9,'process','open',1),\n",
    "        (\"O\", \"P\",4, 10,'file','read',1)]\n",
    "columns = [\"src\", \"dst\",\"Trace\", \"time\",\"object\",\"action\",'malicious']\n",
    "df_samp = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9df6bb4f-d5a0-4172-9d09-c5bc4fff6320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a window method to label traces as malicious given one event\n",
    "w = Window.partitionBy(\"Trace\")#.orderBy(\"time\")\n",
    "\n",
    "# Apply the conditions to the window using the when function\n",
    "df_samp = df_samp.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0))\n",
    "#df_samp = df_samp.withColumn('mal_trace', sum('mal_trace').over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2cbd7146-c877-4c79-9a21-31865e227d86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+-------+------+---------+---------+\n",
      "|src|dst|Trace|time| object|action|malicious|mal_trace|\n",
      "+---+---+-----+----+-------+------+---------+---------+\n",
      "|  A|  B|    1|   1|process|create|        0|        0|\n",
      "|  B|  C|    1|   2|process|create|        0|        0|\n",
      "|  C|  D|    1|   3|process|  open|        0|        0|\n",
      "|  D|  E|    2|   4|process|  open|        1|        1|\n",
      "|  E|  F|    2|   5|   file|  read|        0|        1|\n",
      "|  F|  G|    2|   6|process|create|        0|        1|\n",
      "|  G|  H|    3|   7|process|create|        0|        1|\n",
      "|  H|  I|    3|   8|process|  open|        1|        1|\n",
      "|  I|  J|    3|   9|process|  open|        0|        1|\n",
      "|  J|  K|    3|  10|   file|  read|        1|        1|\n",
      "|  K|  L|    4|   8|process|  open|        1|        1|\n",
      "|  M|  N|    4|   9|process|  open|        1|        1|\n",
      "|  O|  P|    4|  10|   file|  read|        1|        1|\n",
      "+---+---+-----+----+-------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_samp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0394c510-ae91-4161-a4f3-69a7dad27915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df = df.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75b8e02e-4f99-4f31-a22b-c2df1d77925e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: long (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- image_path_ext: string (nullable = true)\n",
      " |-- parent_path_ext: string (nullable = true)\n",
      " |-- new_path_ext: string (nullable = true)\n",
      " |-- file_path_ext: string (nullable = true)\n",
      " |-- mal_trace: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d917983a-96fd-4dcf-970e-c0f9cc09e169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 535:=============================================>      (174 + 16) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|mal_trace|  count|\n",
      "+---------+-------+\n",
      "|        1|  26502|\n",
      "|        0|1374297|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby('mal_trace').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10fc1d3f-2a72-427f-8cb9-e9a53e2a135b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in ['file_path_ext','image_path_ext','parent_path_ext','object','action']:\n",
    "    df, dict_mapping, cols_sparse = oneHotCol(df, c, {}, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e73de53d-120b-4167-b31b-dece6b6a04a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: long (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- new_path_ext: string (nullable = true)\n",
      " |-- file_path_ext_sparse: vector (nullable = true)\n",
      " |-- image_path_ext_sparse: vector (nullable = true)\n",
      " |-- parent_path_ext_sparse: vector (nullable = true)\n",
      " |-- object_sparse: vector (nullable = true)\n",
      " |-- action_sparse: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e9277cee-2acf-435e-bc98-6bc132e62637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hostname</th>\n",
       "      <th>user_name</th>\n",
       "      <th>privileges</th>\n",
       "      <th>image_path</th>\n",
       "      <th>parent_image_path</th>\n",
       "      <th>...</th>\n",
       "      <th>logon_id</th>\n",
       "      <th>requesting_domain</th>\n",
       "      <th>requesting_user</th>\n",
       "      <th>malicious</th>\n",
       "      <th>new_path_ext</th>\n",
       "      <th>file_path_ext_sparse</th>\n",
       "      <th>image_path_ext_sparse</th>\n",
       "      <th>parent_path_ext_sparse</th>\n",
       "      <th>object_sparse</th>\n",
       "      <th>action_sparse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e1</td>\n",
       "      <td>2b2a9a10-bcc0-413e-b6f1-8237c4579166</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>2019-09-23 15:04:40.021</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>e2</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>2019-09-23 15:09:54.088</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trace event                                   src  \\\n",
       "0      1    e1  2b2a9a10-bcc0-413e-b6f1-8237c4579166   \n",
       "1      1    e2  9274fa54-92f9-4b36-84c8-4b103c3e931a   \n",
       "\n",
       "                                    dst               timestamp  \\\n",
       "0  9274fa54-92f9-4b36-84c8-4b103c3e931a 2019-09-23 15:04:40.021   \n",
       "1  97fc76f2-2e7c-4c21-8a82-ae191c19a33f 2019-09-23 15:09:54.088   \n",
       "\n",
       "                     hostname user_name privileges  \\\n",
       "0  SysClient0321.systemia.com      None       None   \n",
       "1  SysClient0321.systemia.com      None       None   \n",
       "\n",
       "                                          image_path  \\\n",
       "0        \\Device\\HarddiskVolume1\\Python27\\python.exe   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...   \n",
       "\n",
       "                                   parent_image_path  ... logon_id  \\\n",
       "0                                               None  ...     None   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...  ...     None   \n",
       "\n",
       "  requesting_domain requesting_user malicious new_path_ext  \\\n",
       "0              None            None         0         None   \n",
       "1              None            None         0         None   \n",
       "\n",
       "                                file_path_ext_sparse  image_path_ext_sparse  \\\n",
       "0  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   (1.0, 0.0, 0.0, 0.0)   \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   (1.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "  parent_path_ext_sparse              object_sparse  \\\n",
       "0   (0.0, 1.0, 0.0, 0.0)  (0.0, 1.0, 0.0, 0.0, 0.0)   \n",
       "1   (1.0, 0.0, 0.0, 0.0)  (1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                                       action_sparse  \n",
       "0  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8a732199-831b-45fa-a14e-c3c0fc1c0f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df = df.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "70bc5f91-9cbf-4c81-af59-e8353f9ba58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now adjust the timediff bins\n",
    "df = ts_diff(df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "81f5b4e0-b920-4a37-bd35-a0b1957fc434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: long (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- new_path_ext: string (nullable = true)\n",
      " |-- file_path_ext_sparse: vector (nullable = true)\n",
      " |-- image_path_ext_sparse: vector (nullable = true)\n",
      " |-- parent_path_ext_sparse: vector (nullable = true)\n",
      " |-- object_sparse: vector (nullable = true)\n",
      " |-- action_sparse: vector (nullable = true)\n",
      " |-- mal_trace: integer (nullable = false)\n",
      " |-- timestamp_difference: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ce1de5d4-9f4a-401c-804e-0982c31ecc98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#develop quantiles for timestamp diff\n",
    "bins = df.approxQuantile(\"timestamp_difference\", [0.2, 0.4, .6, .8], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7d0c5548-c725-48fa-aeb0-110f5286e16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Bucketizer instance\n",
    "#bins = df.approxQuantile(\"timestamp_difference\", [0.2, 0.4, .6, .8], 0)\n",
    "#bins_2 = [float(\"-inf\")] + bins + [float(\"inf\")]\n",
    "#bucketizer = Bucketizer(splits=bins_2, inputCol=\"timestamp_difference\", outputCol=\"timestamp_bins_3\")\n",
    "# transform the DataFrame\n",
    "#df = bucketizer.transform(df)\n",
    "df_2 = df.withColumn(\"bin_column\", when(col(\"timestamp_difference\") <= bins[0], \"1\") \\\n",
    "              .when((col(\"timestamp_difference\") > bins[0]) & (col('timestamp_difference') <= bins[1]), \"2\") \\\n",
    "                .when((col(\"timestamp_difference\") > bins[1]) & (col('timestamp_difference') <= bins[2]), \"3\") \\\n",
    "                     .when((col(\"timestamp_difference\") > bins[2]) & (col('timestamp_difference') <= bins[3]), \"4\") \\\n",
    "                     .when((col(\"timestamp_difference\") > bins[3]), 5).otherwise('first'))\n",
    "                     \n",
    "              #((df[\"timestamp_difference\"] > percentiles[0]) & (df[\"timestamp_difference\"] <= percentiles[1]), \"2\")).otherwise(\"no\"))\n",
    "    #when((df[\"timestamp_difference\"] > percentiles[1]) & (df[\"timestamp_difference\"] <= percentiles[2]), \"3\"), \n",
    "    #when((df[\"timestamp_difference\"] > percentiles[2]) & (df[\"timestamp_difference\"] <= percentiles[3]), \"4\"),\n",
    "    #when(df[\"timestamp_difference\"] > percentiles[3], \"5\").otherwise(\"First\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a13ce-d904-4f2e-baa0-2ec8bde5a65c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a13f7344-134c-4a2c-b755-9ef01eaae898",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = bin_it(ts_diff(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3c8f9365-7e4d-4140-ba28-ac28e8f67d74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>object</th>\n",
       "      <th>action</th>\n",
       "      <th>hostname</th>\n",
       "      <th>user_name</th>\n",
       "      <th>privileges</th>\n",
       "      <th>...</th>\n",
       "      <th>direction</th>\n",
       "      <th>logon_id</th>\n",
       "      <th>requesting_domain</th>\n",
       "      <th>requesting_user</th>\n",
       "      <th>malicious</th>\n",
       "      <th>image_path_ext</th>\n",
       "      <th>parent_path_ext</th>\n",
       "      <th>new_path_ext</th>\n",
       "      <th>file_path_ext</th>\n",
       "      <th>timestamp_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>e1</td>\n",
       "      <td>1c5f0325-2863-46ab-8677-b3355a5bc649</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>2019-09-23 15:09:52.395</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>.pyc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>e2</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>2019-09-23 15:09:54.088</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>.exe</td>\n",
       "      <td>.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trace event                                   src  \\\n",
       "0     26    e1  1c5f0325-2863-46ab-8677-b3355a5bc649   \n",
       "1     26    e2  9274fa54-92f9-4b36-84c8-4b103c3e931a   \n",
       "\n",
       "                                    dst               timestamp   object  \\\n",
       "0  9274fa54-92f9-4b36-84c8-4b103c3e931a 2019-09-23 15:09:52.395     FILE   \n",
       "1  97fc76f2-2e7c-4c21-8a82-ae191c19a33f 2019-09-23 15:09:54.088  PROCESS   \n",
       "\n",
       "  action                    hostname user_name privileges  ... direction  \\\n",
       "0   READ  SysClient0321.systemia.com      None       None  ...      None   \n",
       "1   OPEN  SysClient0321.systemia.com      None       None  ...      None   \n",
       "\n",
       "  logon_id requesting_domain requesting_user malicious image_path_ext  \\\n",
       "0     None              None            None         0           .exe   \n",
       "1     None              None            None         0           .exe   \n",
       "\n",
       "  parent_path_ext new_path_ext  file_path_ext timestamp_bins  \n",
       "0            None         None           .pyc            NaN  \n",
       "1            .exe         None           None            0.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "da942a80-6518-467b-af19-e7ad8deca7ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: long (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- new_path_ext: string (nullable = true)\n",
      " |-- file_path_ext_sparse: vector (nullable = true)\n",
      " |-- image_path_ext_sparse: vector (nullable = true)\n",
      " |-- parent_path_ext_sparse: vector (nullable = true)\n",
      " |-- object_sparse: vector (nullable = true)\n",
      " |-- action_sparse: vector (nullable = true)\n",
      " |-- mal_trace: integer (nullable = false)\n",
      " |-- timestamp_difference: double (nullable = true)\n",
      " |-- timestamp_bins: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ca266861-5eda-4bb8-b55b-cbe9816fa555",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09171324-04f4-4bbb-a2d6-bf3028ed42b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/30 04:33:06 ERROR Executor: Exception in task 1.0 in stage 851.0 (TID 44568)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 6.000041961669922 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR Executor: Exception in task 0.0 in stage 851.0 (TID 44567)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 1693.000078201294 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR Executor: Exception in task 12.0 in stage 851.0 (TID 44579)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 1521.0001468658447 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR Executor: Exception in task 8.0 in stage 851.0 (TID 44575)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 1794769.9999809265 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR Executor: Exception in task 7.0 in stage 851.0 (TID 44574)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 2796658.9999198914 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR Executor: Exception in task 9.0 in stage 851.0 (TID 44576)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 1772542.9999828339 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR Executor: Exception in task 11.0 in stage 851.0 (TID 44578)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 1761456.0000896454 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR Executor: Exception in task 10.0 in stage 851.0 (TID 44577)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Feature value 2577963.9999866486 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n",
      "\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n",
      "\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n",
      "\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n",
      "\t... 39 more\n",
      "23/03/30 04:33:06 ERROR TaskSetManager: Task 0 in stage 851.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2307.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 851.0 failed 1 times, most recent failure: Lost task 0.0 in stage 851.0 (TID 44567) (ip-172-16-41-81.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Feature value 1693.000078201294 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n\t... 39 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Feature value 1693.000078201294 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n\t... 39 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2307.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 851.0 failed 1 times, most recent failure: Lost task 0.0 in stage 851.0 (TID 44567) (ip-172-16-41-81.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Feature value 1693.000078201294 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n\t... 39 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Bucketizer$$Lambda$7033/469333003: (double) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Feature value 1693.000078201294 out of Bucketizer bounds [6381.00004196167, 1351177.0000457764]. Check your features, or loosen the lower/upper bound constraints.\n\tat org.apache.spark.ml.feature.Bucketizer$.binarySearchForBuckets(Bucketizer.scala:294)\n\tat org.apache.spark.ml.feature.Bucketizer.$anonfun$transform$2(Bucketizer.scala:168)\n\tat scala.runtime.java8.JFunction1$mcDD$sp.apply(JFunction1$mcDD$sp.java:23)\n\t... 39 more\n"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72df11c-78ae-4816-825b-b99fc2a300f9",
   "metadata": {},
   "source": [
    "## Now run your function and see what happens, don't forget to update for trace 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "cb23403f-7d2f-4468-a1dd-c01186dd126b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a7adb83d-f827-4f4e-b3d0-62fe345cdaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6:15AM UTC on Mar 30, 2023 --- read time: 0.2825961112976074 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_23 = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6ca56f29-5d6d-4542-8853-43813bb6e9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2315 = df_23.where((col('event_hour') == 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ddfbe7e6-e69b-4876-852b-bf7c540b4a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4487:==============================================>    (183 + 16) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|file_path_ext|\n",
      "+-------------+\n",
      "|         .ini|\n",
      "|         None|\n",
      "|          .pf|\n",
      "|        other|\n",
      "|         .pyc|\n",
      "|         .tmp|\n",
      "|         .bat|\n",
      "|         .exe|\n",
      "|         .dll|\n",
      "|        empty|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2315.select('file_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8f9d0368-a2fa-49dd-ad0d-10d9b3412cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168873"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2315.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "68f9a983-5b8c-40fe-8370-88e3f41415e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.2606477737426758\n",
      "event traces: 466933\n"
     ]
    }
   ],
   "source": [
    "df_graph = create_graph3(df_2315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "63e19e2c-57b0-4099-8993-74484a9e62e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot = transp_expl3(df_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "68877afa-2da2-4d43-bef0-2f4e664d7917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df_onehot = df_onehot.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7ccd958a-16c1-4099-bde6-f46ef0fac692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#now develop time_diff bins\n",
    "df_onehot = bin_it(df_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c352b400-9fec-41cb-901b-0b39d27ba5e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.2523486614227295\n",
      "event traces: 466933\n",
      "create graph: 0.7373178005218506\n"
     ]
    }
   ],
   "source": [
    "#now create graph for event traces of 2 events, 3 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_2315.selectExpr('objectID as id').distinct()\n",
    "dst_vertices = df_2315.selectExpr('actorID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_2315.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_3oa = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                    #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_3oa.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))\n",
    "\n",
    "df_onehot = transp_expl3(cp_2315_3oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "42ee3376-f4ee-4591-9240-444a4164a5df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>object</th>\n",
       "      <th>action</th>\n",
       "      <th>hostname</th>\n",
       "      <th>user_name</th>\n",
       "      <th>privileges</th>\n",
       "      <th>image_path</th>\n",
       "      <th>parent_image_path</th>\n",
       "      <th>new_path</th>\n",
       "      <th>file_path</th>\n",
       "      <th>direction</th>\n",
       "      <th>logon_id</th>\n",
       "      <th>requesting_domain</th>\n",
       "      <th>requesting_user</th>\n",
       "      <th>malicious</th>\n",
       "      <th>bin_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e3</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>2019-09-23 15:09:54.088</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>e2</td>\n",
       "      <td>1e4b229d-e3aa-457e-a351-c90b35ddde69</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>2019-09-23 15:09:54.074</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>e1</td>\n",
       "      <td>df26cb51-6838-4d54-b63b-b32a0152638d</td>\n",
       "      <td>1e4b229d-e3aa-457e-a351-c90b35ddde69</td>\n",
       "      <td>2019-09-23 15:09:53.977</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>e1</td>\n",
       "      <td>23e70934-4ca4-400c-9935-f14884268370</td>\n",
       "      <td>1e4b229d-e3aa-457e-a351-c90b35ddde69</td>\n",
       "      <td>2019-09-23 15:09:54.020</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\SysWOW64\\winbr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>e2</td>\n",
       "      <td>1e4b229d-e3aa-457e-a351-c90b35ddde69</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>2019-09-23 15:09:54.074</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Python27\\python.exe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>e3</td>\n",
       "      <td>9274fa54-92f9-4b36-84c8-4b103c3e931a</td>\n",
       "      <td>97fc76f2-2e7c-4c21-8a82-ae191c19a33f</td>\n",
       "      <td>2019-09-23 15:09:54.088</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0321.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\conho...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trace event                                   src  \\\n",
       "0      1    e3  9274fa54-92f9-4b36-84c8-4b103c3e931a   \n",
       "1      1    e2  1e4b229d-e3aa-457e-a351-c90b35ddde69   \n",
       "2      1    e1  df26cb51-6838-4d54-b63b-b32a0152638d   \n",
       "3      2    e1  23e70934-4ca4-400c-9935-f14884268370   \n",
       "4      2    e2  1e4b229d-e3aa-457e-a351-c90b35ddde69   \n",
       "5      2    e3  9274fa54-92f9-4b36-84c8-4b103c3e931a   \n",
       "\n",
       "                                    dst               timestamp   object  \\\n",
       "0  97fc76f2-2e7c-4c21-8a82-ae191c19a33f 2019-09-23 15:09:54.088  PROCESS   \n",
       "1  9274fa54-92f9-4b36-84c8-4b103c3e931a 2019-09-23 15:09:54.074  PROCESS   \n",
       "2  1e4b229d-e3aa-457e-a351-c90b35ddde69 2019-09-23 15:09:53.977  PROCESS   \n",
       "3  1e4b229d-e3aa-457e-a351-c90b35ddde69 2019-09-23 15:09:54.020     FILE   \n",
       "4  9274fa54-92f9-4b36-84c8-4b103c3e931a 2019-09-23 15:09:54.074  PROCESS   \n",
       "5  97fc76f2-2e7c-4c21-8a82-ae191c19a33f 2019-09-23 15:09:54.088  PROCESS   \n",
       "\n",
       "  action                    hostname user_name privileges  \\\n",
       "0   OPEN  SysClient0321.systemia.com      None       None   \n",
       "1   OPEN  SysClient0321.systemia.com      None       None   \n",
       "2   OPEN  SysClient0321.systemia.com      None       None   \n",
       "3   READ  SysClient0321.systemia.com      None       None   \n",
       "4   OPEN  SysClient0321.systemia.com      None       None   \n",
       "5   OPEN  SysClient0321.systemia.com      None       None   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...   \n",
       "1        \\Device\\HarddiskVolume1\\Python27\\python.exe   \n",
       "2   \\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe   \n",
       "3   \\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe   \n",
       "4        \\Device\\HarddiskVolume1\\Python27\\python.exe   \n",
       "5  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...   \n",
       "\n",
       "                                   parent_image_path new_path  \\\n",
       "0  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...     None   \n",
       "1        \\Device\\HarddiskVolume1\\Python27\\python.exe     None   \n",
       "2   \\Device\\HarddiskVolume1\\Windows\\system32\\cmd.exe     None   \n",
       "3                                               None     None   \n",
       "4        \\Device\\HarddiskVolume1\\Python27\\python.exe     None   \n",
       "5  \\Device\\HarddiskVolume1\\Windows\\system32\\conho...     None   \n",
       "\n",
       "                                           file_path direction logon_id  \\\n",
       "0                                               None      None     None   \n",
       "1                                               None      None     None   \n",
       "2                                               None      None     None   \n",
       "3  \\Device\\HarddiskVolume1\\Windows\\SysWOW64\\winbr...      None     None   \n",
       "4                                               None      None     None   \n",
       "5                                               None      None     None   \n",
       "\n",
       "  requesting_domain requesting_user  malicious bin_column  \n",
       "0              None            None          0          1  \n",
       "1              None            None          0          1  \n",
       "2              None            None          0      first  \n",
       "3              None            None          0      first  \n",
       "4              None            None          0          1  \n",
       "5              None            None          0          1  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot.sort(\"Trace\").limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "82e45f5a-eafb-4dd7-a5f4-cbc0e20ae083",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: long (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- bin_column: string (nullable = false)\n",
      " |-- mal_trace: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_onehot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c1612e31-cfbd-438a-849a-d8063b3ec2bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path_ext\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o6324.fit.\n: org.apache.spark.SparkException: Input column file_path_ext does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.GeneratedMethodAccessor451.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m colm \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path_ext\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path_ext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparent_path_ext\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp_bins\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(colm)\n\u001b[0;32m----> 5\u001b[0m     df_onehot, dict_mapping, list_sparse \u001b[38;5;241m=\u001b[39m \u001b[43moneHotCol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_26025/957511345.py:14\u001b[0m, in \u001b[0;36moneHotCol\u001b[0;34m(df, colm, dict_mapping, cols_sparse)\u001b[0m\n\u001b[1;32m     12\u001b[0m sparse \u001b[38;5;241m=\u001b[39m colm\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_sparse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m indexer \u001b[38;5;241m=\u001b[39m StringIndexer(inputCol\u001b[38;5;241m=\u001b[39mcolm, outputCol\u001b[38;5;241m=\u001b[39mnum, handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m indexer_fitted \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m df_indexed \u001b[38;5;241m=\u001b[39m indexer_fitted\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[1;32m     17\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(inputCols\u001b[38;5;241m=\u001b[39m[num], outputCols\u001b[38;5;241m=\u001b[39m[sparse],dropLast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o6324.fit.\n: org.apache.spark.SparkException: Input column file_path_ext does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\n\tat sun.reflect.GeneratedMethodAccessor451.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "#for all columns to one hot, one hot, preserve mapping\n",
    "for colm in ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action']+[\"timestamp_bins\"]:\n",
    "    print(colm)\n",
    "    df_onehot, dict_mapping, list_sparse = oneHotCol(df_onehot,colm, {}, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a43ab-253f-4d88-9eb3-0a0acf7eb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = colm+'_numeric'\n",
    "sparse = colm+'_sparse'\n",
    "indexer = StringIndexer(inputCol=colm, outputCol=num, handleInvalid=\"keep\")\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df_indexed = indexer_fitted.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bb4713a8-f803-4cfe-acc7-8ddc98fa2f49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.3053398132324219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 466933\n",
      "create graph: 16.650644540786743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transposed explode and bin: 34.64272904396057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot time: 138.65559077262878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elapsed time: 155.27439427375793\n"
     ]
    }
   ],
   "source": [
    "df_enc,dict_mapping = trace_encode(df_2315, 3,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5793cfee-00b9-4f0c-9e37-02bd25861a84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.41779136657714844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 309859\n",
      "create graph: 21.555825233459473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transposed explode and bin: 44.78338027000427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot time: 179.72427415847778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elapsed time: 201.0058970451355\n"
     ]
    }
   ],
   "source": [
    "df_enc4,dict_mapping4 = trace_encode(df_2315, 4,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648555db-59ba-46bf-9a13-ca08a484a7fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read/Write examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5d5c0abc-0f2e-45c7-b25c-cdb490e5ddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2338"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc.where((col('mal_trace')==1)&(col('malicious')==1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "cb3c56d2-098b-4b99-a9cc-cfd53b72d14d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466933"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this math checks out \n",
    "df_enc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a2d5cd6a-cccb-49f3-a44b-6ff63a91f944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.7146964073181152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7568:========================>                              (7 + 9) / 16]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 52796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test = create_graph6(df_2315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "88d15788-fb12-47b4-8a56-d79d9f9cd229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_enc.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/sample3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f7c9c5eb-0b61-4cf9-9b4a-8c904e145519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    }
   ],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_enc4.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/sample4\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66004407-59fc-4966-8329-66d3c7a7a409",
   "metadata": {
    "tags": []
   },
   "source": [
    "#read it back in \n",
    "#now read the traces back in graph back in\n",
    "df_t_23 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/traces_23\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c2f4c-5cd3-4cf6-ae10-722b58a83ae4",
   "metadata": {},
   "source": [
    "### Create real data with just image and parent image extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4673666f-8e80-41f4-8f17-c4dc95016760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:43AM UTC on Mar 31, 2023 --- read time: 0.3529171943664551 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_23 = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2cca04-8253-4599-b0b7-b7c913b26044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23 = df_23.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aef6972-a888-487c-a683-d6917b9bbb61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1209934"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_23.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9017b60-718e-43c0-8f22-e329915b91bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.7539646625518799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 17373568\n",
      "create graph: 22.52540636062622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transposed explode and bin: 149.23698830604553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot time: 355.5194504261017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elapsed time: 380.7928717136383\n"
     ]
    }
   ],
   "source": [
    "df_23_enc_3,dict_mapping_23_3 = trace_encode(df_23, 3,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623d5fdb-9941-4e33-8d54-4d874195c423",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                00]\r"
     ]
    }
   ],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_23_enc_3.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/real/23Sep3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7360f0f-084a-4101-ae6b-a3c096858e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path_ext': ['None',\n",
       "  'empty',\n",
       "  '.dll',\n",
       "  'other',\n",
       "  '.pyc',\n",
       "  '.ini',\n",
       "  '.pf',\n",
       "  '.tmp',\n",
       "  '.exe',\n",
       "  '.bat'],\n",
       " 'image_path_ext': ['.exe', 'empty', 'None', '.com'],\n",
       " 'parent_path_ext': ['.exe', 'None', 'empty'],\n",
       " 'object': ['PROCESS', 'FILE', 'FLOW', 'SHELL'],\n",
       " 'action': ['OPEN',\n",
       "  'READ',\n",
       "  'MODIFY',\n",
       "  'INFO',\n",
       "  'CREATE',\n",
       "  'WRITE',\n",
       "  'DELETE',\n",
       "  'RENAME',\n",
       "  'COMMAND'],\n",
       " 'timestamp_bins': ['4.0', '1.0', '0.0', '2.0', '3.0']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_mapping_23_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f38d78e-f5a9-4695-ad65-f0ca25aa4d08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.4708554744720459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 18438173\n",
      "create graph: 25.82018232345581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2082:===========>                                            (1 + 4) / 5]]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 01:57:18 ERROR TaskSetManager: Total size of serialized results of 4 tasks (1256.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2082:===========>                                            (1 + 3) / 5]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1229.approxQuantile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 4 tasks (1256.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:103)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_23_enc_4,dict_mapping_23_4 \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_23\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile_path_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparent_path_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/ipykernel_10304/3142289654.py:26\u001b[0m, in \u001b[0;36mtrace_encode\u001b[0;34m(df, traces, list_cols, output)\u001b[0m\n\u001b[1;32m     21\u001b[0m df_onehot \u001b[38;5;241m=\u001b[39m df_onehot\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmal_trace\u001b[39m\u001b[38;5;124m'\u001b[39m, when(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmalicious\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mover(w) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#ideally pagerank, will likely have to join\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#now develop time_diff bins\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m df_onehot \u001b[38;5;241m=\u001b[39m \u001b[43mbin_it\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_onehot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#instantiate dictionary and return df\u001b[39;00m\n\u001b[1;32m     29\u001b[0m dict_mapping \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/tmp/ipykernel_10304/1391471322.py:10\u001b[0m, in \u001b[0;36mbin_it\u001b[0;34m(stacked_df)\u001b[0m\n\u001b[1;32m      7\u001b[0m stacked_df \u001b[38;5;241m=\u001b[39m stacked_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp_bins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# create a Bucketizer instance with quantiles\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m bins \u001b[38;5;241m=\u001b[39m \u001b[43mstacked_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp_difference\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m bins_2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;241m+\u001b[39m bins \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     14\u001b[0m bucketizer \u001b[38;5;241m=\u001b[39m Bucketizer(splits\u001b[38;5;241m=\u001b[39mbins_2, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp_difference\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp_bins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2851\u001b[0m, in \u001b[0;36mDataFrame.approxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   2848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelativeError should be >= 0.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2849\u001b[0m relativeError \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(relativeError)\n\u001b[0;32m-> 2851\u001b[0m jaq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelativeError\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2852\u001b[0m jaq_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(j) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m jaq]\n\u001b[1;32m   2853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jaq_list[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m isStr \u001b[38;5;28;01melse\u001b[39;00m jaq_list\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1229.approxQuantile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 4 tasks (1256.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:103)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 01:57:22 ERROR TaskSetManager: Total size of serialized results of 5 tasks (1590.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2082:======================>                                 (2 + 1) / 5]\r"
     ]
    }
   ],
   "source": [
    "df_23_enc_4,dict_mapping_23_4 = trace_encode(df_23, 4,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b0f16-0f68-4cc5-a532-a690b9268bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_23_enc_6,dict_mapping_23_6 = trace_encode(df_23, 6,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
