{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c605132c-ae51-48ff-98bf-b7028d120233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/hands-on-generative-adversarial-networks-gan-for-signal-processing-with-python-ff5b8d78bd28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a675010-0fd6-47bf-b96f-8f2bb887bc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4d9ebd-ce9a-4150-8fc4-cbe0a4eecc31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-eef77a52-ea98-4a8e-9212-ee0f2bc35829;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 252ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-eef77a52-ea98-4a8e-9212-ee0f2bc35829\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 17:51:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 17:51:18 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run ./read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f752d7-2f69-493b-a03a-0acb4db74db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/68036975/valueerror-shape-must-be-at-least-rank-3-but-is-rank-2-for-node-biasadd\n",
    "# config for rank error in lstm\n",
    "tf.keras.backend.set_image_data_format(\"channels_last\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295239d-3d08-4f7d-bef1-1de64e1d0b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64fc124-d080-4ff2-8c79-9918822704ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ds = spark.read.parquet(*[\"s3a://sapient-bucket-trusted/prod/graph/encoded/real/23Sep3/*\"]) \\\n",
    "                .withColumn(\"event_sequence\",col('event_sequence').cast('string')) \\\n",
    "                .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e704df-8177-4e3d-8436-16f3e46e533c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tot = ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20dd31f-3f4c-4b1d-9213-29859381e1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------------+\n",
      "|mal_trace|cnt_per_group|perc_of_count_total|\n",
      "+---------+-------------+-------------------+\n",
      "|        1|       118763| 0.6835843967111419|\n",
      "|        0|     17254805|  99.31641560328886|\n",
      "+---------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ds.groupBy(\"mal_trace\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'cnt_per_group') \\\n",
    "    .withColumn('perc_of_count_total', (col('cnt_per_group') / tot) * 100 ) \\\n",
    "    .sort(\"perc_of_count_total\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51475f9f-73c6-477b-a45e-d3f76a3d4d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Config\n",
    "max_length = 6\n",
    "sequence_length = 6\n",
    "max_features = 6\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "input_len = 5000\n",
    "\n",
    "# place to save model after\n",
    "ckt_path_generator = 'saved_model/generator'\n",
    "ckt_path_tokenizer = 'saved_model/tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d53c55-83ec-4b3f-929b-dc66f0fb3a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb58e17-1e00-4ad8-a799-9a6d250f6597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ds_events = ds.select('event_sequence').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992eb682-a6c9-490c-8426-46248ea03038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcf9cc65-f4eb-4612-b0c2-ec4d4e640779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get vocab for full dataset \n",
    "tokenizer.fit_on_texts(ds_events)\n",
    "# Get our training data word index\n",
    "word_index = tokenizer.word_index\n",
    "vocab_count = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73177773-2fdd-49c3-8db5-fd474068ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer for reuse\n",
    "# tokenizer.save('ckt_path_tokenizer')\n",
    "with open(ckt_path_tokenizer, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80e719-db06-4b06-bd48-056d924b2576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842ecdb-06a1-4469-85bd-91dc0059f1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seq_enc_tensor.shape\n",
    "# # shape - (input_len, sequence_length, vocab_size)\n",
    "# input_len = seq_enc_tensor.shape[0]\n",
    "# sequence_length = seq_enc_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b6f5d-1ae6-4632-8570-8b0751b5f695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(sequence_length, vocab_count)),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_count, activation='softmax')),\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(sequence_length, vocab_count)),\n",
    "            tf.keras.layers.LSTM(128, return_sequences=False),\n",
    "            tf.keras.layers.Dense(2, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_latent_space():\n",
    "    n = tf.random.uniform(shape=[input_len, sequence_length, vocab_count], minval=1, maxval=vocab_count, dtype=tf.int32)\n",
    "    return n\n",
    "\n",
    "def generate_fake_samples(generator):\n",
    "    # generate points in latent space & pass through generator\n",
    "    x = generator.predict(generate_latent_space(), verbose=0)\n",
    "    # create class labels\n",
    "    y = zeros((input_len,1))\n",
    "    return x, y\n",
    "\n",
    "# def generate_real_samples():\n",
    "#     x = seq_enc_tensor\n",
    "#     # create class labels\n",
    "#     y = ones((input_len,1))\n",
    "#     return x, y\n",
    "\n",
    "def generate_real_samples():\n",
    "    # each time, this should produce a different sample of the larger data\n",
    "    # get only malicious data\n",
    "    ds_mal = ds.filter( col('mal_trace') == 1).cache()\n",
    "\n",
    "    n = input_len  # number of rows to select\n",
    "    total_rows = ds_mal.count()\n",
    "    fraction = float(n) / float(total_rows)\n",
    "    df_sample, _ = ds_mal.randomSplit([fraction, 1.0 - fraction], seed=42)\n",
    "    df_sample = df_sample.limit(n)\n",
    "\n",
    "    ds.unpersist()\n",
    "    dm_events = df_sample.select('event_sequence').rdd.flatMap(lambda x: x).collect()\n",
    "    dm_labels = df_sample.select('mal_trace').rdd.flatMap(lambda x: x).collect()\n",
    "    # one hot encode the data\n",
    "    dm_sequences = tokenizer.texts_to_sequences(dm_events)\n",
    "    dm_padded = tf.keras.utils.pad_sequences(dm_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    seq_enc_tensor = tf.one_hot(dm_padded, vocab_count)\n",
    "    x = seq_enc_tensor\n",
    "    # create class labels\n",
    "    y = ones((input_len,1))\n",
    "    return x, y\n",
    "\n",
    "def train(g_model, d_model, gan_model, epochs=5, n_eval=500):\n",
    "    d_acc_history = []\n",
    "    d_loss_history = []\n",
    "    g_acc_history = []\n",
    "    g_loss_history = []\n",
    "\n",
    "    d_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    g_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    # manually enumerate epochs\n",
    "    for i in range(epochs):\n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples()\n",
    "        # prepare fake examples using the generator\n",
    "        x_fake, y_fake = generate_fake_samples(g_model)\n",
    "\n",
    "        # update discriminator\n",
    "        d_real_loss = d_model.train_on_batch(x_real, y_real)\n",
    "        d_fake_loss = d_model.train_on_batch(x_fake, y_fake)\n",
    "        d_loss = 0.5 * (d_real_loss + d_fake_loss)\n",
    "\n",
    "        d_real_pred = d_model.predict_on_batch(x_real)\n",
    "        d_fake_pred = d_model.predict_on_batch(x_fake)\n",
    "\n",
    "        d_metric.update_state(tf.concat([y_real, y_fake], axis=0), tf.concat([d_real_pred, d_fake_pred], axis=0))\n",
    "        d_acc = d_metric.result().numpy()\n",
    "\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_space()\n",
    "        y_gan = ones((input_len, 1))\n",
    "        g_loss = gan_model.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_pred = gan_model.predict_on_batch(x_gan)\n",
    "        g_metric.update_state(tf.one_hot(tf.cast(y_gan, tf.int32), depth=2), gan_pred)\n",
    "        g_acc = g_metric.result().numpy()\n",
    "\n",
    "        d_acc_history.append(d_acc)\n",
    "        d_loss_history.append(d_loss)\n",
    "        g_acc_history.append(g_acc)\n",
    "        g_loss_history.append(g_loss)\n",
    "\n",
    "        if i % n_eval == 0:\n",
    "            print(f\"Epoch {i}: Discriminator Loss: {d_loss}, Discriminator Accuracy: {d_acc}, Generator Loss: {g_loss}, Generator Accuracy: {g_acc}\")\n",
    "    # checkpoint model after completion of epochs\n",
    "    g_model.save(ckt_path_generator)\n",
    "\n",
    "    return d_acc_history, d_loss_history, g_acc_history, g_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2bc25-832b-4b86-bf51-609dde91aa96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4068c3-bd4a-4228-9991-43cbdb8fce24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = define_generator()\n",
    "discriminator = define_discriminator()\n",
    "gan = define_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab7c04-e0cd-4e7e-9226-43ac91672f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "d_acc_history, d_loss_history, g_acc_history, g_loss_history = train(generator, discriminator, gan, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db84c0-205d-48e6-8cb0-9eed0fca944d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(range(1, epochs + 1), d_loss_history, label='Discriminator Loss')\n",
    "ax1.plot(range(1, epochs + 1), g_loss_history, label='Generator Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(range(1, epochs + 1), d_acc_history, label='Discriminator Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4b995-caab-4e73-9b5a-752cd170f276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbed33-f7b4-4700-9249-e9928b16ebbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeff66a-b0b6-47d9-b54a-601d60790fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a basic model instance\n",
    "generator = define_generator()\n",
    "generator.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccaa0fd-e3db-4baa-810a-8d58f48e42b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_samples():\n",
    "    x_gen, y_gen = generate_fake_samples(generator)\n",
    "    new_seq_enc = tf.argmax(x_gen, axis=-1)\n",
    "    new_seq_texts = tokenizer.sequences_to_texts(new_seq_enc.numpy())\n",
    "    gen_events = [[','.join(n.split())] for n in new_seq_texts]\n",
    "    gen_labels = [1]*input_len\n",
    "    trace_list = [99999]*input_len\n",
    "    d_gen = list(zip(trace_list, gen_labels, gen_labels, gen_events))\n",
    "    df_gen = spark.createDataFrame(d_gen, ['Trace', 'mal_trace', 'malicious', 'event_sequence'])\n",
    "    return df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07577f-c420-4f3a-9b78-f8656efe1f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of iterations\n",
    "num_iterations = 10\n",
    "\n",
    "#initialize combined dataframe\n",
    "gens = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d1ace-1ae6-41e2-a726-743bd1382ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over the input parameters\n",
    "for param in range(num_iterations):\n",
    "    # Create a dataframe with the current input parameter\n",
    "    df = make_samples()\n",
    "    # Append the dataframe to the combined dataframe using union\n",
    "    if gens is None:\n",
    "        gens = df\n",
    "    else:\n",
    "        gens = gens.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947bacc-608d-450e-b4c6-7f6475d90049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aee1f3-7a5a-4ede-a40c-dd5d6c2daac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gens.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6e83f-bc9e-4eaf-a91c-edf6370c2210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gens.groupBy(\"mal_trace\").agg(countDistinct('event_sequence')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57202612-6797-4012-9dcc-c16c724a161a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gens.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f702f0e-bf8f-49d6-9850-17f7cfcb86fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gens.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(\"s3a://sapient-bucket-trusted/prod/graph/motifs/gen_malicious/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc526ca-7176-4386-95d6-208067c2a39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
