{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970130b0-b08c-4536-936e-ca923cc595a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setup based on this: https://t-redactyl.io/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html\n",
    "import boto3\n",
    "import json \n",
    "import time\n",
    "import ntpath\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef5d6-368f-43af-b519-ba5112c87b79",
   "metadata": {},
   "source": [
    "## Set Spark Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8e9008-6651-4c17-8c12-eef2b18e10fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7486ea00-f477-4f51-bf6e-b66a57f1a557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(\n",
    "    SecretId='sapient-s3-access'\n",
    ")\n",
    "response = json.loads(response['SecretString'])\n",
    "access_key = response[\"aws_access_key_id\"]\n",
    "secret_key = response[\"aws_secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372553c2-9f17-4c4f-b253-275e30a1ef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars()))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9392e443-f6d6-4a54-bbb8-2972eafaf97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 15:11:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/25 15:11:22 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/03/25 15:11:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/25 15:11:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/configuration.html#memory-management\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', access_key)\n",
    "    .config('fs.s3a.secret.key', secret_key)\n",
    "    .config('spark.network.timeout', 300)\n",
    "    .config('spark.local.dir', '/home/ec2-user/SageMaker/tmp')\n",
    "    .config(\"spark.executor.memory\", \"100g\")\n",
    "    .config(\"spark.driver.memory\", \"90g\")\n",
    "    .config('spark.sql.shuffle.partitions', 300)\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\",\"20g\")\n",
    "    .appName(\"sapient\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0acc-5281-4b73-a33b-67ac7c05b96a",
   "metadata": {},
   "source": [
    "## Functions to Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5e8ea5-b6a8-4fdd-84d5-22e3b31c7340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read from raw bucket + write to refined bucket + aggregate final to the trusted bucket\n",
    "s3_url_raw = \"s3a://sapient-bucket-raw/\"\n",
    "s3_url_refined = \"s3a://sapient-bucket-refined/\"\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "ecar_cols = [\n",
    "    'id','timestamp','objectID','actorID','object','action','hostname', 'user_name', 'privileges', 'image_path', \n",
    "    'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user'\n",
    "            ]\n",
    "bro_cols_conn = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "                 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "bro_cols_rep = ['ts', 'level', 'message', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481fb69e-f75d-4b6c-bbfc-7b02c0f1264f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[hostname: string, id: string, objectID: string, actorID: string, timestamp: timestamp, object: string, action: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a new dataframe with distinct objectIDs to identify malcious ObjectIds\n",
    "df_labels = spark.read.parquet(f\"{s3_url_refined}/prod/labels\").cache()\n",
    "malcious_events = list(df_labels.select('id').distinct().toPandas()['id'])\n",
    "df_labels.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7064e41-3fbf-40ba-b097-85c6cebef667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getFirstEvents(df):\n",
    "    \"\"\"\n",
    "    this creates then drops duplicates and gets the first appearance of each relationship entry\n",
    "    input - dataframe with columns objectID and actorID\n",
    "    output dataframe\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(\"relationship\").orderBy(\"timestamp\")\n",
    "    # label malicious events, filter to only hosts with malicious events\n",
    "    df = df.withColumn('relationship', concat(df.actorID, lit('->'),df.objectID) ) \\\n",
    "                .withColumn('rank', rank().over(window)) \\\n",
    "                .filter(col('rank') == 1) \\\n",
    "                .drop('rank')\n",
    "    mal_hosts = list(df.select('hostname').filter( col('malicious') == 1 ).distinct().toPandas()['hostname'])\n",
    "    df = df.filter(col(\"hostname\").isin(mal_hosts))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a79f608-d3ad-41ca-8804-cfc6e725a2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def readCheckpoint(type='ecar', env='prod', size='small'):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro\n",
    "    \"\"\"\n",
    "    if type == 'labels':\n",
    "        s3_parquet_loc = f\"{s3_url_trusted}/{env}/{type}\"\n",
    "    else:\n",
    "        s3_parquet_loc = f\"{s3_url_trusted}/{env}/{type}/{size}\"\n",
    "    start_time = time.time()\n",
    "    df = spark.read.parquet(s3_parquet_loc)\\\n",
    "                .withColumn(\"malicious\", when(col('id').isin( malcious_events ), 1) \n",
    "                .otherwise(0))\n",
    "    df.groupBy('malicious').count().show()\n",
    "    df = df.filter( col('actorID') != col('objectID')) \n",
    "    # get first connect events\n",
    "    df = getFirstEvents(df)\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and cache time: %s seconds ---\" % (time.time() - start_time))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b765892-5020-4680-ad84-aa6d2a06a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_firsts(size='all'):\n",
    "    \"\"\"\n",
    "    this creates then drops duplicates and writes them to file in S3\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    df = readCheckpoint(size=size).cache()\n",
    "    # create file features\n",
    "    df.withColumn(\"image_path\", getFileUDF(col(\"image_path\"))) \\\n",
    "        .withColumn(\"parent_image_path\", getFileUDF(col(\"parent_image_path\"))) \\\n",
    "        .withColumn(\"new_path\", getFileUDF(col(\"new_path\"))) \\\n",
    "        .withColumn(\"file_path\", getFileUDF(col(\"file_path\"))) \\\n",
    "        .withColumn(\"image_path_ext\", regexp_extract(\"image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"parent_path_ext\", regexp_extract(\"parent_image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"new_path_ext\", regexp_extract(\"new_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"file_path_ext\", regexp_extract(\"file_path\", \"\\.[0-9a-z]+$\", 0))\n",
    "    df_first_events.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/first_events\")\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and write time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3f2729-6144-4bf8-ab82-efce5f847966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def writeFirstEvents(size='all'):\n",
    "    \"\"\"\n",
    "    this creates then drops duplicates and writes them to file in S3\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    # remove self references\n",
    "    df = readCheckpoint(size=size).cache()\n",
    "    # get first connect events, create file features, label malicious events, filter to only hosts with malicious events\n",
    "    df_first_events = getFirstEvents(df).cache() \n",
    "    df_first_events.limit(1).toPandas()\n",
    "    # filter first event hosts to only those with malicious events\n",
    "    mal_hosts = list(df_first_events.select('hostname').filter( col('malicious') == 1 ).distinct().toPandas()['hostname'])\n",
    "    df_first_events.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/first_events\")\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and write time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05544d7-c867-4b86-8621-90363c8ca72e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def just_write_new_features(df):\n",
    "    start_time = time.time()\n",
    "    df.withColumn(\"image_path\", getFileUDF(col(\"image_path\"))) \\\n",
    "        .withColumn(\"parent_image_path\", getFileUDF(col(\"parent_image_path\"))) \\\n",
    "        .withColumn(\"new_path\", getFileUDF(col(\"new_path\"))) \\\n",
    "        .withColumn(\"file_path\", getFileUDF(col(\"file_path\"))) \\\n",
    "        .withColumn(\"image_path_ext\", regexp_extract(\"image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"parent_path_ext\", regexp_extract(\"parent_image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"new_path_ext\", regexp_extract(\"new_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"file_path_ext\", regexp_extract(\"file_path\", \"\\.[0-9a-z]+$\", 0))\n",
    "    df.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/first_events\")\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and write time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c2febd7-e719-47bd-b68a-aa532a601c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getFile(str):\n",
    "    \"\"\"\n",
    "    udf to get the file from a full file path\n",
    "    similar udf (non-windows): https://stackoverflow.com/questions/40848681/udf-to-extract-only-the-file-name-from-path-in-spark-sql\n",
    "    \"\"\"\n",
    "    if str == None:\n",
    "        pass\n",
    "    else:\n",
    "        new_str = ntpath.basename(str)\n",
    "        return new_str\n",
    "getFileUDF = udf(lambda z: getFile(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "593da080-d725-4f08-be7b-5a8519e6719a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def readFirstEvents():\n",
    "    start_time = time.time()\n",
    "    df = spark.read.parquet(f\"{s3_url_trusted}/prod/final_features\")\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read time: %s seconds ---\" % (time.time() - start_time))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d185654a-db6d-4aa0-ac46-8e9ddd42e2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|malicious|    count|\n",
      "+---------+---------+\n",
      "|        1|   188448|\n",
      "|        0|452586026|\n",
      "+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7:40PM UTC on Mar 22, 2023 --- read and cache time: 395.0087320804596 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# remove self references\n",
    "df = readCheckpoint(size='all').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e000d7fa-a693-458e-8264-a6f7c7f1a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4595692"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "205a0782-ccc4-43aa-85a9-579eba49e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7:47PM UTC on Mar 22, 2023 --- read and write time: 782.0313174724579 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/final_features\")\n",
    "df.unpersist()\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and write time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c3f2406-c82c-4b4d-96b4-cb8dd307da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df = spark.read.parquet(f\"{s3_url_trusted}/prod/final_features\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b602da07-4357-44d5-a8b0-6271a09f1459",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:======================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|malicious|  count|\n",
      "+---------+-------+\n",
      "|        1|   8596|\n",
      "|        0|4587096|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('malicious').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b306c1f8-5397-47b8-809b-894123b2ef14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7:47PM UTC on Mar 22, 2023 --- read and cache time: 6.256082057952881 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and cache time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "348a505a-1dfe-475f-89d1-5ef621e0f056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292264\n"
     ]
    }
   ],
   "source": [
    "print(len(malcious_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc4333b-6dfa-418f-8e3c-56e0961dcb98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103816"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "292264-188448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "638745d7-00c0-4a1c-9807-4efbbe3c8971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|malicious|    count|\n",
      "+---------+---------+\n",
      "|        1|   188448|\n",
      "|        0|452586026|\n",
      "+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:23PM UTC on Mar 25, 2023 --- read and cache time: 703.2229986190796 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#re-writing for testing\n",
    "df = readCheckpoint(size='all').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a63bd4-13ad-4c6a-8b2b-53cc2cfb0f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:50PM UTC on Mar 25, 2023 --- read and write time: 1404.1512756347656 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#now I want to write to first events with all the cool new features\n",
    "just_write_new_features(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
