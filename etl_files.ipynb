{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970130b0-b08c-4536-936e-ca923cc595a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup based on this: https://t-redactyl.io/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html\n",
    "import boto3\n",
    "import json \n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, isnan, when, count, col\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker_pyspark\n",
    "import botocore.session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef5d6-368f-43af-b519-ba5112c87b79",
   "metadata": {},
   "source": [
    "## Set Spark Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8e9008-6651-4c17-8c12-eef2b18e10fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7486ea00-f477-4f51-bf6e-b66a57f1a557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(\n",
    "    SecretId='sapient-s3-access'\n",
    ")\n",
    "response = json.loads(response['SecretString'])\n",
    "access_key = response[\"aws_access_key_id\"]\n",
    "secret_key = response[\"aws_secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372553c2-9f17-4c4f-b253-275e30a1ef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9392e443-f6d6-4a54-bbb8-2972eafaf97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/15 02:07:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', access_key)\n",
    "    .config('fs.s3a.secret.key', secret_key)\n",
    "    .config('spark.network.timeout', 300)\n",
    "    .config('spark.memory.offHeap.size','4g')\n",
    "    .config('spark.executor.memory', '16g')\n",
    "    .appName(\"sapient\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0acc-5281-4b73-a33b-67ac7c05b96a",
   "metadata": {},
   "source": [
    "## Functions to Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5e8ea5-b6a8-4fdd-84d5-22e3b31c7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from raw bucket + write to refined bucket + aggregate final to the trusted bucket\n",
    "s3_url_raw = \"s3a://sapient-bucket-raw/\"\n",
    "s3_url_refined = \"s3a://sapient-bucket-refined/\"\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "bro_cols_conn = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "                 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "bro_cols_rep = ['ts', 'level', 'message', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dff8a6f-87ed-4b8c-a119-9ceeaaaa9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecarbro.json, AIA-1-25.ecar.json, conn.09_00_00-10_00_00.log\n",
    "def s3_file(file):\n",
    "    dev_file = f\"{s3_url_raw}/{env}/{file}\"\n",
    "    prod_file = \"\"\n",
    "    if env == \"prod\":\n",
    "      filename = prod_file\n",
    "    else:\n",
    "      filename = dev_file\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "948047f5-3308-49ad-b9c7-f281aabb8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndCheckpoint(type):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro, labels\n",
    "    This function reads a file from json or log text and writes it as a parquet.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    if type == 'ecar':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'ecar-bro':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        # this will extract and flatten nested properties column\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'bro':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/**/**/*.log\", sep=\"\\t\", comment=\"#\", header=False)\n",
    "        df = df.limit(1000)\n",
    "        df = df.toDF(*bro_cols_conn)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'labels':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/{type}/*.csv\", sep=\",\", header=True)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee57e1-0d17-4b96-bdc9-ce32d2d2e9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8042a2bb-ac2c-40a2-9502-9f068f999a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the end of the file\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
