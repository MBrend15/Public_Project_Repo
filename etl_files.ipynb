{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970130b0-b08c-4536-936e-ca923cc595a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setup based on this: https://t-redactyl.io/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html\n",
    "import boto3\n",
    "import json \n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, isnan, when, count, col\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker_pyspark\n",
    "import botocore.session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef5d6-368f-43af-b519-ba5112c87b79",
   "metadata": {},
   "source": [
    "## Set Spark Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8e9008-6651-4c17-8c12-eef2b18e10fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7486ea00-f477-4f51-bf6e-b66a57f1a557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(\n",
    "    SecretId='sapient-s3-access'\n",
    ")\n",
    "response = json.loads(response['SecretString'])\n",
    "access_key = response[\"aws_access_key_id\"]\n",
    "secret_key = response[\"aws_secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372553c2-9f17-4c4f-b253-275e30a1ef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9392e443-f6d6-4a54-bbb8-2972eafaf97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 07:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/22 07:02:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', access_key)\n",
    "    .config('fs.s3a.secret.key', secret_key)\n",
    "    .config('spark.network.timeout', 300)\n",
    "    .config('spark.memory.offHeap.size','4g')\n",
    "    .config('spark.executor.memory', '16g')\n",
    "    .appName(\"sapient\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0acc-5281-4b73-a33b-67ac7c05b96a",
   "metadata": {},
   "source": [
    "## Functions to Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5e8ea5-b6a8-4fdd-84d5-22e3b31c7340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read from raw bucket + write to refined bucket + aggregate final to the trusted bucket\n",
    "s3_url_raw = \"s3a://sapient-bucket-raw/\"\n",
    "s3_url_refined = \"s3a://sapient-bucket-refined/\"\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "bro_cols_conn = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "                 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "bro_cols_rep = ['ts', 'level', 'message', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "948047f5-3308-49ad-b9c7-f281aabb8c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadAndCheckpoint(type='ecar-bro', env='dev', size='small'):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro, labels\n",
    "    This function reads a file from json or log text and writes it as a parquet.\n",
    "    \"\"\"\n",
    "    if size == 'small':\n",
    "        # 1 million\n",
    "        read_lim = 1000000\n",
    "    elif size == 'medium':\n",
    "        # 1 billion\n",
    "        read_lim = 1000000000\n",
    "    elif size == 'large':\n",
    "        # 15 billion\n",
    "        read_lim = 15000000000   \n",
    "    start_time = time.time()\n",
    "    if type in ('ecar', 'car'):\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        df = df.limit(read_lim)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{size}/{env}/ecar\")\n",
    "    elif type in ('ecar-bro','car-bro'):\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        # this will extract and flatten nested properties column\n",
    "        df = df.limit(read_lim)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{size}/{env}/ecar-bro\")\n",
    "    elif type == 'bro':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/**/**/conn*.log\", sep=\"\\t\", comment=\"#\", header=False)\n",
    "        df = df.limit(read_lim)\n",
    "        df = df.toDF(*bro_cols_conn)\n",
    "        df.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{size}/{env}/bro\")\n",
    "    elif type == 'labels':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/{type}/*.csv\", sep=\",\", header=True)\n",
    "        df.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{size}/{env}/labels\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ee57e1-0d17-4b96-bdc9-ce32d2d2e9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env='prod'\n",
    "size='small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b7a8c8-e1fc-45d9-a8ff-72d9fabc2474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/22 07:02:06 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 54.75096869468689 seconds ---\n"
     ]
    }
   ],
   "source": [
    "loadAndCheckpoint('bro', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa96722-457e-4ccb-a7fd-3858e33acd04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 367.31977105140686 seconds ---\n"
     ]
    }
   ],
   "source": [
    "loadAndCheckpoint('car-bro', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d8a17-0c40-49ce-b71b-81429239956f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=>                                                (1046 + 16) / 50536]\r"
     ]
    }
   ],
   "source": [
    "loadAndCheckpoint('car', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670f8f8-3d2b-41a0-a899-ab92c4d4656b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580eb82-aba7-41cb-9a7b-85cbe0149014",
   "metadata": {},
   "outputs": [],
   "source": [
    "size='medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df758b-e752-41ce-80ca-202b2292585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadAndCheckpoint('bro', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836379b5-2f93-47f8-86a4-814b5da767b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadAndCheckpoint('car-bro', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdda15-61ac-4270-81b1-8330478c3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadAndCheckpoint('car', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f552b78-75d3-4f41-bbd1-2083e9a5f6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ca7bc-2556-42e6-932f-c12a45afa318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size='large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f420f-a783-44f8-8662-640832808949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadAndCheckpoint('bro', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d1c44-cbfb-4aa3-bd46-eee8dddb9fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadAndCheckpoint('car-bro', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa18d64-c723-4858-a8c3-47052a95bef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadAndCheckpoint('car', env=env, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211f3bf-4ad5-428a-a35c-cce2027bd6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
