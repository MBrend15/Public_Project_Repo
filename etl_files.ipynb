{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970130b0-b08c-4536-936e-ca923cc595a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup based on this: https://t-redactyl.io/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html\n",
    "import boto3\n",
    "import json \n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, isnan, when, count, col\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker_pyspark\n",
    "import botocore.session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef5d6-368f-43af-b519-ba5112c87b79",
   "metadata": {},
   "source": [
    "## Set Spark Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8e9008-6651-4c17-8c12-eef2b18e10fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7486ea00-f477-4f51-bf6e-b66a57f1a557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(\n",
    "    SecretId='sapient-s3-access'\n",
    ")\n",
    "response = json.loads(response['SecretString'])\n",
    "access_key = response[\"aws_access_key_id\"]\n",
    "secret_key = response[\"aws_secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372553c2-9f17-4c4f-b253-275e30a1ef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9392e443-f6d6-4a54-bbb8-2972eafaf97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/16 02:18:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/16 02:18:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/02/16 02:18:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', access_key)\n",
    "    .config('fs.s3a.secret.key', secret_key)\n",
    "    .config('spark.network.timeout', 300)\n",
    "    .config('spark.memory.offHeap.size','4g')\n",
    "    .config('spark.executor.memory', '16g')\n",
    "    .appName(\"sapient\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0acc-5281-4b73-a33b-67ac7c05b96a",
   "metadata": {},
   "source": [
    "## Functions to Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5e8ea5-b6a8-4fdd-84d5-22e3b31c7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from raw bucket + write to refined bucket + aggregate final to the trusted bucket\n",
    "s3_url_raw = \"s3a://sapient-bucket-raw/\"\n",
    "s3_url_refined = \"s3a://sapient-bucket-refined/\"\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "bro_cols_conn = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "                 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "bro_cols_rep = ['ts', 'level', 'message', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dff8a6f-87ed-4b8c-a119-9ceeaaaa9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecarbro.json, AIA-1-25.ecar.json, conn.09_00_00-10_00_00.log\n",
    "def s3_file(file):\n",
    "    dev_file = f\"{s3_url_raw}/{env}/{file}\"\n",
    "    prod_file = \"\"\n",
    "    if env == \"prod\":\n",
    "      filename = prod_file\n",
    "    else:\n",
    "      filename = dev_file\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948047f5-3308-49ad-b9c7-f281aabb8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndCheckpoint(type):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro, labels\n",
    "    This function reads a file from json or log text and writes it as a parquet.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    if type == 'ecar':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'ecar-bro':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        # this will extract and flatten nested properties column\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'bro':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/**/**/*.log\", sep=\"\\t\", comment=\"#\", header=False)\n",
    "        df = df.limit(1000)\n",
    "        df = df.toDF(*bro_cols_conn)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'labels':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/{type}/*.csv\", sep=\",\", header=True)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee57e1-0d17-4b96-bdc9-ce32d2d2e9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7a8c8-e1fc-45d9-a8ff-72d9fabc2474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa96722-457e-4ccb-a7fd-3858e33acd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53879374-d9e3-490d-99d2-905878d27948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/16 02:18:49 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(118 + 2) / 120]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/16 02:20:19 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "--- 91.13822054862976 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = spark.read.json(f\"{s3_url_raw}/dev/ecar/evaluation/23Sep19-red/AIA-1-25/AIA-1-25.ecar.json\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a585447-d68b-4eb2-9e01-c054b9187f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>(118 + 2) / 120]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 84.66692614555359 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = spark.read.format(\"json\").load(f\"{s3_url_raw}/dev/ecar/evaluation/23Sep19-red/AIA-1-25/AIA-1-25.ecar.json\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6168070d-17b2-4993-9d18-646781acdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca818e3e-f8b2-412b-9d64-7f19d9a19344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(s3_url_raw)\n",
    "prefix=\"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a9f4a94-09b7-4b6d-8a8c-331eb88080cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/towards-data-engineering/get-keys-inside-an-s3-bucket-at-the-subfolder-level-7be42d858372\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :type bucket: str\n",
    "    :param prefix: Only fetch objects whose key starts with this prefix (optional).\n",
    "    :type prefix: tuple, list, str\n",
    "    :param suffix: Only fetch objects whose keys end with this suffix (optional).\n",
    "    :type suffix: str\n",
    "    :return: None\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    count = 0\n",
    "    files_list = []\n",
    "    \n",
    "    for key_prefix in prefixes:\n",
    "        for object_summary in my_bucket.objects.filter(Prefix=key_prefix):\n",
    "            key = object_summary.key\n",
    "            if key.endswith(suffix):\n",
    "                count += 1\n",
    "                files_list.append(key)\n",
    "    print(f\"count of total objects is {count}.\")\n",
    "    print(f\"guesstimated time is \" + str(round(2*count/60, 0)) + \" hours.\")\n",
    "    # print(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b2cc566-9eec-4b16-8dee-2fcc52f91630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of total objects is 527.\n",
      "guesstimated time is 18.0 hours.\n"
     ]
    }
   ],
   "source": [
    "get_matching_s3_objects(\"sapient-bucket-raw\", \"prod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e110259-43c2-4654-a564-1f5ab650273d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f02a4-e470-48bd-ab66-e52496443f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5d3db-0fcc-4fdb-acbf-45a904f9182e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27f4e106-0963-4074-815f-0540febb602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8804fd6-0d97-4fc1-87a2-e599135b4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_json_gz(s3client, bucket, key):\n",
    "    ''' download gzipped json file from s3 and convert to dict '''\n",
    "    response = s3client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read()\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(content), mode='rb') as fh:\n",
    "        return json.load(fh)# at the end of the file\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05632fba-9d61-424e-a095-938f01bd08ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'s3.ServiceResource' object has no attribute 'get_object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27088/2922330316.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbucketname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sapient-bucket-raw'\u001b[0m      \u001b[0;31m# input for your bucketname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3://sapient-bucket-raw/prod/ecar/evaluation/23Sep-night/AIA-501-525/AIA-501-525.ecar-2019-11-16T23-22-29.234.json.gz'\u001b[0m  \u001b[0;31m# input for your key on S3 (means S3 object fullpath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_json_gz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucketname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_27088/641209235.py\u001b[0m in \u001b[0;36mdownload_json_gz\u001b[0;34m(s3client, bucket, key)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_json_gz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m''' download gzipped json file from s3 and convert to dict '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 's3.ServiceResource' object has no attribute 'get_object'"
     ]
    }
   ],
   "source": [
    "bucketname = 'sapient-bucket-raw'      # input for your bucketname\n",
    "key = 's3://sapient-bucket-raw/prod/ecar/evaluation/23Sep-night/AIA-501-525/AIA-501-525.ecar-2019-11-16T23-22-29.234.json.gz'  # input for your key on S3 (means S3 object fullpath)\n",
    "actual = download_json_gz(s3, bucketname, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb6f07-08c7-4e9a-bf01-609bbff39d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6dcfd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_json_gz(s3client, bucket, key):\n",
    "    ''' download gzipped json file from s3 and convert to dict '''\n",
    "    response = s3client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read()\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(content), mode='rb') as fh:\n",
    "        return json.load(fh)# at the end of the file\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
