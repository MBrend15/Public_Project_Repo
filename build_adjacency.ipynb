{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d101505b-bd38-4008-a881-db5901ba8123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ba83b9fb-e18c-4119-8c8f-87e74a869269;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 234ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ba83b9fb-e18c-4119-8c8f-87e74a869269\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/10 15:16:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/10 15:16:59 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/03/10 15:17:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "%run ./read_file.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2fa8920-22ac-4196-aca7-56f47575d99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_adjaceny(df, dataset = 'ecar'):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if dataset == 'labels':\n",
    "        df = df.sort(col(\"ts2\").asc())\n",
    "    else : \n",
    "        #first step sort the data and check\n",
    "        df = df.sort(col(\"timestamp\").asc())\n",
    "    \n",
    "    #now get overlapping connections\n",
    "\n",
    "    #build collections of each type of ID\n",
    "    #TODO make this one select statement\n",
    "    col_objID = df.select(\"objectID\").distinct().collect()\n",
    "    col_actID = df.select(\"actorID\").distinct().collect()\n",
    "    \n",
    "    print(len(col_actID))\n",
    "\n",
    "    #put these columns into a dictionary and turn into a spark data frame\n",
    "    data = [{\"actorIDs\" : [row.actorID for row in col_actID], \\\n",
    "                   \"objectIDs\" : [row.objectID for row in col_objID]}]\n",
    "\n",
    "    id_intersect = spark.createDataFrame(data)\n",
    "\n",
    "    #compare ID pools to find actIDs that are nodes\n",
    "    id_intersect = id_intersect.withColumn(\"id_int\", \\\n",
    "                                           array_intersect(col(\"actorIDs\"),col(\"objectIDs\")))\n",
    "\n",
    "    #now build a list of the nodal actIDs and print the length\n",
    "    actIDs = [row.id_int for row in id_intersect.select(\"id_int\").collect()][0]\n",
    "    \n",
    "    print(\"length actIDs: \"+str(len(actIDs)))\n",
    "    print(\"elapsed time: \"+ str(time.time() - start_time))\n",
    "    #create ref dict\n",
    "    dict_node_edge_events = {}\n",
    "    count = 0 \n",
    "    \n",
    "    #for all actors in actIDs\n",
    "    for aID in actIDs: \n",
    "        \n",
    "        #I think we can pull parent time here and record this in the dictionary somehow\n",
    "        birth_event = df.where((col(\"objectID\") == aID) \\\n",
    "                               & (col(\"actorID\")!= aID)).limit(1).collect()\n",
    "        \n",
    "        if len(birth_event) > 0: \n",
    "            str_first_id = birth_event[0]['id']\n",
    "\n",
    "            #now add this to the dictionary as a key\n",
    "            first_event = str(aID+':'  \n",
    "                              +str(birth_event[0]['timestamp'])+\":\" \n",
    "                              +birth_event[0]['object']+\":\" \n",
    "                              +birth_event[0]['parent_image_path']+\":\" \n",
    "                              +birth_event[0]['image_path']+':USER_placeholder')\n",
    "            \n",
    "            dict_node_edge_events[first_event] = []\n",
    "\n",
    "            #now add all the events where the actorID is present but not the objectID, group by unique object ID\n",
    "            #and pull out the first event - and I checked and believe the sorting holds through grouping\n",
    "            col_select_children_events = df.select('objectID','id')\\\n",
    "            .where((col(\"actorID\") == aID) & \\\n",
    "                     (col(\"objectID\") != aID)).groupBy(\"objectID\").agg(first(\"id\")).collect() \n",
    "\n",
    "            #add children objects into dictioinary \n",
    "            dict_node_edge_events[first_event] = [str(row[\"objectID\"]+':'+row[\"first(id)\"]) for row in col_select_children_events]\n",
    "        else:\n",
    "            count += 1\n",
    "    \n",
    "    print(\"no birth event for \" + str(count)+\" events.\")\n",
    "    \n",
    "    # create a list of tuples from the dictionary\n",
    "    data_list = list(dict_node_edge_events.items())\n",
    "\n",
    "    # convert the list of tuples to a PySpark RDD\n",
    "    data_rdd = spark.sparkContext.parallelize(data_list)\n",
    "\n",
    "    # create a PySpark DataFrame from the RDD\n",
    "    df_final = data_rdd.toDF()\n",
    "    \n",
    "    #explode list into column, keeping position for some sort of rank just in case\n",
    "    df_final_exp = df_final.select(\"_1\",posexplode(\"_2\"))\n",
    "    df_final_exp_col = df_final_exp.selectExpr(\"_1 as parent\", \"pos as order\", \"col as child\")\n",
    "    \n",
    "    print(\"elapsed time 2: \"+ str(time.time() - start_time))\n",
    "    return df_final_exp_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25dbf1c-039f-4857-8358-a7d431725698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:17PM UTC on Mar 10, 2023 --- read and cache time: 1.7642974853515625e-05 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event count292367\n",
      "root\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_labels = readCheckpoint_bcm('labels')\n",
    "#create a timestamp column and drop the old timestamp - does not appear necessary\n",
    "df_labels = df_labels.withColumn('ts2', to_timestamp(col('timestamp'))).drop('timestamp')\n",
    "print('event count' + str(df_labels.count()))\n",
    "#check new col data types\n",
    "df_labels.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488225b9-7a54-41d8-a675-1e5c07e281d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SysClient0559.systemia.com\n",
      "SysClient0255.systemia.com\n",
      "SysClient0205.systemia.com\n",
      "SysClient0503.systemia.com\n",
      "SysClient0355.systemia.com\n",
      "SysClient0104.systemia.com\n",
      "SysClient0874.systemia.com\n",
      "SysClient0955.systemia.com\n",
      "SysClient0201.systemia.com\n",
      "SysClient0402.systemia.com\n",
      "SysClient0660.systemia.com\n",
      "SysClient0321.systemia.com\n",
      "SysClient0609.systemia.com\n",
      "SysClient0771.systemia.com\n",
      "SysClient0462.systemia.com\n",
      "SysClient0419.systemia.com\n",
      "SysClient0010.systemia.com\n",
      "SysClient0069.systemia.com\n",
      "SysClient0358.systemia.com\n",
      "SysClient0203.systemia.com\n",
      "SysClient0618.systemia.com\n",
      "SysClient0851.systemia.com\n",
      "SysClient0501.systemia.com\n",
      "SysClient0811.systemia.com\n",
      "SysClient0170.systemia.com\n",
      "SysClient0351.systemia.com\n",
      "SysClient0051.systemia.com\n"
     ]
    }
   ],
   "source": [
    "#what hosts are in malicious events\n",
    "col_hosts = df_labels.select('hostname').distinct().collect()\n",
    "for row in col_hosts:\n",
    "    print(row.hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e4d549-79df-482d-8acf-213e24da1286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#work on extracting parent time and subtracting from child time\n",
    "#get actIDs\n",
    "col_objID = df_labels.select(\"objectID\").distinct().collect()\n",
    "col_actID = df_labels.select(\"actorID\").distinct().collect()\n",
    "\n",
    "#put these columns into a dictionary and turn into a spark data frame\n",
    "data = [{\"actorIDs\" : [row.actorID for row in col_actID], \\\n",
    "               \"objectIDs\" : [row.objectID for row in col_objID]}]\n",
    "\n",
    "id_intersect = spark.createDataFrame(data)\n",
    "\n",
    "#compare ID pools to find actIDs that are nodes\n",
    "id_intersect = id_intersect.withColumn(\"id_int\", \\\n",
    "                                       array_intersect(col(\"actorIDs\"),col(\"objectIDs\")))\n",
    "\n",
    "#now build a list of the nodal actIDs and print the length\n",
    "actIDs = [row.id_int for row in id_intersect.select(\"id_int\").collect()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062333af-dfa8-4c6b-a026-bce821a35862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(hostname='SysClient0462.systemia.com', id='e15537fe-fdb0-49d4-a1fd-de34e8215617', objectID='d5846376-9596-4283-8505-8e6112abf1f2', actorID='bf4681e3-9ed7-4824-8612-b478b58a4347', object='PROCESS', action='CREATE', ts2=datetime.datetime(2019, 9, 23, 18, 44, 56, 834000))]\n"
     ]
    }
   ],
   "source": [
    "#now pull time and child_event for first actor\n",
    "first_act = actIDs[0]\n",
    "birth_event = df_labels.where((col(\"objectID\") == first_act) \\\n",
    "                        & (col(\"actorID\")!= first_act)).limit(1).collect()\n",
    "print(birth_event)\n",
    "#great so all the elements are in there, what does the string of the timestamp look like\n",
    "#it looks fine as a timestampe just turn it into a ts column in the adjaceny matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b0bf628-1b2c-4391-ad6e-bf86264aadfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5:27PM UTC on Mar 10, 2023 --- read and cache time: 0.4163367748260498 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#load in size medium\n",
    "df_epm = readCheckpoint(size='medium')\n",
    "list_mal_hosts = ['SysClient0559.systemia.com',\n",
    "'SysClient0255.systemia.com',\n",
    "'SysClient0205.systemia.com',\n",
    "'SysClient0503.systemia.com',\n",
    "'SysClient0355.systemia.com',\n",
    "'SysClient0104.systemia.com',\n",
    "'SysClient0874.systemia.com',\n",
    "'SysClient0955.systemia.com',\n",
    "'SysClient0201.systemia.com',\n",
    "'SysClient0402.systemia.com',\n",
    "'SysClient0660.systemia.com',\n",
    "'SysClient0321.systemia.com',\n",
    "'SysClient0609.systemia.com',\n",
    "'SysClient0771.systemia.com',\n",
    "'SysClient0462.systemia.com',\n",
    "'SysClient0419.systemia.com',\n",
    "'SysClient0010.systemia.com',\n",
    "'SysClient0069.systemia.com',\n",
    "'SysClient0358.systemia.com',\n",
    "'SysClient0203.systemia.com',\n",
    "'SysClient0618.systemia.com',\n",
    "'SysClient0851.systemia.com',\n",
    "'SysClient0501.systemia.com',\n",
    "'SysClient0811.systemia.com',\n",
    "'SysClient0170.systemia.com',\n",
    "'SysClient0351.systemia.com',\n",
    "'SysClient0051.systemia.com']\n",
    "df_epm = df_epm.filter(col(\"hostname\").isin(list_mal_hosts))\n",
    "\n",
    "# Create a new dataframe with distinct objectIDs to identify malcious ObjectIds\n",
    "df_malcious_objectIDs = df_labels.select('id').distinct()\n",
    "\n",
    "# Create a new column called 'malicious' in df_ecar to label malicious records\n",
    "df_epm = df_epm.withColumn('malicious', when(col('id').isin(df_malcious_objectIDs.rdd.flatMap(lambda x: x).collect()), 1).otherwise(0))\n",
    "\n",
    "#first let's pull a small section of the benign\n",
    "df_epm_benign = df_epm.where(col('malicious') == 0).limit(300000)\n",
    "df_epm_mal = df_epm.where(col('malicious') == 1).limit(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db8fc1d5-6de2-405f-8512-8f5b1bd22d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "length actIDs: 4\n",
      "elapsed time: 331.3237373828888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:================>                                       (24 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/10 17:42:55 ERROR Executor: Exception in task 28.0 in stage 46.0 (TID 527)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:772)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1618/1007908362.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "23/03/10 17:42:55 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 28.0 in stage 46.0 (TID 527),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:772)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1618/1007908362.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "23/03/10 17:42:55 ERROR TaskSetManager: Task 28 in stage 46.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:================>                                       (24 + 9) / 83]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o292587.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 28 in stage 46.0 failed 1 times, most recent failure: Lost task 28.0 in stage 46.0 (TID 527) (ip-172-16-62-81.us-west-2.compute.internal executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:772)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1618/1007908362.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:772)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1618/1007908362.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_samp \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_adjaceny\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_epm_mal\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 44\u001b[0m, in \u001b[0;36mbuild_adjaceny\u001b[0;34m(df, dataset)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#for all actors in actIDs\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m aID \u001b[38;5;129;01min\u001b[39;00m actIDs: \n\u001b[1;32m     41\u001b[0m     \n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m#I think we can pull parent time here and record this in the dictionary somehow\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     birth_event \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobjectID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maID\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 44\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactorID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maID\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(birth_event) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     47\u001b[0m         str_first_id \u001b[38;5;241m=\u001b[39m birth_event[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o292587.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 28 in stage 46.0 failed 1 times, most recent failure: Lost task 28.0 in stage 46.0 (TID 527) (ip-172-16-62-81.us-west-2.compute.internal executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:772)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1618/1007908362.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.hasNext(MemoryStore.scala:772)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1618/1007908362.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54654)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_samp = build_adjaceny(df_epm_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e158c9-cf44-4efe-baa7-bf4b9f52dd31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71350"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_adj.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f1b7c28-5564-4dd4-916a-384932228e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_adj.select('parent').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392aadd0-6729-4297-a7b2-ed8efd0c6a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71350"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels_adj.select('child').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0079b5-0027-4697-a5ac-56da0d874337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
