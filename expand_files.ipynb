{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970130b0-b08c-4536-936e-ca923cc595a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup based on this: https://t-redactyl.io/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html\n",
    "import boto3\n",
    "import json \n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, isnan, when, count, col\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker_pyspark\n",
    "import botocore.session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef5d6-368f-43af-b519-ba5112c87b79",
   "metadata": {},
   "source": [
    "## Set Spark Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8e9008-6651-4c17-8c12-eef2b18e10fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7486ea00-f477-4f51-bf6e-b66a57f1a557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(\n",
    "    SecretId='sapient-s3-access'\n",
    ")\n",
    "response = json.loads(response['SecretString'])\n",
    "access_key = response[\"aws_access_key_id\"]\n",
    "secret_key = response[\"aws_secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372553c2-9f17-4c4f-b253-275e30a1ef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9392e443-f6d6-4a54-bbb8-2972eafaf97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/19 16:13:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', access_key)\n",
    "    .config('fs.s3a.secret.key', secret_key)\n",
    "    .config('spark.network.timeout', 300)\n",
    "    .config('spark.memory.offHeap.size','4g')\n",
    "    .config('spark.executor.memory', '16g')\n",
    "    .appName(\"sapient\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0acc-5281-4b73-a33b-67ac7c05b96a",
   "metadata": {},
   "source": [
    "## Functions to Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5e8ea5-b6a8-4fdd-84d5-22e3b31c7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from raw bucket + write to refined bucket + aggregate final to the trusted bucket\n",
    "s3_url_raw = \"s3a://sapient-bucket-raw/\"\n",
    "s3_url_refined = \"s3a://sapient-bucket-refined/\"\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "bro_cols_conn = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "                 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "bro_cols_rep = ['ts', 'level', 'message', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948047f5-3308-49ad-b9c7-f281aabb8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndCheckpoint(type):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro, labels\n",
    "    This function reads a file from json or log text and writes it as a parquet.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    if type == 'ecar':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'ecar-bro':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        # this will extract and flatten nested properties column\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'bro':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/**/**/*.log\", sep=\"\\t\", comment=\"#\", header=False)\n",
    "        df = df.limit(1000)\n",
    "        df = df.toDF(*bro_cols_conn)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'labels':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/{type}/*.csv\", sep=\",\", header=True)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee57e1-0d17-4b96-bdc9-ce32d2d2e9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7a8c8-e1fc-45d9-a8ff-72d9fabc2474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa96722-457e-4ccb-a7fd-3858e33acd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53879374-d9e3-490d-99d2-905878d27948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.814697265625e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# df = spark.read.json(f\"{s3_url_raw}/dev/ecar/evaluation/23Sep19-red/AIA-1-25/AIA-1-25.ecar.json\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a585447-d68b-4eb2-9e01-c054b9187f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.123283386230469e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# df = spark.read.format(\"json\").load(f\"{s3_url_raw}/dev/ecar/evaluation/23Sep19-red/AIA-1-25/AIA-1-25.ecar.json\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6168070d-17b2-4993-9d18-646781acdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca818e3e-f8b2-412b-9d64-7f19d9a19344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(s3_url_raw)\n",
    "prefix=\"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a9f4a94-09b7-4b6d-8a8c-331eb88080cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/towards-data-engineering/get-keys-inside-an-s3-bucket-at-the-subfolder-level-7be42d858372\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :type bucket: str\n",
    "    :param prefix: Only fetch objects whose key starts with this prefix (optional).\n",
    "    :type prefix: tuple, list, str\n",
    "    :param suffix: Only fetch objects whose keys end with this suffix (optional).\n",
    "    :type suffix: str\n",
    "    :return: None\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    count = 0\n",
    "    files_list = []\n",
    "    \n",
    "    for key_prefix in prefixes:\n",
    "        for object_summary in my_bucket.objects.filter(Prefix=key_prefix):\n",
    "            key = object_summary.key\n",
    "            if key.endswith(suffix):\n",
    "                count += 1\n",
    "                files_list.append(key)\n",
    "    print(f\"count of total objects is {count}.\")\n",
    "    print(f\"guesstimated time is \" + str(round(178*count/60/60, 0)) + \" hours.\")\n",
    "    return count, files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f02a4-e470-48bd-ab66-e52496443f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5d3db-0fcc-4fdb-acbf-45a904f9182e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27f4e106-0963-4074-815f-0540febb602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import gzip\n",
    "import boto3\n",
    "import shutil\n",
    "from boto3.s3.transfer import TransferConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d082d7ef-55a5-4097-856e-31e7cfd29756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = TransferConfig(multipart_threshold=1024 * 300, \n",
    "                        max_concurrency=10,\n",
    "                        multipart_chunksize=1024 * 300,\n",
    "                        use_threads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8804fd6-0d97-4fc1-87a2-e599135b4117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/aws-s3-multipart-upload-download-using-boto3-python-sdk-2dedb0945f11\n",
    "# https://stackoverflow.com/questions/48466421/python-how-to-decompress-a-gzip-file-to-an-uncompressed-file-on-disk\n",
    "def expand_json_gz(bucket, key):\n",
    "    ''' download gzipped json file from s3, expand, and send to s3 '''\n",
    "    s3 = boto3.resource('s3')\n",
    "    client = boto3.client('s3')\n",
    "    tmp_loc = f'/home/ec2-user/SageMaker/tmp/{key}'\n",
    "    new_dir = os.path.dirname(tmp_loc)\n",
    "    start_time = time.time()\n",
    "    try: \n",
    "        os.makedirs(new_dir)\n",
    "    except:\n",
    "        pass\n",
    "    # download gz file\n",
    "    response = s3.meta.client.download_file(Bucket=bucket, Key=key, Filename=tmp_loc)\n",
    "    exp_loc = tmp_loc.replace(\".gz\", \"\")\n",
    "    key_exp = key.replace(file_pre, \"\").replace(\".gz\", \"\")\n",
    "    print(exp_loc)\n",
    "    with gzip.open(tmp_loc, 'r') as f_in: \n",
    "        with open(exp_loc, 'wb') as f_out:\n",
    "            try:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "                response = client.upload_file(\n",
    "                                Filename = exp_loc,\n",
    "                                Bucket=bucket,\n",
    "                                Key= 'prod/'+ key_exp.lstrip(file_pre),\n",
    "                                Config=config\n",
    "        )\n",
    "                print(\"file upload complete\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"failed file: \" + file_pre)\n",
    "                pass\n",
    "    try: \n",
    "        os.remove(tmp_loc)\n",
    "        os.remove(exp_loc)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e033e1-bae8-40af-84e5-18a540ec481d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04bc58e1-25d4-477e-a6c6-85c1a8a5ce09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_pre = \"pre_prod/\"\n",
    "file_prod = \"prod\"\n",
    "log_type = \"bro\"\n",
    "bucket = \"sapient-bucket-raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d329490-63dc-4130-a5d4-8ff7ccd2ec50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05632fba-9d61-424e-a095-938f01bd08ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/tmp/pre_prod/ecar/benign/18-19Sep19/AIA-451-475/AIA-451-475.ecar-2019-12-07T01-28-46.139.json\n",
      "Not a gzipped file (b'<!')\n",
      "failed file: pre_prod/\n",
      "--- 0.12856626510620117 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Single file test expansion\n",
    "bucketname = 'sapient-bucket-raw'      # input for your bucketname\n",
    "key = 'pre_prod/ecar/benign/18-19Sep19/AIA-451-475/AIA-451-475.ecar-2019-12-07T01-28-46.139.json.gz'  # input for your key on S3 (means S3 object fullpath)\n",
    "actual = expand_json_gz(bucketname, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb6f07-08c7-4e9a-bf01-609bbff39d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f29cdc-0041-42e8-bc9e-88f3483a0f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16cd74bd-64a5-4d7e-a3cd-a96031fdba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time at 4:52AM UTC on Feb 20, 2023\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_matching_s3_objects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m completed \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ec2-user/SageMaker/tmp/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m s3_count, s3_files \u001b[38;5;241m=\u001b[39m \u001b[43mget_matching_s3_objects\u001b[49m(bucket \u001b[38;5;241m=\u001b[39m bucket, prefix \u001b[38;5;241m=\u001b[39m file_pre \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecar/\u001b[39m\u001b[38;5;124m\"\u001b[39m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m s3_files:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m completed:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_matching_s3_objects' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"start time at\" + time.strftime('%l:%M%p %Z on %b %d, %Y'))\n",
    "# ignore completed files previously logged\n",
    "infile = r\"/home/ec2-user/SageMaker/sapient/expand_files.log\"\n",
    "fs = \".json\"\n",
    "\n",
    "with open(infile) as f:\n",
    "    f = f.readlines()\n",
    "    \n",
    "completed = [x.replace(\"/home/ec2-user/SageMaker/tmp/\", \"\").replace(\"\\n\", \"\") for x in f if \".json\" in x]\n",
    "start_time = time.time()\n",
    "s3_count, s3_files = get_matching_s3_objects(bucket = bucket, prefix = file_pre + \"ecar/\", suffix=\"gz\")\n",
    "for f in s3_files:\n",
    "    if f.replace(\".gz\", \"\") not in completed:\n",
    "        print(f)\n",
    "        s3_count -= 1\n",
    "    # expand_json_gz(bucketname, f)\n",
    "    # print(\"There are \" + str(s3_count) + \" files remaining to convert\")\n",
    "print(\"total time was  --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2150e66-7137-427f-8b2c-bca8bb39f6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d65127a-59c9-4376-a6b2-bee6ae07fe29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n"
     ]
    }
   ],
   "source": [
    "infiles = [r\"/home/ec2-user/SageMaker/sapient/expand_files.log\", r\"/home/ec2-user/SageMaker/sapient/expand_files.log1\",\n",
    "          r\"/home/ec2-user/SageMaker/sapient/expand_files.log2\"]\n",
    "corpus = []\n",
    "\n",
    "for infile in infiles:\n",
    "    with open(infile) as f:\n",
    "        f = f.readlines()\n",
    "    corpus = corpus + f\n",
    "\n",
    "# print(corpus)\n",
    "    \n",
    "# corpus = [*set(corpus)]\n",
    "completed = [x.replace(\"/home/ec2-user/SageMaker/tmp/\", \"\").replace(\"\\n\", \"\") for x in corpus if \".json\" in x]\n",
    "\n",
    "print(len(completed))\n",
    "# print(completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733d708-308c-4f6a-9db0-537639beabd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
