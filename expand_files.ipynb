{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970130b0-b08c-4536-936e-ca923cc595a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup based on this: https://t-redactyl.io/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html\n",
    "import boto3\n",
    "import json \n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, isnan, when, count, col\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker_pyspark\n",
    "import botocore.session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef5d6-368f-43af-b519-ba5112c87b79",
   "metadata": {},
   "source": [
    "## Set Spark Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e9008-6651-4c17-8c12-eef2b18e10fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486ea00-f477-4f51-bf6e-b66a57f1a557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(\n",
    "    SecretId='sapient-s3-access'\n",
    ")\n",
    "response = json.loads(response['SecretString'])\n",
    "access_key = response[\"aws_access_key_id\"]\n",
    "secret_key = response[\"aws_secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372553c2-9f17-4c4f-b253-275e30a1ef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392e443-f6d6-4a54-bbb8-2972eafaf97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/17 23:02:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/17 23:02:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', access_key)\n",
    "    .config('fs.s3a.secret.key', secret_key)\n",
    "    .config('spark.network.timeout', 300)\n",
    "    .config('spark.memory.offHeap.size','4g')\n",
    "    .config('spark.executor.memory', '16g')\n",
    "    .appName(\"sapient\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0acc-5281-4b73-a33b-67ac7c05b96a",
   "metadata": {},
   "source": [
    "## Functions to Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e8ea5-b6a8-4fdd-84d5-22e3b31c7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from raw bucket + write to refined bucket + aggregate final to the trusted bucket\n",
    "s3_url_raw = \"s3a://sapient-bucket-raw/\"\n",
    "s3_url_refined = \"s3a://sapient-bucket-refined/\"\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "bro_cols_conn = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "                 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "bro_cols_rep = ['ts', 'level', 'message', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948047f5-3308-49ad-b9c7-f281aabb8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAndCheckpoint(type):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro, labels\n",
    "    This function reads a file from json or log text and writes it as a parquet.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    if type == 'ecar':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'ecar-bro':\n",
    "        df = spark.read.json(f\"{s3_url_raw}/{env}/{type}/**/**/**/*.json\")\n",
    "        # this will extract and flatten nested properties column\n",
    "        df = df.limit(1000)\n",
    "        df = df.select(*df.columns, \"properties.*\").drop('properties')\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'bro':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/**/**/*.log\", sep=\"\\t\", comment=\"#\", header=False)\n",
    "        df = df.limit(1000)\n",
    "        df = df.toDF(*bro_cols_conn)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    elif type == 'labels':\n",
    "        df = spark.read.csv(f\"{s3_url_raw}/{env}/{type}/*.csv\", sep=\",\", header=True)\n",
    "        df.write.option(\"maxRecordsPerFile\", 100000).mode(\"overwrite\").parquet(f\"{s3_url_refined}/{env}/{type}\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee57e1-0d17-4b96-bdc9-ce32d2d2e9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7a8c8-e1fc-45d9-a8ff-72d9fabc2474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa96722-457e-4ccb-a7fd-3858e33acd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53879374-d9e3-490d-99d2-905878d27948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.00543212890625e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# df = spark.read.json(f\"{s3_url_raw}/dev/ecar/evaluation/23Sep19-red/AIA-1-25/AIA-1-25.ecar.json\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a585447-d68b-4eb2-9e01-c054b9187f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.4332275390625e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# df = spark.read.format(\"json\").load(f\"{s3_url_raw}/dev/ecar/evaluation/23Sep19-red/AIA-1-25/AIA-1-25.ecar.json\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168070d-17b2-4993-9d18-646781acdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca818e3e-f8b2-412b-9d64-7f19d9a19344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(s3_url_raw)\n",
    "prefix=\"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9f4a94-09b7-4b6d-8a8c-331eb88080cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/towards-data-engineering/get-keys-inside-an-s3-bucket-at-the-subfolder-level-7be42d858372\n",
    "def get_matching_s3_objects(bucket, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Generate objects in an S3 bucket.\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :type bucket: str\n",
    "    :param prefix: Only fetch objects whose key starts with this prefix (optional).\n",
    "    :type prefix: tuple, list, str\n",
    "    :param suffix: Only fetch objects whose keys end with this suffix (optional).\n",
    "    :type suffix: str\n",
    "    :return: None\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(prefix, str):\n",
    "        prefixes = (prefix, )\n",
    "    else:\n",
    "        prefixes = prefix\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    \n",
    "    count = 0\n",
    "    files_list = []\n",
    "    \n",
    "    for key_prefix in prefixes:\n",
    "        for object_summary in my_bucket.objects.filter(Prefix=key_prefix):\n",
    "            key = object_summary.key\n",
    "            if key.endswith(suffix):\n",
    "                count += 1\n",
    "                files_list.append(key)\n",
    "    print(f\"count of total objects is {count}.\")\n",
    "    print(f\"guesstimated time is \" + str(round(2*count/60, 0)) + \" hours.\")\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f02a4-e470-48bd-ab66-e52496443f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5d3db-0fcc-4fdb-acbf-45a904f9182e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27f4e106-0963-4074-815f-0540febb602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import gzip\n",
    "from boto3.s3.transfer import TransferConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d082d7ef-55a5-4097-856e-31e7cfd29756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = TransferConfig(multipart_threshold=1024 * 300, \n",
    "                        max_concurrency=10,\n",
    "                        multipart_chunksize=1024 * 300,\n",
    "                        use_threads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed5ee0c5-7847-49f5-9f5a-c9556d70a89a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProgressPercentage(object):\n",
    "        def __init__(self, filename):\n",
    "            self._filename = filename\n",
    "            self._size = float(os.path.getsize(filename))\n",
    "            self._seen_so_far = 0\n",
    "            self._lock = threading.Lock()\n",
    "\n",
    "        def __call__(self, bytes_amount):\n",
    "            # To simplify we'll assume this is hooked up\n",
    "            # to a single filename.\n",
    "            with self._lock:\n",
    "                self._seen_so_far += bytes_amount\n",
    "                percentage = (self._seen_so_far / self._size) * 100\n",
    "                sys.stdout.write(\n",
    "                    \"\\r%s  %s / %s  (%.2f%%)\" % (\n",
    "                        self._filename, self._seen_so_far, self._size,\n",
    "                        percentage))\n",
    "                sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8804fd6-0d97-4fc1-87a2-e599135b4117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/aws-s3-multipart-upload-download-using-boto3-python-sdk-2dedb0945f11\n",
    "# https://stackoverflow.com/questions/48466421/python-how-to-decompress-a-gzip-file-to-an-uncompressed-file-on-disk\n",
    "def expand_json_gz(bucket, key):\n",
    "    ''' download gzipped json file from s3, expand, and send to s3 '''\n",
    "    s3 = boto3.resource('s3')\n",
    "    client = boto3.client('s3')\n",
    "    tmp_loc = f'/home/ec2-user/SageMaker/tmp/{key}'\n",
    "    new_dir = os.path.dirname(tmp_loc)\n",
    "    try: \n",
    "        os.makedirs(new_dir)\n",
    "    except:\n",
    "        pass\n",
    "    # download gz file\n",
    "    response = s3.meta.client.download_file(Bucket=bucket, Key=key, Filename=tmp_loc)\n",
    "    exp_filename = tmp_loc.rstrip('.gz')\n",
    "    print(exp_filename)\n",
    "    with gzip.open(tmp_loc, 'r') as f_in, open(exp_filename, 'wb') as f_out:\n",
    "        response = client.upload_file(\n",
    "                            Filename = exp_filename,\n",
    "                            Bucket=bucket,\n",
    "                            Key= 'prod/'+ key.lstrip(file_pre),\n",
    "                            Config=config\n",
    "    )\n",
    "    print(\"file upload complete\")\n",
    "    try: \n",
    "        os.remove(tmp_loc)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e033e1-bae8-40af-84e5-18a540ec481d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04bc58e1-25d4-477e-a6c6-85c1a8a5ce09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of total objects is 233.\n",
      "guesstimated time is 8.0 hours.\n"
     ]
    }
   ],
   "source": [
    "file_pre = \"pre_prod/\"\n",
    "file_prod = \"prod\"\n",
    "log_type = \"bro\"\n",
    "bucket = \"sapient-bucket-raw\"\n",
    "s3_files = get_matching_s3_objects(bucket, file_pre+log_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da1c6b-a80a-4135-81e6-319986192b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d329490-63dc-4130-a5d4-8ff7ccd2ec50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05632fba-9d61-424e-a095-938f01bd08ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/tmp/pre_prod/ecar/evaluation/23Sep-night/AIA-501-525/AIA-501-525.ecar-2019-11-16T23-22-29.234.json\n",
      "file upload complete\n"
     ]
    }
   ],
   "source": [
    "bucketname = 'sapient-bucket-raw'      # input for your bucketname\n",
    "key = 'pre_prod/ecar/evaluation/23Sep-night/AIA-501-525/AIA-501-525.ecar-2019-11-16T23-22-29.234.json.gz'  # input for your key on S3 (means S3 object fullpath)\n",
    "actual = expand_json_gz(bucketname, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb6f07-08c7-4e9a-bf01-609bbff39d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f29cdc-0041-42e8-bc9e-88f3483a0f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ab525-c3a6-41cf-af20-65a19895cd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2f26b-dc5a-4e44-9135-656e6f811899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd74bd-64a5-4d7e-a3cd-a96031fdba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in s3_files:\n",
    "    print(f.lstrip(file_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670f8f8-3d2b-41a0-a899-ab92c4d4656b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
