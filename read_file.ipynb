{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970130b0-b08c-4536-936e-ca923cc595a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setup based on this: https://t-redactyl.io/blog/2020/08/reading-s3-data-into-a-spark-dataframe-using-sagemaker.html\n",
    "import os\n",
    "import boto3\n",
    "import json \n",
    "import time\n",
    "import ntpath\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef5d6-368f-43af-b519-ba5112c87b79",
   "metadata": {},
   "source": [
    "## Set Spark Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8e9008-6651-4c17-8c12-eef2b18e10fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7486ea00-f477-4f51-bf6e-b66a57f1a557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(\n",
    "    SecretId='sapient-s3-access'\n",
    ")\n",
    "response = json.loads(response['SecretString'])\n",
    "access_key = response[\"aws_access_key_id\"]\n",
    "secret_key = response[\"aws_secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372553c2-9f17-4c4f-b253-275e30a1ef35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.8.2-spark3.2-s_2.12 pyspark-shell'\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9392e443-f6d6-4a54-bbb8-2972eafaf97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://spark.apache.org/docs/latest/configuration.html#memory-management\u001b[39;00m\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mSparkSession\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(conf\u001b[38;5;241m=\u001b[39mconf) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfs.s3a.access.key\u001b[39m\u001b[38;5;124m'\u001b[39m, access_key)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfs.s3a.secret.key\u001b[39m\u001b[38;5;124m'\u001b[39m, secret_key)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.network.timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.local.dir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ec2-user/SageMaker/tmp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m90g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.memory.offHeap.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.memory.offHeap.size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.maxResultSize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msapient\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/configuration.html#memory-management\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf) \\\n",
    "    .config('fs.s3a.access.key', access_key)\n",
    "    .config('fs.s3a.secret.key', secret_key)\n",
    "    .config('spark.network.timeout', 300)\n",
    "    .config('spark.local.dir', '/home/ec2-user/SageMaker/tmp')\n",
    "    .config(\"spark.executor.memory\", \"90g\")\n",
    "    .config(\"spark.driver.memory\", \"50g\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\",\"20g\")\n",
    "    .config(\"spark.driver.maxResultSize\",\"5g\")\n",
    "    \n",
    "    .appName(\"sapient\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f0acc-5281-4b73-a33b-67ac7c05b96a",
   "metadata": {},
   "source": [
    "## Functions to Load and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5e8ea5-b6a8-4fdd-84d5-22e3b31c7340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read from raw bucket + write to refined bucket + aggregate final to the trusted bucket\n",
    "s3_url_raw = \"s3a://sapient-bucket-raw/\"\n",
    "s3_url_refined = \"s3a://sapient-bucket-refined/\"\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "bro_cols_conn = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "                 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "bro_cols_rep = ['ts', 'level', 'message', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f40b57-7603-413d-881e-fd397b934503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def readCheckpoint(type='ecar', env='prod', size='small'):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro\n",
    "    \"\"\"\n",
    "    if type == 'labels':\n",
    "        s3_parquet_loc = f\"{s3_url_trusted}/{env}/{type}\"\n",
    "    else:\n",
    "        s3_parquet_loc = f\"{s3_url_trusted}/{env}/{type}/{size}\"\n",
    "    start_time = time.time()\n",
    "    df = spark.read.parquet(s3_parquet_loc).cache()\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and cache time: %s seconds ---\" % (time.time() - start_time))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc1dc1f3-183b-48e2-b545-3caae0a75888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def readCheckpoint_bcm(type='ecar-bro', env='dev', size='small'):\n",
    "    \"\"\"\n",
    "    type: ecar, ecar-bro, bro\n",
    "    \"\"\"\n",
    "    if type == 'labels' or type == 'ecar':\n",
    "        s3_parquet_loc = f\"{s3_url_refined}/{env}/{type}\"\n",
    "        df = spark.read.parquet(s3_parquet_loc).cache()\n",
    "    else:\n",
    "        s3_parquet_loc = f\"{s3_url_refined}/{env}/{type}/{size}\"\n",
    "    start_time = time.time()\n",
    "    #df = spark.select('id').read.parquet(s3_parquet_loc).cache()\n",
    "    # rdd = spark.sparkContext.parallelize(df.take(1000))\n",
    "    # print(f\"Your dataframe has {rdd.count():,} rows.\")\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read and cache time: %s seconds ---\" % (time.time() - start_time))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de5ed7a-7277-4986-9d57-f13861775ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firsts(df):\n",
    "    \"\"\"\n",
    "    this creates then drops duplicates and gets the first appearance of each relationship entry\n",
    "    input - dataframe with columns objectID and actorID\n",
    "    output dataframe\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy(\"relationship\").orderBy(\"timestamp\")\n",
    "    df_new = df.withColumn('relationship', concat(df.actorID, lit('->'),df.objectID) ) \\\n",
    "                .withColumn('rank', rank().over(window)) \\\n",
    "                .filter(col('rank') == 1) \\\n",
    "                .drop('rank') \\\n",
    "                .cache()\n",
    "    df.unpersist()\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243746a1-15c9-4527-8a12-79b7756bc5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFile(str):\n",
    "    \"\"\"\n",
    "    udf to get the file from a full file path\n",
    "    similar udf (non-windows): https://stackoverflow.com/questions/40848681/udf-to-extract-only-the-file-name-from-path-in-spark-sql\n",
    "    \"\"\"\n",
    "    if str == None:\n",
    "        pass\n",
    "    else:\n",
    "        new_str = ntpath.basename(str)\n",
    "        return new_str\n",
    "getFileUDF = udf(lambda z: getFile(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a953d736-b7a8-47c8-9ab2-121fee7dc8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def darpaColFeatures(df):\n",
    "    df_new = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/first_events\")\\\n",
    "                .withColumn(\"image_path\", getFileUDF(col(\"image_path\"))) \\\n",
    "                .withColumn(\"parent_image_path\", getFileUDF(col(\"parent_image_path\"))) \\\n",
    "                .withColumn(\"new_path\", getFileUDF(col(\"new_path\"))) \\\n",
    "                .withColumn(\"file_path\", getFileUDF(col(\"file_path\"))) \\\n",
    "                .cache()\n",
    "    df.unpersist()\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d469ce5d-f298-4abc-866a-9af4853e1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFirstEvents():\n",
    "    start_time = time.time()\n",
    "    df = spark.read.parquet(f\"{s3_url_trusted}/prod/final_features\")\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read time: %s seconds ---\" % (time.time() - start_time))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fd7f68d-5202-4e04-9896-cd3d53e7b2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkFirstEventRead(day = 23):\n",
    "    start_time = time.time()\n",
    "    df = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/first_events\").filter(col('event_day')==day)\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y') + \" --- read time: %s seconds ---\" % (time.time() - start_time))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
