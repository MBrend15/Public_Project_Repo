{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d33c4c-00c3-4100-aa35-4f2211512928",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: graphframes in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.6)\n",
      "Requirement already satisfied: nose in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphframes) (1.3.7)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphframes) (1.22.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c8c42cb8-d438-4266-b998-ddf54a1eba19;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 286ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c8c42cb8-d438-4266-b998-ddf54a1eba19\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 19:31:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/08 19:31:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb8e467-de12-4c58-bdb9-0fddedacc43c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "req_cols = ['file_path_ext','image_path_ext', 'parent_path_ext','object','action','timestamp_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31af1971-3fd1-4efa-bb1b-0e7a906713ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:09AM UTC on Apr 06, 2023 --- read time: 4.475607395172119 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3756864\n",
      "read in all data and count: 32.920527935028076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = de_dupe().cache()\n",
    "print(df.count())\n",
    "print(\"read in all data and count: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41610741-21e8-4183-98ed-ce5a4317dff2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- event_minute: integer (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- file_path_ext: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e309ea5c-4714-43a6-b8be-a0cce18c2e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/dedupe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69002d3e-81d4-4665-8884-84b4017e77bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, timestamp: timestamp, objectID: string, actorID: string, object: string, action: string, hostname: string, user_name: string, privileges: string, image_path: string, parent_image_path: string, new_path: string, file_path: string, direction: string, logon_id: string, requesting_domain: string, requesting_user: string, event_minute: int, event_day: int, event_hour: int, malicious: int, file_path_ext: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a82169d-a0e9-415c-b027-7404063c055e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/dedupe\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead52406-3ddf-4b4b-8cb5-694bfb891349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================================> (33 + 1) / 34]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3756864\n",
      "write, read, and count: 50.82806396484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(\"write, read, and count: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73474f14-1058-425b-8680-ebb60b89607c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:====>                                                 (17 + 32) / 200]\r"
     ]
    }
   ],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ebad3d-e3e6-4794-a6f8-1f5488894810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.42069172859191895\n",
      "filtered connections: 0.4339885711669922\n",
      "graph: 0.6944127082824707\n"
     ]
    }
   ],
   "source": [
    "#now create graph with motif 2, create bins that you will use for all ts diff, and bin\n",
    "#create two motifs at a time, join, unpersist, write, then unpersist\n",
    "start_time = time.time()\n",
    "df_2 = create_graph2(df).cache()\n",
    "print(\"graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86930633-fecc-47d4-ac91-a7e0cd9d9f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30613703\n",
      "count: 30.060638189315796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(df_2.count())\n",
    "print(\"count: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df21d9ca-4f52-466e-ae74-55d669f0ebfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1730209.0001106262, 8254118.999958038, 21626879.999876022, 44756893.000125885]\n",
      "binning: 250.15912413597107\n"
     ]
    }
   ],
   "source": [
    "#create ts_bins that you will use for all subsequent binning\n",
    "'''start_time = time.time()\n",
    "df_2 = ts_diff(df_2).cache()\n",
    "bins = df_2.approxQuantile(\"timestamp_difference\", [0.2, 0.4, .6, .8], 0)\n",
    "print(bins)\n",
    "print(\"binning: \"+ str(time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae90cac-ecc3-48fe-a734-a2c4e8dad2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create graph and explode: 0.3021578788757324\n",
      "bin time: 0.6034109592437744\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#explode\n",
    "df_2 = transp_expl2(df_2).cache()\n",
    "df_2 = df_2.withColumn('motif',lit(2)).cache()\n",
    "print(\"create graph and explode: \"+ str(time.time() - start_time))\n",
    "df_2 = bin_it(df_2).cache()\n",
    "print(\"bin time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "770092b4-fd3c-4406-b50b-1c6506cbc009",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: string (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- file_path_ext: string (nullable = true)\n",
      " |-- motif: integer (nullable = false)\n",
      " |-- timestamp_bins: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5f7816-3a90-44ad-b1c8-4a2038da6b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 421.1579158306122\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "df_2.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/2\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a34cb0-3430-417b-bd55-05f56408feb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, event: string, src: string, dst: string, timestamp: timestamp, object: string, action: string, hostname: string, user_name: string, privileges: string, image_path: string, parent_image_path: string, new_path: string, file_path: string, direction: string, logon_id: string, requesting_domain: string, requesting_user: string, malicious: int, file_path_ext: string, motif: int, timestamp_bins: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d1296b-187a-4892-89ec-7369aa063c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.12255430221557617\n",
      "graph: 0.26387548446655273\n"
     ]
    }
   ],
   "source": [
    "#now df3\n",
    "start_time = time.time()\n",
    "df_3 = create_graph3(df).cache()\n",
    "print(\"graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a6bf88-7fb2-43e2-aaf1-8e297b637f04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:=====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110105995\n",
      "count: 194.46526455879211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(df_3.count())\n",
    "print(\"count: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c03cd5a-49f6-4764-8b52-35a01cd283d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transp-explode: 0.16888213157653809\n",
      "binned: 0.29276418685913086\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_3 = transp_expl3(df_3).cache()\n",
    "df_3 = df_3.withColumn('motif',lit(3)).cache()\n",
    "print(\"transp-explode: \"+ str(time.time() - start_time))\n",
    "df_3 = bin_it(df_3).cache()\n",
    "print(\"binned: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288f6902-b56b-4c12-af27-c1581e7f3d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time (np): 1485.1701052188873\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "df_3.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/3\")\n",
    "print(\"write time (np): \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "766e1114-3bb4-4175-8714-1a35de280435",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, event: string, src: string, dst: string, timestamp: timestamp, object: string, action: string, hostname: string, user_name: string, privileges: string, image_path: string, parent_image_path: string, new_path: string, file_path: string, direction: string, logon_id: string, requesting_domain: string, requesting_user: string, malicious: int, file_path_ext: string, motif: int, timestamp_bins: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f79e955-b4b3-46a6-8a84-bce85593a6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.5519614219665527\n",
      "graph: 0.9262793064117432\n"
     ]
    }
   ],
   "source": [
    "#now df4\n",
    "start_time = time.time()\n",
    "df_4 = create_graph4(df).cache()\n",
    "print(\"graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7acd9d-2bcd-46d0-9857-07852fbee74d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109881292\n",
      "count: 222.0687873363495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(df_4.count())\n",
    "print(\"count: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58d0b72-5621-447f-afdc-7838ca979db8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transp-explode: 0.3355429172515869\n",
      "binned: 0.6755263805389404\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_4 = transp_expl4(df_4).cache()\n",
    "df_4 = df_4.withColumn('motif',lit(4)).cache()\n",
    "print(\"transp-explode: \"+ str(time.time() - start_time))\n",
    "df_4 = bin_it(df_4).cache()\n",
    "print(\"binned: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a9a1d9-da3a-4f48-b6a7-17c8a4ea070f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 1909.8718740940094\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "df_4.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/4\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab58b7d-ce8c-4002-986c-c22d09413f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, event: string, src: string, dst: string, timestamp: timestamp, object: string, action: string, hostname: string, user_name: string, privileges: string, image_path: string, parent_image_path: string, new_path: string, file_path: string, direction: string, logon_id: string, requesting_domain: string, requesting_user: string, malicious: int, file_path_ext: string, motif: int, timestamp_bins: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2320c6fa-2f37-491a-9c58-f2909be9e133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.24536418914794922\n",
      "graph: 0.4464895725250244\n"
     ]
    }
   ],
   "source": [
    "#now df6\n",
    "start_time = time.time()\n",
    "df_6 = create_graph6(df).cache()\n",
    "print(\"graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8029dae0-7306-4052-8313-0b4221d068f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:====================================================>(199 + 1) / 200]]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127702239\n",
      "count: 515.6148951053619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(df_6.count())\n",
    "print(\"count: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f2a4cf-02e4-4171-bb1b-b94328014a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transp-explode: 0.26151442527770996\n",
      "binned: 0.3914022445678711\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_6 = transp_expl(df_6).cache()\n",
    "df_6 = df_6.withColumn('motif',lit(6)).cache()\n",
    "print(\"transp-explode: \"+ str(time.time() - start_time))\n",
    "df_6 = bin_it(df_6).cache()\n",
    "print(\"binned: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42614fc5-6c4f-4800-ad82-3198130b50e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 3273.2002720832825\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "df_6.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/6\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c0fddb-980d-4094-99d6-ebdb80e4a24d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.6179800033569336\n",
      "create graph: 1.0455591678619385\n"
     ]
    }
   ],
   "source": [
    "#now df5\n",
    "start_time = time.time()\n",
    "df_5 = create_graph5(df).cache()\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82adb292-95e3-4908-ab31-d69890e13827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, event: string, src: string, dst: string, timestamp: timestamp, object: string, action: string, hostname: string, user_name: string, privileges: string, image_path: string, parent_image_path: string, new_path: string, file_path: string, direction: string, logon_id: string, requesting_domain: string, requesting_user: string, malicious: int, file_path_ext: string, motif: int, timestamp_bins: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6d3134-c232-4904-be17-046283dda63e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97166164\n",
      "count: 550.1845195293427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(df_5.count())\n",
    "print(\"count: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c04b10-a014-4124-9ea9-69b5a4cfaf34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transp-explode: 0.37686634063720703\n",
      "binned: 0.8165910243988037\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_5 = transp_expl5(df_5).cache()\n",
    "df_5 = df_5.withColumn('motif',lit(5)).cache()\n",
    "print(\"transp-explode: \"+ str(time.time() - start_time))\n",
    "df_5 = bin_it(df_5).cache()\n",
    "print(\"binned: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67abdfff-871c-404a-8c26-83c2ec47cb94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 2679.5069410800934\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "df_5.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/5\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f17b084-36e9-469a-98bc-939c18b6f046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, event: string, src: string, dst: string, timestamp: timestamp, object: string, action: string, hostname: string, user_name: string, privileges: string, image_path: string, parent_image_path: string, new_path: string, file_path: string, direction: string, logon_id: string, requesting_domain: string, requesting_user: string, malicious: int, file_path_ext: string, motif: int, timestamp_bins: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0b85b98-11d4-4963-ac21-231d227c79ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, timestamp: timestamp, objectID: string, actorID: string, object: string, action: string, hostname: string, user_name: string, privileges: string, image_path: string, parent_image_path: string, new_path: string, file_path: string, direction: string, logon_id: string, requesting_domain: string, requesting_user: string, event_minute: int, event_day: int, event_hour: int, malicious: int, file_path_ext: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8234a614-48ed-4dd0-ba50-3201a56469f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time all: 1.114145040512085\n"
     ]
    }
   ],
   "source": [
    "#read in and count all motifs\n",
    "start_time = time.time()\n",
    "df_all_motifs = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/motifs/*\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb86f37b-0c88-40f3-9eae-43f50fce5215",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.             (32 + 32) / 616]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#now count everything you read in\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_all_motifs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread time all: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#now count everything you read in\n",
    "print(df_all_motifs.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88dd2b8-58b6-41e2-b820-58598aa2cbd6",
   "metadata": {},
   "source": [
    "### now run all motifs through one hot and see what you get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67abaad-3ce7-4476-bd7e-52527486e8d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- src: string (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- malicious: integer (nullable = true)\n",
      " |-- file_path_ext: string (nullable = true)\n",
      " |-- motif: integer (nullable = true)\n",
      " |-- timestamp_bins: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all_motifs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abad104-f26a-4093-bb50-696b01c5eb45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>object</th>\n",
       "      <th>action</th>\n",
       "      <th>hostname</th>\n",
       "      <th>user_name</th>\n",
       "      <th>privileges</th>\n",
       "      <th>...</th>\n",
       "      <th>new_path</th>\n",
       "      <th>file_path</th>\n",
       "      <th>direction</th>\n",
       "      <th>logon_id</th>\n",
       "      <th>requesting_domain</th>\n",
       "      <th>requesting_user</th>\n",
       "      <th>malicious</th>\n",
       "      <th>file_path_ext</th>\n",
       "      <th>motif</th>\n",
       "      <th>timestamp_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100236-2</td>\n",
       "      <td>e1</td>\n",
       "      <td>12541710-ae34-495d-bac2-9fde87b1e4e0</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>2019-09-23 14:27:16.609</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\install\\MarkupSafe\\bui...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100236-2</td>\n",
       "      <td>e2</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>9fb3404f-50d8-4a42-b479-89b5d4e84f1f</td>\n",
       "      <td>2019-09-23 15:05:03.986</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100246-2</td>\n",
       "      <td>e1</td>\n",
       "      <td>61d0949f-cd87-439f-ac78-f2a8990596b6</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>2019-09-23 14:54:39.243</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Users\\khendricks\\AppDa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100246-2</td>\n",
       "      <td>e2</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>9fb3404f-50d8-4a42-b479-89b5d4e84f1f</td>\n",
       "      <td>2019-09-23 15:05:03.986</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100315-2</td>\n",
       "      <td>e1</td>\n",
       "      <td>2905ddbe-cfb8-4792-9bac-53940ae91234</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>2019-09-23 14:27:26.440</td>\n",
       "      <td>FILE</td>\n",
       "      <td>READ</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\ncr\\mantra\\runtime\\Lib...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100315-2</td>\n",
       "      <td>e2</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>9fb3404f-50d8-4a42-b479-89b5d4e84f1f</td>\n",
       "      <td>2019-09-23 15:05:03.986</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Trace event                                   src  \\\n",
       "0  100236-2    e1  12541710-ae34-495d-bac2-9fde87b1e4e0   \n",
       "1  100236-2    e2  65fdc95d-207b-4a4e-a630-2bf056b0d762   \n",
       "2  100246-2    e1  61d0949f-cd87-439f-ac78-f2a8990596b6   \n",
       "3  100246-2    e2  65fdc95d-207b-4a4e-a630-2bf056b0d762   \n",
       "4  100315-2    e1  2905ddbe-cfb8-4792-9bac-53940ae91234   \n",
       "5  100315-2    e2  65fdc95d-207b-4a4e-a630-2bf056b0d762   \n",
       "\n",
       "                                    dst               timestamp   object  \\\n",
       "0  65fdc95d-207b-4a4e-a630-2bf056b0d762 2019-09-23 14:27:16.609     FILE   \n",
       "1  9fb3404f-50d8-4a42-b479-89b5d4e84f1f 2019-09-23 15:05:03.986  PROCESS   \n",
       "2  65fdc95d-207b-4a4e-a630-2bf056b0d762 2019-09-23 14:54:39.243     FILE   \n",
       "3  9fb3404f-50d8-4a42-b479-89b5d4e84f1f 2019-09-23 15:05:03.986  PROCESS   \n",
       "4  65fdc95d-207b-4a4e-a630-2bf056b0d762 2019-09-23 14:27:26.440     FILE   \n",
       "5  9fb3404f-50d8-4a42-b479-89b5d4e84f1f 2019-09-23 15:05:03.986  PROCESS   \n",
       "\n",
       "  action                    hostname user_name privileges  ... new_path  \\\n",
       "0   READ  SysClient0419.systemia.com      None       None  ...     None   \n",
       "1   OPEN  SysClient0419.systemia.com      None       None  ...     None   \n",
       "2   READ  SysClient0419.systemia.com      None       None  ...     None   \n",
       "3   OPEN  SysClient0419.systemia.com      None       None  ...     None   \n",
       "4   READ  SysClient0419.systemia.com      None       None  ...     None   \n",
       "5   OPEN  SysClient0419.systemia.com      None       None  ...     None   \n",
       "\n",
       "                                           file_path direction logon_id  \\\n",
       "0  \\Device\\HarddiskVolume1\\install\\MarkupSafe\\bui...      None     None   \n",
       "1                                               None      None     None   \n",
       "2  \\Device\\HarddiskVolume1\\Users\\khendricks\\AppDa...      None     None   \n",
       "3                                               None      None     None   \n",
       "4  \\Device\\HarddiskVolume1\\ncr\\mantra\\runtime\\Lib...      None     None   \n",
       "5                                               None      None     None   \n",
       "\n",
       "  requesting_domain requesting_user malicious file_path_ext  motif  \\\n",
       "0              None            None         0         empty      2   \n",
       "1              None            None         0          None      2   \n",
       "2              None            None         0         empty      2   \n",
       "3              None            None         0          None      2   \n",
       "4              None            None         0         empty      2   \n",
       "5              None            None         0          None      2   \n",
       "\n",
       "  timestamp_bins  \n",
       "0            NaN  \n",
       "1            1.0  \n",
       "2            NaN  \n",
       "3            0.0  \n",
       "4            NaN  \n",
       "5            1.0  \n",
       "\n",
       "[6 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_motifs.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba12de9-12e0-4e27-a06a-d718972792b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I am now going to reduce the rows. Should this be a function, absolutely. Maybe later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5a32a-2b2a-42f1-9d8a-f5b01840d470",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start with pre-processing for overall df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06df0d4-4caf-41d7-80be-57b1aa339a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#so now pivot, groupBy Trace and return first motif, src and destination\n",
    "#first pivot aka transpose and keep all events\n",
    "df_all_motifs = df_all_motifs.groupBy('Trace').pivot('event')\\\n",
    ".agg(first('src'),first('dst'), first('motif')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09659208-48ee-4ddb-86ae-0a1c29e7274f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#consolidate motif columns by finding the average or mode of each column\n",
    "df_all_motifs = df_all_motifs.withColumn('motif', col('e1_first(motif)')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e38bb0b-dd48-4219-81f7-95277049c290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>e1_first(src)</th>\n",
       "      <th>e1_first(dst)</th>\n",
       "      <th>e1_first(motif)</th>\n",
       "      <th>e2_first(src)</th>\n",
       "      <th>e2_first(dst)</th>\n",
       "      <th>e2_first(motif)</th>\n",
       "      <th>e3_first(src)</th>\n",
       "      <th>e3_first(dst)</th>\n",
       "      <th>e3_first(motif)</th>\n",
       "      <th>e4_first(src)</th>\n",
       "      <th>e4_first(dst)</th>\n",
       "      <th>e4_first(motif)</th>\n",
       "      <th>e5_first(src)</th>\n",
       "      <th>e5_first(dst)</th>\n",
       "      <th>e5_first(motif)</th>\n",
       "      <th>e6_first(src)</th>\n",
       "      <th>e6_first(dst)</th>\n",
       "      <th>e6_first(motif)</th>\n",
       "      <th>motif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100016-2</td>\n",
       "      <td>944be8ff-30c7-46cf-841f-ba87a391f69f</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>2</td>\n",
       "      <td>65fdc95d-207b-4a4e-a630-2bf056b0d762</td>\n",
       "      <td>9fb3404f-50d8-4a42-b479-89b5d4e84f1f</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100074-5</td>\n",
       "      <td>e2bc274c-4bd6-411f-afcd-0353b8f9963e</td>\n",
       "      <td>fa89e9b4-2776-4ca2-8555-0c649b0e1f55</td>\n",
       "      <td>5</td>\n",
       "      <td>fa89e9b4-2776-4ca2-8555-0c649b0e1f55</td>\n",
       "      <td>273bfab2-2849-42d2-bf5e-7f01d185c792</td>\n",
       "      <td>5</td>\n",
       "      <td>273bfab2-2849-42d2-bf5e-7f01d185c792</td>\n",
       "      <td>d952da47-3aa0-41ca-87e8-94be47f78234</td>\n",
       "      <td>5.0</td>\n",
       "      <td>d952da47-3aa0-41ca-87e8-94be47f78234</td>\n",
       "      <td>3b587487-ead0-4209-a4b6-ad6561a96bfa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3b587487-ead0-4209-a4b6-ad6561a96bfa</td>\n",
       "      <td>14ac57f8-46ff-422b-bc8d-1f947099ee4c</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Trace                         e1_first(src)  \\\n",
       "0  100016-2  944be8ff-30c7-46cf-841f-ba87a391f69f   \n",
       "1  100074-5  e2bc274c-4bd6-411f-afcd-0353b8f9963e   \n",
       "\n",
       "                          e1_first(dst)  e1_first(motif)  \\\n",
       "0  65fdc95d-207b-4a4e-a630-2bf056b0d762                2   \n",
       "1  fa89e9b4-2776-4ca2-8555-0c649b0e1f55                5   \n",
       "\n",
       "                          e2_first(src)                         e2_first(dst)  \\\n",
       "0  65fdc95d-207b-4a4e-a630-2bf056b0d762  9fb3404f-50d8-4a42-b479-89b5d4e84f1f   \n",
       "1  fa89e9b4-2776-4ca2-8555-0c649b0e1f55  273bfab2-2849-42d2-bf5e-7f01d185c792   \n",
       "\n",
       "   e2_first(motif)                         e3_first(src)  \\\n",
       "0                2                                  None   \n",
       "1                5  273bfab2-2849-42d2-bf5e-7f01d185c792   \n",
       "\n",
       "                          e3_first(dst)  e3_first(motif)  \\\n",
       "0                                  None              NaN   \n",
       "1  d952da47-3aa0-41ca-87e8-94be47f78234              5.0   \n",
       "\n",
       "                          e4_first(src)                         e4_first(dst)  \\\n",
       "0                                  None                                  None   \n",
       "1  d952da47-3aa0-41ca-87e8-94be47f78234  3b587487-ead0-4209-a4b6-ad6561a96bfa   \n",
       "\n",
       "   e4_first(motif)                         e5_first(src)  \\\n",
       "0              NaN                                  None   \n",
       "1              5.0  3b587487-ead0-4209-a4b6-ad6561a96bfa   \n",
       "\n",
       "                          e5_first(dst)  e5_first(motif) e6_first(src)  \\\n",
       "0                                  None              NaN          None   \n",
       "1  14ac57f8-46ff-422b-bc8d-1f947099ee4c              5.0          None   \n",
       "\n",
       "  e6_first(dst)  e6_first(motif)  motif  \n",
       "0          None              NaN      2  \n",
       "1          None              NaN      5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_motifs_m.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d49ec9-895b-4854-9661-262670d54c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop rendundat motif columns\n",
    "df_all_motifs = df_all_motifs.drop('e1_first(motif)','e2_first(motif)','e3_first(motif)',\n",
    "                       'e4_first(motif)','e5_first(motif)','e6_first(motif)').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5008a9-5c02-4778-90c4-e3c0237a9fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop redundant src columns and keep the destination\n",
    "df_all_motifs = df_all_motifs.drop('e2_first(src)','e3_first(src)','e4_first(src)',\n",
    "                       'e5_first(src)','e6_first(src)').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96f9b33-5f8c-4676-b02f-987305c9570d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 5085.627703428268\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "df_all_motifs.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivoted\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aec50ed-a62a-4db6-8ea5-502103383597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, e4_first(dst): string, e5_first(dst): string, e6_first(dst): string, motif: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_motifs.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58920dc-b199-47f0-ae92-24c03ed1c92d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time all: 6.313919544219971\n"
     ]
    }
   ],
   "source": [
    "#read in and count all motifs\n",
    "start_time = time.time()\n",
    "df_all_motifs_piv = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivoted\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "862434f0-6390-40b1-acd5-5f82dd7fabcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681010386.8307817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>(227 + 2) / 229]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475469393\n",
      "read time all: 168.15877604484558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#read it back in\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "#now count everything you read in\n",
    "print(df_all_motifs_piv.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adcf2f-6f21-45b5-a710-1750a256c56a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Now let's start with trace length 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017fa76d-a4b8-45a2-8a3e-3456ad9008fb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-84576547-9e25-4bf9-9f4d-a508eb00dabb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 146ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-84576547-9e25-4bf9-9f4d-a508eb00dabb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 03:18:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 03:18:36 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d900a6-e390-4369-9956-6fd84ca4460d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3112fbc-e4b2-4116-974c-b5965042f726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#algorithm as follows (repeat for all trace length 6 through 3: \n",
    "#create int df, drop last column \n",
    "df_int_6 = df_all_motifs_piv.drop('e6_first(dst)').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ae40c7-86e1-4f8e-be4e-81f2192efbf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:======================================================>(228 + 1) / 229]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475469393\n",
      "int 6 count: 231.39778804779053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(df_int_6.count())\n",
    "print(\"int 6 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a689c8bf-0e70-4a5a-ac15-71289bed66e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#write pivint6 now\\nstart_time = time.time()\\nprint(start_time)\\ndf_int_6.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint6\")\\nprint(\"write time: \"+ str(time.time() - start_time))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#write pivint6 now\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_int_6.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint6\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ffcbae7-8840-4330-8f02-d6033894d20e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, e4_first(dst): string, e5_first(dst): string, e6_first(dst): string, motif: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_motifs_piv.unpersist()\n",
    "#df_int_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ec1a9-037d-4ad7-b746-43651f4712ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''#read in pivint6\n",
    "#I think I should read in the partial first.\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_int_6 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint6\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c29d41-d2a9-4391-8f65-8b2ade1e1466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''start = time.time()\n",
    "print(df_int_6.count())\n",
    "print(\"int 6 count: \"+ str(time.time() - start))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934ad86b-2f07-4ff8-8ad6-70614bddcd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int 6 concat: 0.07466006278991699\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#concat columns and compare to drop\n",
    "df_int_6 = df_int_6.withColumn('concat',concat(df_int_6.columns[1], df_int_6.columns[2], \n",
    "                                     df_int_6.columns[3], df_int_6.columns[4],\n",
    "                                                            df_int_6.columns[5],df_int_6.columns[6])).cache()\n",
    "print(\"int 6 concat: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a33fd9-8804-4244-8511-9fac74203458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================================================>(228 + 1) / 229]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475469393\n",
      "count: 205.18920969963074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(df_int_6.count())\n",
    "print(\"count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c743b325-5b1c-455f-b087-8c6aef986f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>e1_first(src)</th>\n",
       "      <th>e1_first(dst)</th>\n",
       "      <th>e2_first(dst)</th>\n",
       "      <th>e3_first(dst)</th>\n",
       "      <th>e4_first(dst)</th>\n",
       "      <th>e5_first(dst)</th>\n",
       "      <th>motif</th>\n",
       "      <th>concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1425929187402-5</td>\n",
       "      <td>158f7657-c0dd-4541-9ac4-6cf3a9e987f7</td>\n",
       "      <td>bf963665-a321-47e8-88b9-f4e9b9a13ac9</td>\n",
       "      <td>f3f37b3e-5565-48f5-ae3a-4c9583110734</td>\n",
       "      <td>c38cd8e6-553f-4814-898f-3d83eaf2b447</td>\n",
       "      <td>de4be40d-15b3-465a-8dc8-d18655de613e</td>\n",
       "      <td>201719d5-c2ed-43c6-be16-649c8374b14e</td>\n",
       "      <td>5</td>\n",
       "      <td>158f7657-c0dd-4541-9ac4-6cf3a9e987f7bf963665-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1425929187412-6</td>\n",
       "      <td>f89d404f-3530-4d25-b9b0-327b8f8c8b37</td>\n",
       "      <td>bf963665-a321-47e8-88b9-f4e9b9a13ac9</td>\n",
       "      <td>77a17f8a-1cdc-44e8-ac50-43ae85e69ce8</td>\n",
       "      <td>c38cd8e6-553f-4814-898f-3d83eaf2b447</td>\n",
       "      <td>de4be40d-15b3-465a-8dc8-d18655de613e</td>\n",
       "      <td>f3f37b3e-5565-48f5-ae3a-4c9583110734</td>\n",
       "      <td>6</td>\n",
       "      <td>f89d404f-3530-4d25-b9b0-327b8f8c8b37bf963665-a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trace                         e1_first(src)  \\\n",
       "0  1425929187402-5  158f7657-c0dd-4541-9ac4-6cf3a9e987f7   \n",
       "1  1425929187412-6  f89d404f-3530-4d25-b9b0-327b8f8c8b37   \n",
       "\n",
       "                          e1_first(dst)                         e2_first(dst)  \\\n",
       "0  bf963665-a321-47e8-88b9-f4e9b9a13ac9  f3f37b3e-5565-48f5-ae3a-4c9583110734   \n",
       "1  bf963665-a321-47e8-88b9-f4e9b9a13ac9  77a17f8a-1cdc-44e8-ac50-43ae85e69ce8   \n",
       "\n",
       "                          e3_first(dst)                         e4_first(dst)  \\\n",
       "0  c38cd8e6-553f-4814-898f-3d83eaf2b447  de4be40d-15b3-465a-8dc8-d18655de613e   \n",
       "1  c38cd8e6-553f-4814-898f-3d83eaf2b447  de4be40d-15b3-465a-8dc8-d18655de613e   \n",
       "\n",
       "                          e5_first(dst)  motif  \\\n",
       "0  201719d5-c2ed-43c6-be16-649c8374b14e      5   \n",
       "1  f3f37b3e-5565-48f5-ae3a-4c9583110734      6   \n",
       "\n",
       "                                              concat  \n",
       "0  158f7657-c0dd-4541-9ac4-6cf3a9e987f7bf963665-a...  \n",
       "1  f89d404f-3530-4d25-b9b0-327b8f8c8b37bf963665-a...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_6.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47b72b82-41ba-40cf-b7e1-05de1a61a0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680969408.7436364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int 6 col gen: 441.4695794582367\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "#now set the window. partition by concat and order by concat all for int mat, pull inv filter\n",
    "#if this step is too hot to handle then you can revert back to simply select columns etc\n",
    "window = Window.partitionBy(\"concat\").orderBy(col(\"motif\").desc())\n",
    "col_int = df_int_6.orderBy(col('concat'))\\\n",
    "            .withColumn(\"rank\", rank().over(window))\\\n",
    "                .filter((col(\"rank\") != 1) & (col('concat').isNotNull()))\\\n",
    "                .select('Trace').distinct().collect()\n",
    "print(\"int 6 col gen: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7770f643-876a-4e19-ac28-4bb642c4ee42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['661425149970-5', '300647924572-5', '146029139753-5', '154619094741-5', '1425929184918-5']\n"
     ]
    }
   ],
   "source": [
    "print([r.Trace for r in col_int[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d912b0ee-6de3-4d2d-ad82-b2342f652afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680970628.46869\n",
      "dataframe create: 223.79657220840454\n"
     ]
    }
   ],
   "source": [
    "#create df for anti-join\n",
    "start = time.time()\n",
    "print(start)\n",
    "#turn the col_int into a dataframe\n",
    "from pyspark.sql import Row\n",
    "R = Row('Trace')\n",
    "# use the row function to create a bunch of rows\n",
    "df_int = spark.createDataFrame([R(x.Trace) for x in col_int]).cache()\n",
    "print(\"dataframe create: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcd66a86-58cb-4654-99cc-16c51daaadc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680970852.2700748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17924097\n",
      "1\n",
      "df_int count: 2.3876726627349854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count for df_int\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int.count())\n",
    "print(len(col_int[0][:]))\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8322ce41-2d38-482f-b565-2047824b8697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680970854.6652005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 33.91192293167114\n"
     ]
    }
   ],
   "source": [
    "#write the int dataframe right now\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_int.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/filter_list\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3402105f-e39c-43c8-90e8-735f776a37e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#clear col_int\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_int\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m col_int\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:988\u001b[0m, in \u001b[0;36mDataFrame.unpersist\u001b[0;34m(self, blocking)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03mmemory and disk.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;124;03m`blocking` default has changed to ``False`` to match Scala in 2.0.\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 988\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "#clear col_int\n",
    "df_int.unpersist()\n",
    "col_int.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ddc41b-9e95-46c8-adf8-3bcc21c9ce9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681012096.8483539\n",
      "read time all: 0.4086313247680664\n"
     ]
    }
   ],
   "source": [
    "#now read in filter list, and then read in motif 5 data\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_filter = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/filter_list\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad4179a5-4951-4f2c-8a87-f4660899a61f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681012100.764311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                         (17 + 15) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17924097\n",
      "df_int count: 1.9081246852874756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count filter\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_filter.count())\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee32485-cd70-4311-93cd-2b0fea083ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680985633.1623528\n",
      "read time all: 0.45999789237976074\n"
     ]
    }
   ],
   "source": [
    "#try to filter motif 5, if not we may have an issue\n",
    "#read in motif 5\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_5 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/motifs/5\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d105b3ab-d816-436f-9d52-e69c8cac5725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680985633.627209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 152:====================================================>(134 + 2) / 136]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485830820\n",
      "motif 5 count: 214.52204751968384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_5.count())\n",
    "print(\"motif 5 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ee5e76-250f-47c2-a00b-28b70c82c37d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>257698242047-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1666447685536-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trace\n",
       "0   257698242047-5\n",
       "1  1666447685536-5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddb573dd-f207-4cc9-9b54-1c835022a501",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+--------------------+--------------------+--------------------+-------+------+--------------------+---------+----------+----------------+-----------------+--------+---------+---------+--------+-----------------+---------------+---------+-------------+-----+--------------+\n",
      "|          Trace|event|                 src|                 dst|           timestamp| object|action|            hostname|user_name|privileges|      image_path|parent_image_path|new_path|file_path|direction|logon_id|requesting_domain|requesting_user|malicious|file_path_ext|motif|timestamp_bins|\n",
      "+---------------+-----+--------------------+--------------------+--------------------+-------+------+--------------------+---------+----------+----------------+-----------------+--------+---------+---------+--------+-----------------+---------------+---------+-------------+-----+--------------+\n",
      "|1666447685536-5|   e1|d52980c4-b337-474...|f69acad0-defb-449...|2019-09-24 15:44:...|PROCESS|  OPEN|SysClient0874.sys...|     null|      null|        PING.EXE|            other|    null|     null|     null|    null|             null|           null|        0|         None|    5|          null|\n",
      "|1666447685536-5|   e2|f69acad0-defb-449...|7d85151f-be17-47a...|2019-09-24 15:44:...|PROCESS|  OPEN|SysClient0874.sys...|     null|      null|     conhost.exe|      conhost.exe|    null|     null|     null|    null|             null|           null|        0|         None|    5|           0.0|\n",
      "|1666447685536-5|   e3|7d85151f-be17-47a...|e0f068a9-7544-4d2...|2019-09-24 17:19:...|PROCESS|  OPEN|SysClient0874.sys...|     null|      null|       csrss.exe|        csrss.exe|    null|     null|     null|    null|             null|           null|        0|         None|    5|           1.0|\n",
      "|1666447685536-5|   e4|e0f068a9-7544-4d2...|1c0e9b14-cb91-444...|2019-09-24 17:21:...|PROCESS|  OPEN|SysClient0874.sys...|     null|      null|     svchost.exe|      svchost.exe|    null|     null|     null|    null|             null|           null|        0|         None|    5|           0.0|\n",
      "|1666447685536-5|   e5|1c0e9b14-cb91-444...|bb37be24-71ad-4e5...|2019-09-24 19:31:...|PROCESS|  OPEN|SysClient0874.sys...|     null|      null|GoogleUpdate.exe| GoogleUpdate.exe|    null|     null|     null|    null|             null|           null|        0|         None|    5|           1.0|\n",
      "+---------------+-----+--------------------+--------------------+--------------------+-------+------+--------------------+---------+----------+----------------+-----------------+--------+---------+---------+--------+-----------------+---------------+---------+-------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_motif_5.where(col('Trace') == '1666447685536-5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f740f08-9b28-4a12-b58a-c9f595a0b793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680985848.1536875\n",
      "df_motif_5 join: 0.04090309143066406\n"
     ]
    }
   ],
   "source": [
    "#filter motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "df_motif_5 = df_motif_5.join(df_filter, [\"Trace\"], \"leftanti\").cache()\n",
    "print(\"df_motif_5 join: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2cfebc4-3078-4ba8-a762-701cf3feae40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680985848.1987429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 157:====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396210335\n",
      "motif 5 count: 349.82933235168457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_5.count())\n",
    "print(\"motif 5 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f9ccc8a-c73a-4674-829a-1a0f75cc14ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680986198.0326545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 918.3402369022369\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_5.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/filtered/5\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9730ee7c-b283-4ee7-9e16-599ba8a9fdbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#before you read in df_5 you need to filter the df_int matrix\n",
    "#first separate the int dfs into motifs - 1 and all other motifs\n",
    "df_int_a = df_int_6.filter(col(\"motif\") == 5).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05dfa8e5-0821-406c-9312-7721af0f5e33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681011751.023592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(228 + 1) / 229]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97166164\n",
      "df_int_a count: 115.9735631942749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_a.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b87ece0-7666-433e-8566-f54257405190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_int_b = df_int_6.filter(col(\"motif\") < 5).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34985218-a24f-4b1d-892a-c534ad66f24d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681011867.0215585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=====================================================>(227 + 2) / 229]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250600990\n",
      "df_int_a count: 135.9872283935547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_b\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_b.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e86588-f2f8-4c68-b6b2-dc61d341c286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, e4_first(dst): string, e5_first(dst): string, motif: int, concat: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7a7d8d4-14b4-4944-b082-a9b9f26b93bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681012117.6044633\n",
      "antijoin: 0.02583456039428711\n"
     ]
    }
   ],
   "source": [
    "#filter int_a\n",
    "#now execute anti join\n",
    "start = time.time()\n",
    "print(start)\n",
    "df_int_a = df_int_a.join(df_filter, [\"Trace\"], \"leftanti\")\n",
    "print(\"antijoin: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30b27d0f-5c15-4dc2-97b9-2bdb44c76e40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681012122.1516697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:======================================================> (33 + 1) / 34]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79242067\n",
      "df_int_a count: 20.231421947479248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count a \n",
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_a.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afb10c85-e31b-4376-9f1b-1c37550f8a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#union all the ints, select on motif \n",
    "df_int_5 = df_int_a.union(df_int_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18c7757e-ee96-4be9-9841-9a49297d2a34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 1048.8404083251953\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_int_5.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint5\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3731da-c2fe-4702-bf32-ffcff5c44631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8ffaf-5de8-4915-9599-30696c8edf10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''#read in motif 5\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_5 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/motifs/5\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97a21d-ddf7-4899-bcfd-5f7328e5c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#count motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_5.count())\n",
    "print(\"df_int count: \"+ str(time.time() - start))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49c9ed-c850-4e22-b26e-8da029b7476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#read in and count all motifs\n",
    "start_time = time.time()\n",
    "df_all_motifs_piv = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivoted\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7cba0-2b98-46bc-baa3-b51b399fc5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#read it back in\n",
    "start_time = time.time()\n",
    "#now count everything you read in\n",
    "print(df_all_motifs_piv.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340f6d8-5e91-45f0-8a65-7e1e1c64f061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''start = time.time()\n",
    "#now filter big matrix and write it\n",
    "df_all_motifs_piv = df_all_motifs_piv.filter(~col(\"Trace\").isin(inv_filt))\n",
    "print(\"all filter: \"+ str(time.time() - start))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7b107-73e0-4944-8c05-2ee46d2258f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''start_time = time.time()\n",
    "df_all_motifs_piv.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/piv6\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a77931-1f7d-405b-8e9f-28c38dde5829",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Now run for 5, beginning by reading everything back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f60d57e-3934-47b2-97ee-bf47d532401f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a2154af2-4472-430c-9963-9a44e71528c9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.2-spark3.2-s_2.12!graphframes.jar (41ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (32ms)\n",
      ":: resolution report :: resolve 703ms :: artifacts dl 74ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a2154af2-4472-430c-9963-9a44e71528c9\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (281kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 14:07:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 14:07:57 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0ce4bc-67ca-4215-a0c7-23f794d45ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee2dd28-e9f9-4307-b1c0-6b45547f9898",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681049287.957085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time all: 8.510698318481445\n"
     ]
    }
   ],
   "source": [
    "#I think I should read in the partial first.\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_int_5 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint/pivint5\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1ff1d6-3e38-41fc-afbf-2cba4f1fefba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681049305.9767997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>(209 + 2) / 211]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329843057\n",
      "read time all: 134.19801354408264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(start_time)\n",
    "#now count everything you read in\n",
    "print(df_int_5.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e8ae23-1efb-4c30-a0a4-75964c975b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>e1_first(src)</th>\n",
       "      <th>e1_first(dst)</th>\n",
       "      <th>e2_first(dst)</th>\n",
       "      <th>e3_first(dst)</th>\n",
       "      <th>e4_first(dst)</th>\n",
       "      <th>motif</th>\n",
       "      <th>concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005022347378-5</td>\n",
       "      <td>91665c33-9ceb-4f5b-955e-e6fae3e70863</td>\n",
       "      <td>6257db09-fa0f-4160-bc17-97f22adc951c</td>\n",
       "      <td>a6e8c449-1e46-4092-afc9-4b88b2590bd0</td>\n",
       "      <td>add1b4c9-690e-407a-af00-6bf83e5ce7a8</td>\n",
       "      <td>7a0da822-d7fd-4705-9119-43a42d24f56e</td>\n",
       "      <td>5</td>\n",
       "      <td>91665c33-9ceb-4f5b-955e-e6fae3e708636257db09-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1005022347714-5</td>\n",
       "      <td>97b06ce8-279f-4ef7-ba55-1f61f2efbf3f</td>\n",
       "      <td>cbd63108-5c09-4274-951d-b3bacbc2a696</td>\n",
       "      <td>c48e49bb-7819-4726-a0a3-6dd67e0d361a</td>\n",
       "      <td>add1b4c9-690e-407a-af00-6bf83e5ce7a8</td>\n",
       "      <td>7a0da822-d7fd-4705-9119-43a42d24f56e</td>\n",
       "      <td>5</td>\n",
       "      <td>97b06ce8-279f-4ef7-ba55-1f61f2efbf3fcbd63108-5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trace                         e1_first(src)  \\\n",
       "0  1005022347378-5  91665c33-9ceb-4f5b-955e-e6fae3e70863   \n",
       "1  1005022347714-5  97b06ce8-279f-4ef7-ba55-1f61f2efbf3f   \n",
       "\n",
       "                          e1_first(dst)                         e2_first(dst)  \\\n",
       "0  6257db09-fa0f-4160-bc17-97f22adc951c  a6e8c449-1e46-4092-afc9-4b88b2590bd0   \n",
       "1  cbd63108-5c09-4274-951d-b3bacbc2a696  c48e49bb-7819-4726-a0a3-6dd67e0d361a   \n",
       "\n",
       "                          e3_first(dst)                         e4_first(dst)  \\\n",
       "0  add1b4c9-690e-407a-af00-6bf83e5ce7a8  7a0da822-d7fd-4705-9119-43a42d24f56e   \n",
       "1  add1b4c9-690e-407a-af00-6bf83e5ce7a8  7a0da822-d7fd-4705-9119-43a42d24f56e   \n",
       "\n",
       "   motif                                             concat  \n",
       "0      5  91665c33-9ceb-4f5b-955e-e6fae3e708636257db09-f...  \n",
       "1      5  97b06ce8-279f-4ef7-ba55-1f61f2efbf3fcbd63108-5...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick peek at what the data looks like \n",
    "df_int_5.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec6fa8a-c912-43da-a253-be359ff5bc05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================================>(209 + 2) / 211]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|motif|\n",
      "+-----+\n",
      "|    5|\n",
      "|    3|\n",
      "|    4|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_int_5.select('motif').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f47c48c4-9754-489b-a9b6-5e85de4f208a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first drop the last column and the current motif\n",
    "df_int_5 = df_int_5.drop('e5_first(dst)').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4528c1-d932-4cf4-acf3-0338892350d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681049688.5393791\n",
      "int 5 concat: 0.037262916564941406\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "#concat columns and compare to drop\n",
    "df_int_5 = df_int_5.withColumn('concat',concat(df_int_5.columns[1], df_int_5.columns[2], \n",
    "                                     df_int_5.columns[3], df_int_5.columns[4],df_int_5.columns[5])).cache()\n",
    "print(\"int 5 concat: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "866a7858-0196-4f6e-977c-0f324fe87ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================================>(210 + 1) / 211]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329843057\n",
      "read time all: 967.8283665180206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#now count everything you read in\n",
    "print(df_int_5.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6285abf4-e3a8-41e9-aaf4-5dac8545b875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681050661.6851473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int 5 col gen: 691.7900104522705\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "#now set the window. partition by concat and order by concat all for int mat, pull inv filter\n",
    "#if this step is too hot to handle then you can revert back to simply select columns etc\n",
    "window = Window.partitionBy(\"concat\").orderBy(col(\"motif\").desc())\n",
    "col_int = df_int_5.orderBy(col('concat'))\\\n",
    "            .withColumn(\"rank\", rank().over(window))\\\n",
    "                .filter((col(\"rank\") != 1) & (col('concat').isNotNull()))\\\n",
    "                .select('Trace').distinct().collect()\n",
    "print(\"int 5 col gen: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ddb1b9c-1993-4287-9bd4-ece6d09cbcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051353.4798877\n",
      "dataframe create: 153.89524483680725\n"
     ]
    }
   ],
   "source": [
    "#create df for anti-join\n",
    "start = time.time()\n",
    "print(start)\n",
    "#turn the col_int into a dataframe\n",
    "from pyspark.sql import Row\n",
    "R = Row('Trace')\n",
    "# use the row function to create a bunch of rows\n",
    "df_filter = spark.createDataFrame([R(x.Trace) for x in col_int]).cache()\n",
    "print(\"dataframe create: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feba180-94e2-4e9f-904c-f40b81d5de79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#count for df_int\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_filter.count())\n",
    "print(len(col_int[0]))\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "948296ff-951e-4ee7-bd48-f53fc8bc1963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051507.379963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 36.7403998374939\n"
     ]
    }
   ],
   "source": [
    "#write the int dataframe right now\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_filter.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/filters_all/filter_5\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a716a02a-66fc-455e-8c0a-e4f845f9f904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clear col_int\n",
    "df_filter.unpersist()\n",
    "col_int.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "678a8f94-6823-4fd2-862b-e859af98de67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051559.6483438\n",
      "read time all: 0.22326350212097168\n"
     ]
    }
   ],
   "source": [
    "#now read in filter list, and then read in motif 5 data\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_filter = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/filters_all/filter_5\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ef02306-96d1-4dda-bf0d-707414b2dd57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051564.0875297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:========================>                              (12 + 15) / 27]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12493038\n",
      "df_int count: 1.69136381149292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count filter\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_filter.count())\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319a299-c7ec-4d6b-819f-ae8b916ecab5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### now filter pivint, then filter motif 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a068c2c-8410-4b9f-9762-0e3ee0564840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#before you read in df_5 you need to filter the df_int matrix\n",
    "#first separate the int dfs into motifs - 1 and all other motifs\n",
    "df_int_a = df_int_5.filter(col(\"motif\") == 4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd73ae03-a1de-4b21-853d-a3962169b246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051581.0121644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>(210 + 1) / 211]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109881292\n",
      "df_int_a count: 121.39162993431091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_a.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6cfb6d9-c75d-4665-9e8d-e9d5cf5f5cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_int_b = df_int_5.filter(col(\"motif\") < 4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aecd39a-69eb-4912-b8c5-acb52f9f8655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051702.4279013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=====================================================>(208 + 3) / 211]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140719698\n",
      "df_int_a count: 79.7126522064209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_b\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_b.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a931be69-c05a-4f1f-9f8b-d66e72903d55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, e4_first(dst): string, motif: int, concat: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_5.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "626dc962-a626-4570-b710-3cecf245dcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051782.2638674\n",
      "antijoin: 0.02251887321472168\n"
     ]
    }
   ],
   "source": [
    "#filter int_a\n",
    "#now execute anti join\n",
    "start = time.time()\n",
    "print(start)\n",
    "df_int_a = df_int_a.join(df_filter, [\"Trace\"], \"leftanti\").cache()\n",
    "print(\"antijoin: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "867ed889-8810-4581-bbf6-46fd6f8cfa74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051782.2904072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:====================================================> (196 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97388254\n",
      "df_int_a count: 134.08898544311523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count a \n",
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_a.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f430827-5546-49a8-a25f-2f3f9276ea5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#union all the ints, select on motif \n",
    "df_int_4 = df_int_a.union(df_int_b).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bac358fa-6572-41c1-b0c7-0503f7592c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681051916.431659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=====================================================>(410 + 1) / 411]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238107952\n",
      "df_int_4 count: 162.86321139335632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_4.count())\n",
    "print(\"df_int_4 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "979ba262-9163-41b8-b349-5e5842b1a648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, e4_first(dst): string, motif: int, concat: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_a.unpersist()\n",
    "df_int_b.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465cd39a-0018-432e-b3ba-b790edff0991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 770.2307872772217\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_int_4.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint/pivint4\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cd65acd-c5f5-4330-b0b7-281c631456bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, e4_first(dst): string, motif: int, concat: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_4.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf6094-210d-42e9-955a-4addf994ce8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### now filter motif 4 all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1977230c-d258-4993-bf83-b2261d7129a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681052901.7389646\n",
      "read time all: 0.4986238479614258\n"
     ]
    }
   ],
   "source": [
    "#try to filter motif 5, if not we may have an issue\n",
    "#read in motif 5\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_4 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/motifs/4\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6941e566-d021-40b7-9969-7c2747a13eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681052902.2424214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=====================================================>(137 + 1) / 138]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439525168\n",
      "df_motif_4 count: 205.62926268577576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count dfunpersistf_4\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_4.count())\n",
    "print(\"df_motif_4 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e525ddde-4a41-4ec3-b4bd-af7905fdad90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681053107.8760726\n",
      "df_motif_4 join: 0.03316521644592285\n"
     ]
    }
   ],
   "source": [
    "#filter motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "df_motif_4 = df_motif_4.join(df_filter, [\"Trace\"], \"leftanti\").cache()\n",
    "print(\"df_motif_4 join: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ab6a62b-54c0-42ec-a139-da0a41b948b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681053107.9138927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389553016\n",
      "motif 5 count: 680.4210677146912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_4.count())\n",
    "print(\"motif 5 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6af6f3b-5137-4fd7-b500-e4a02becdfcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681053788.3395166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 949.3029878139496\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_4.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/filtered/4\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ff2242e-70a4-4b7e-9d06-8414968c4f7c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_motif_4.unpersist()\n",
    "df_filter.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4978d-b909-407e-8cd6-ba35668c2b46",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Now onto filtering trace 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333f2a68-7912-4517-8c19-2e4a6551b151",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b9ee02bc-8baa-4776-ad89-2c2d8ec19af6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 266ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b9ee02bc-8baa-4776-ad89-2c2d8ec19af6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:50:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:50:50 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run trace_encode.ipynb\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2a24ef-68f6-41be-a8fb-f4db663310e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681055622.52179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time all: 6.610005855560303\n"
     ]
    }
   ],
   "source": [
    "#I think I should read in the partial first.\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_int_4 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint/pivint4\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658d64fd-6a62-473e-a9fc-a322acd0fbd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681055631.5159247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>(174 + 1) / 175]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238107952\n",
      "read time all: 113.91786861419678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(start_time)\n",
    "#now count everything you read in\n",
    "print(df_int_4.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af306a7d-1812-4e32-8daf-05c0e1df3dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>e1_first(src)</th>\n",
       "      <th>e1_first(dst)</th>\n",
       "      <th>e2_first(dst)</th>\n",
       "      <th>e3_first(dst)</th>\n",
       "      <th>motif</th>\n",
       "      <th>concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005022347472-4</td>\n",
       "      <td>08a54ea7-2486-489b-b576-ace858b12d12</td>\n",
       "      <td>36c8e58b-a693-465f-9ebb-c05d0383aea6</td>\n",
       "      <td>743dc0ee-ab92-4e70-8add-bf533bfc233f</td>\n",
       "      <td>7a0da822-d7fd-4705-9119-43a42d24f56e</td>\n",
       "      <td>4</td>\n",
       "      <td>08a54ea7-2486-489b-b576-ace858b12d1236c8e58b-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1005022347574-4</td>\n",
       "      <td>00fb243e-bec7-4fb4-b133-9b337db5557b</td>\n",
       "      <td>6257db09-fa0f-4160-bc17-97f22adc951c</td>\n",
       "      <td>743dc0ee-ab92-4e70-8add-bf533bfc233f</td>\n",
       "      <td>7a0da822-d7fd-4705-9119-43a42d24f56e</td>\n",
       "      <td>4</td>\n",
       "      <td>00fb243e-bec7-4fb4-b133-9b337db5557b6257db09-f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trace                         e1_first(src)  \\\n",
       "0  1005022347472-4  08a54ea7-2486-489b-b576-ace858b12d12   \n",
       "1  1005022347574-4  00fb243e-bec7-4fb4-b133-9b337db5557b   \n",
       "\n",
       "                          e1_first(dst)                         e2_first(dst)  \\\n",
       "0  36c8e58b-a693-465f-9ebb-c05d0383aea6  743dc0ee-ab92-4e70-8add-bf533bfc233f   \n",
       "1  6257db09-fa0f-4160-bc17-97f22adc951c  743dc0ee-ab92-4e70-8add-bf533bfc233f   \n",
       "\n",
       "                          e3_first(dst)  motif  \\\n",
       "0  7a0da822-d7fd-4705-9119-43a42d24f56e      4   \n",
       "1  7a0da822-d7fd-4705-9119-43a42d24f56e      4   \n",
       "\n",
       "                                              concat  \n",
       "0  08a54ea7-2486-489b-b576-ace858b12d1236c8e58b-a...  \n",
       "1  00fb243e-bec7-4fb4-b133-9b337db5557b6257db09-f...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick peek at what the data looks like \n",
    "df_int_4.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba4eec8-e414-4c71-866d-b814ea2f6ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first drop the last column and the current motif\n",
    "df_int_4 = df_int_4.drop('e4_first(dst)').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801e9fb1-2263-4078-97d2-d5394eb75714",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>(173 + 2) / 175]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|motif|\n",
      "+-----+\n",
      "|    4|\n",
      "|    3|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_int_4.select('motif').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed3fd55-c453-4966-9a1f-741b8f677623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056140.2651806\n",
      "int 4 concat: 0.05903029441833496\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "#concat columns and compare to drop\n",
    "df_int_4 = df_int_4.withColumn('concat',concat(df_int_4.columns[1], df_int_4.columns[2], \n",
    "                                     df_int_4.columns[3], df_int_4.columns[4])).cache()\n",
    "print(\"int 4 concat: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5466c35e-7066-468b-8c2d-94dc24d669a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================================>(174 + 1) / 175]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238107952\n",
      "read time all: 144.27227783203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#now count everything you read in\n",
    "print(df_int_4.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae7c4d7-a3c9-4056-91de-61d3862cf629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056290.0372102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int 4 col gen: 149.01053094863892\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "#now set the window. partition by concat and order by concat all for int mat, pull inv filter\n",
    "#if this step is too hot to handle then you can revert back to simply select columns etc\n",
    "window = Window.partitionBy(\"concat\").orderBy(col(\"motif\").desc())\n",
    "col_int = df_int_4.orderBy(col('concat'))\\\n",
    "            .withColumn(\"rank\", rank().over(window))\\\n",
    "                .filter((col(\"rank\") != 1) & (col('concat').isNotNull()))\\\n",
    "                .select('Trace').distinct().collect()\n",
    "print(\"int 4 col gen: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a408b1b3-6911-4d4d-9289-ea3baef8547e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056439.0523725\n",
      "dataframe create: 212.71369338035583\n"
     ]
    }
   ],
   "source": [
    "#create df for anti-join\n",
    "start = time.time()\n",
    "print(start)\n",
    "#turn the col_int into a dataframe\n",
    "from pyspark.sql import Row\n",
    "R = Row('Trace')\n",
    "# use the row function to create a bunch of rows\n",
    "df_filter = spark.createDataFrame([R(x.Trace) for x in col_int]).cache()\n",
    "print(\"dataframe create: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9a650a2-dd6b-4df9-ae88-15b7866bc436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056651.7709193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                       (0 + 32) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17308235\n",
      "1\n",
      "df_int count: 4.661752223968506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count for df_int\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_filter.count())\n",
    "print(len(col_int[0]))\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9988f083-82d6-4c68-b290-a7284e6efee4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056656.4372032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 35.15904378890991\n"
     ]
    }
   ],
   "source": [
    "#write the int dataframe right now\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_filter.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/filters_all/filter_4\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34005a1f-97c2-4210-8baf-27189ee2b742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clear col_int\n",
    "df_filter.unpersist()\n",
    "col_int.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41730c6-7a99-4c99-905c-6313b7bd68eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056789.2313719\n",
      "read time all: 0.30702805519104004\n"
     ]
    }
   ],
   "source": [
    "#now read in filter list, and then read in motif 5 data\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_filter = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/filters_all/filter_4\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9aa749e-b57a-4a28-806f-705c67461908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056793.3436227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:========================================>               (23 + 9) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17308235\n",
      "df_int count: 2.88210129737854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count filter\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_filter.count())\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc3d89aa-194b-4849-99f0-11d1dfb97e27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          Trace|\n",
      "+---------------+\n",
      "|   8590410322-3|\n",
      "| 575526087924-3|\n",
      "|1331439993799-3|\n",
      "|  17180364305-3|\n",
      "| 420907511591-3|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cf524-7b0f-43a0-91d7-f1adba975489",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Now work on pivint4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e99d2b4-3ae7-40df-ab04-2c09513fb619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#before you read in df_5 you need to filter the df_int matrix\n",
    "#first separate the int dfs into motifs - 1 and all other motifs\n",
    "df_int_a = df_int_4.filter(col(\"motif\") == 3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b87d591-ed77-4658-a678-5c604b4efd94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056848.107922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=====================================================>(174 + 1) / 175]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110105995\n",
      "df_int_a count: 103.91281366348267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_a.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbc0c5dc-b319-4537-af40-708e4b99aae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_int_b = df_int_4.filter(col(\"motif\") < 3).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "895616cb-8a55-409e-bf50-d1d0f11a141c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056952.044404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=====================================================>(173 + 2) / 175]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30613703\n",
      "df_int_a count: 45.017478942871094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_b\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_b.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "053f6e63-48f6-43b2-a5aa-6d96396dcdaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, motif: int, concat: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_4.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c6e64f7-aec2-4ad8-9c22-4d807bcb0e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056997.1019576\n",
      "antijoin: 0.07093667984008789\n"
     ]
    }
   ],
   "source": [
    "#filter int_a\n",
    "#now execute anti join\n",
    "start = time.time()\n",
    "print(start)\n",
    "df_int_a = df_int_a.join(df_filter, [\"Trace\"], \"leftanti\").cache()\n",
    "print(\"antijoin: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eea51c49-8110-4a73-a8b7-7555db0f702e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681056997.1772232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:===================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92797760\n",
      "df_int_a count: 62.310394048690796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count a \n",
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_a.count())\n",
    "print(\"df_int_a count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13feb61b-208e-4a5e-89b3-f368e4f38d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#union all the ints, select on motif \n",
    "df_int_3 = df_int_a.union(df_int_b).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1391be8d-ae7a-48a9-98fa-427cdade95a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681057059.5383801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=====================================================>(374 + 1) / 375]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123411463\n",
      "df_int_3 count: 85.40104174613953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count df_int_a\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_int_3.count())\n",
    "print(\"df_int_3 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7377d38b-6218-4082-ae00-a1226ed59d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, motif: int, concat: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_a.unpersist()\n",
    "df_int_b.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5b4c72e-219f-4f4b-a5a7-d744ec14395f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 494.2364308834076\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_int_3.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint/pivint3\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f28638a4-bb24-4913-b16a-23413c44485c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string, e1_first(src): string, e1_first(dst): string, e2_first(dst): string, e3_first(dst): string, motif: int, concat: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_int_3.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51ec30-abae-4ffb-abd5-c3b33b897f03",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### now filter motif 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6712df96-d96a-4585-95fc-5430dc754e08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681057639.2233677\n",
      "read time all: 0.457775354385376\n"
     ]
    }
   ],
   "source": [
    "#try to filter motif 5, if not we may have an issue\n",
    "#read in motif 5\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_3 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/motifs/3\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15b11394-531b-4ccc-aafe-c46ebea0292f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681057639.685428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:=====================================================>(125 + 1) / 126]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330317985\n",
      "df_motif_3 count: 167.5418312549591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count dfunpersistf_4\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_3.count())\n",
    "print(\"df_motif_3 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cd59b49-6631-4cb1-9ff3-d3db9ba90c77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681057807.2315397\n",
      "df_motif_3 join: 0.03256702423095703\n"
     ]
    }
   ],
   "source": [
    "#filter motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "df_motif_3 = df_motif_3.join(df_filter, [\"Trace\"], \"leftanti\").cache()\n",
    "print(\"df_motif_3 join: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd702cf7-5c97-47ba-a38f-58f5e1dcb9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681057807.2682076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278393280\n",
      "motif 3 count: 219.56270575523376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_3.count())\n",
    "print(\"motif 3 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056eb060-12e6-463a-b9f0-a1ca17ffaebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681058026.8357575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 701.4553935527802\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_3.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/filtered/3\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f142071-78ab-4e67-982a-933ae1a123d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_motif_3.unpersist()\n",
    "df_filter.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeca01b-e872-4ad1-8358-59e2a7d8bf59",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Now do everything for trace 2!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcf3bce-a67e-41d0-a558-3584bb8595c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ac33e6c5-459d-4ab8-9ec1-7c85b803d786;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 157ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ac33e6c5-459d-4ab8-9ec1-7c85b803d786\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 16:48:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 16:48:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run trace_encode.ipynb\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79c5d5d-004d-4d29-9125-3930e3bc766f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681058935.9945955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read time all: 6.843519687652588\n"
     ]
    }
   ],
   "source": [
    "#I think I should read in the partial first.\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_int_3 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all_motifs/pivint/pivint3\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069336b8-637d-4fd5-8ca5-911b70bf4039",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681058942.8426425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================================>(115 + 1) / 116]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123411463\n",
      "read time all: 52.45708394050598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(start_time)\n",
    "#now count everything you read in\n",
    "print(df_int_3.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d79b627-2386-455c-8e0f-133251ccad3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>e1_first(src)</th>\n",
       "      <th>e1_first(dst)</th>\n",
       "      <th>e2_first(dst)</th>\n",
       "      <th>motif</th>\n",
       "      <th>concat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005022347312-3</td>\n",
       "      <td>a07fa153-c9f9-4c37-bd0c-ae41ff700f29</td>\n",
       "      <td>d47bfdd9-8b11-4fe5-bd8c-4fe8071140af</td>\n",
       "      <td>605ee76d-8fa6-4bf8-92da-8dcab1c33b43</td>\n",
       "      <td>3</td>\n",
       "      <td>a07fa153-c9f9-4c37-bd0c-ae41ff700f29d47bfdd9-8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1005022347353-3</td>\n",
       "      <td>282bfbdc-aafe-4326-b157-22643db6cd04</td>\n",
       "      <td>974bcc72-fb40-4907-b9e8-174954644deb</td>\n",
       "      <td>05597551-552e-4e4a-b888-2be93559b617</td>\n",
       "      <td>3</td>\n",
       "      <td>282bfbdc-aafe-4326-b157-22643db6cd04974bcc72-f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trace                         e1_first(src)  \\\n",
       "0  1005022347312-3  a07fa153-c9f9-4c37-bd0c-ae41ff700f29   \n",
       "1  1005022347353-3  282bfbdc-aafe-4326-b157-22643db6cd04   \n",
       "\n",
       "                          e1_first(dst)                         e2_first(dst)  \\\n",
       "0  d47bfdd9-8b11-4fe5-bd8c-4fe8071140af  605ee76d-8fa6-4bf8-92da-8dcab1c33b43   \n",
       "1  974bcc72-fb40-4907-b9e8-174954644deb  05597551-552e-4e4a-b888-2be93559b617   \n",
       "\n",
       "   motif                                             concat  \n",
       "0      3  a07fa153-c9f9-4c37-bd0c-ae41ff700f29d47bfdd9-8...  \n",
       "1      3  282bfbdc-aafe-4326-b157-22643db6cd04974bcc72-f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick peek at what the data looks like \n",
    "df_int_3.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778baa13-a076-4e78-871a-144d3b7c1244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first drop the last column and the current motif\n",
    "df_int_3 = df_int_3.drop('e3_first(dst)').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d0a98f-3e83-43b1-9ad9-46aad512802d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================> (113 + 3) / 116]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|motif|\n",
      "+-----+\n",
      "|    3|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_int_3.select('motif').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947c07e1-7dc3-46e8-b52c-9fc85cb38b28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059139.3101964\n",
      "int 3 concat: 0.054877281188964844\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "#concat columns and compare to drop\n",
    "df_int_3 = df_int_3.withColumn('concat',concat(df_int_3.columns[1], df_int_3.columns[2], \n",
    "                                     df_int_3.columns[3])).cache()\n",
    "print(\"int 3 concat: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a177a9e-2ad4-4d6d-9717-0946c1b7eff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================> (113 + 3) / 116]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123411463\n",
      "read time all: 80.91670417785645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#now count everything you read in\n",
    "print(df_int_3.count())\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb8029c-a153-41cd-9a49-1707c60efeff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059224.4690127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int 3 col gen: 60.258606910705566\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(start)\n",
    "#now set the window. partition by concat and order by concat all for int mat, pull inv filter\n",
    "#if this step is too hot to handle then you can revert back to simply select columns etc\n",
    "window = Window.partitionBy(\"concat\").orderBy(col(\"motif\").desc())\n",
    "col_int = df_int_3.orderBy(col('concat'))\\\n",
    "            .withColumn(\"rank\", rank().over(window))\\\n",
    "                .filter((col(\"rank\") != 1) & (col('concat').isNotNull()))\\\n",
    "                .select('Trace').distinct().collect()\n",
    "print(\"int 3 col gen: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15af644-37ab-4e1d-9874-c9f11b2cdc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059284.7325149\n",
      "dataframe create: 91.74432229995728\n"
     ]
    }
   ],
   "source": [
    "#create df for anti-join\n",
    "start = time.time()\n",
    "print(start)\n",
    "#turn the col_int into a dataframe\n",
    "from pyspark.sql import Row\n",
    "R = Row('Trace')\n",
    "# use the row function to create a bunch of rows\n",
    "df_filter = spark.createDataFrame([R(x.Trace) for x in col_int]).cache()\n",
    "print(\"dataframe create: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b94ef262-6679-4e0c-bc6d-1d64bad242d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059376.481606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                       (0 + 32) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7365254\n",
      "df_int count: 1.8421056270599365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count for df_int\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_filter.count())\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ff2715-4842-4f1f-b295-fcb8e04a6c02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059378.328299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 20.023906469345093\n"
     ]
    }
   ],
   "source": [
    "#write the int dataframe right now\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_filter.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/filters_all/filter_3\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "312e1103-7ad6-4026-b630-c36719bf9d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clear col_int\n",
    "df_filter.unpersist()\n",
    "col_int.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0469713-b8c0-47c5-8080-20e26761a4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059398.9610183\n",
      "read time all: 0.21618032455444336\n"
     ]
    }
   ],
   "source": [
    "#now read in filter list, and then read in motif 5 data\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_filter = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/filters_all/filter_3\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f5f73a-d3a1-4d6f-a77b-375ff2defdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059399.1816046\n",
      "7365254\n",
      "df_int count: 0.860105037689209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count filter\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_filter.count())\n",
    "print(\"df_int count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b98f450-3908-4e67-9c2c-c38fa355b5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          Trace|\n",
      "+---------------+\n",
      "| 781684082668-2|\n",
      "| 283467852112-2|\n",
      "| 661425042903-2|\n",
      "| 429496770170-2|\n",
      "|1176821133542-2|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7f644-16f6-42a0-9c54-c9992e9860ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### now filter motif 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cdb081b-3b67-45ce-9dd1-43f92844d9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059497.1049292\n",
      "read time all: 0.44818592071533203\n"
     ]
    }
   ],
   "source": [
    "#try to filter motif 5, if not we may have an issue\n",
    "#read in motif 5\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_2 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/motifs/2\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0054f9a3-75c4-429b-ab67-3730df0335f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059500.9240417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:===========================================>            (28 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61227406\n",
      "df_motif_2 count: 34.9476113319397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_2.count())\n",
    "print(\"df_motif_2 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e78edd8d-9b9a-4335-8e47-92fc77476c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059535.8761902\n",
      "df_motif_2 join: 0.07049059867858887\n"
     ]
    }
   ],
   "source": [
    "#filter motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "df_motif_2 = df_motif_2.join(df_filter, [\"Trace\"], \"leftanti\").cache()\n",
    "print(\"df_motif_2 join: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ee9aec3-afeb-4b9a-b47e-22c72ac7efbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059535.9509368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46496898\n",
      "motif 2 count: 28.631112575531006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count motif 5\n",
    "start = time.time()\n",
    "print(start)\n",
    "print(df_motif_2.count())\n",
    "print(\"motif 2 count: \"+ str(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3302dcce-4fec-4ace-bba0-29c7a7087df8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681059564.5869417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write time: 182.24507784843445\n"
     ]
    }
   ],
   "source": [
    "#write to parquet and unpersist\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "df_motif_2.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/motifs/filtered/2\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5242ee8-bc19-4d56-b1b3-e13f0116e54d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_motif_2.unpersist()\n",
    "df_filter.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dc4cb-c5f1-4381-8059-92a9c814060e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8ad56-cf09-48ce-9b9e-6c26c97995ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35b7b1-233b-4178-bd28-a2dd95599905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc2b71-693b-4c46-aaa2-d3278aac144e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598802e3-c36a-4f61-828f-17779ae1aaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724d1f4-55be-49a8-9b40-b0b150365234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0dda9-0369-44c2-9ee6-398803bf1379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fe6ad-b2c9-41cf-9459-a3f7f4388c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184f0e0-3894-499f-9a43-96a640d6754e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ea3f7-2278-4d95-9675-2d1359c04459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c3763-e5c3-46d2-ab63-1dba63647c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298a4d0-5e4d-4a66-8a25-709d98116f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c592fe1-9e9c-4441-b130-3abf13b5aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1e52a-71e7-404a-908d-349bfcee6c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c9524-e099-4436-97fe-e25ad71d962c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d1fad-0d39-4213-84a1-cf35dd202f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7572d85-f208-4888-a42d-f5dd29bf51fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56a54093-8f9f-4ac3-85ec-0242b1204dc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now one hot encoding! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8c3659a-a360-4e0d-880a-7632f06e928b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"inputCols\". Could not convert object to list of strings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:503\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:182\u001b[0m, in \u001b[0;36mTypeConverters.toListString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [TypeConverters\u001b[38;5;241m.\u001b[39mtoString(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m value]\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to list of strings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m value)\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert object to list of strings",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m list_sparse \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#attempting one hot without string indexer\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_all_motifs, dict_mapping, list_sparse \u001b[38;5;241m=\u001b[39m \u001b[43moneHotCol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_all_motifs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject onehot time: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "File \u001b[0;32m/tmp/ipykernel_13321/1122553425.py:17\u001b[0m, in \u001b[0;36moneHotCol\u001b[0;34m(df, colm, dict_mapping, cols_sparse)\u001b[0m\n\u001b[1;32m     12\u001b[0m sparse \u001b[38;5;241m=\u001b[39m colm\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_sparse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#indexer = StringIndexer(inputCol=colm, outputCol=num, handleInvalid=\"keep\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#indexer_fitted = indexer.fit(df)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#df = indexer_fitted.transform(df)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropLast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m encoder_fit \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit(df)\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m encoder_fit\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/feature.py:3191\u001b[0m, in \u001b[0;36mOneHotEncoder.__init__\u001b[0;34m(self, inputCols, outputCols, handleInvalid, dropLast, inputCol, outputCol)\u001b[0m\n\u001b[1;32m   3189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.feature.OneHotEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n\u001b[1;32m   3190\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m-> 3191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetParams\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/feature.py:3233\u001b[0m, in \u001b[0;36mOneHotEncoder.setParams\u001b[0;34m(self, inputCols, outputCols, handleInvalid, dropLast, inputCol, outputCol)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03msetParams(self, \\\\*, inputCols=None, outputCols=None, handleInvalid=\"error\", \\\u001b[39;00m\n\u001b[1;32m   3229\u001b[0m \u001b[38;5;124;03m          dropLast=True, inputCol=None, outputCol=None)\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;124;03mSets params for this OneHotEncoder.\u001b[39;00m\n\u001b[1;32m   3231\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3232\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[0;32m-> 3233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:505\u001b[0m, in \u001b[0;36mParams._set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             value \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mtypeConverter(value)\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 505\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramMap[p] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"inputCols\". Could not convert object to list of strings"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#instantiate dictionary and return df\n",
    "dict_mapping = {}\n",
    "#list of sparse cols\n",
    "list_sparse = []\n",
    "#attempting one hot without string indexer\n",
    "df_all_motifs, dict_mapping, list_sparse = oneHotCol(df_all_motifs,'object', dict_mapping, list_sparse)\n",
    "print(\"object onehot time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db97a1-6da0-48d9-bc02-87879710eac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_motifs.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b876f2-8dfe-414a-bc6f-d20db3e0543d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now write results\n",
    "start_time = time.time()\n",
    "df_all_motifs.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all/onehot\")\n",
    "print(\"write time: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f305b3-f39d-4f82-8f9f-938bd6e1131f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now unpersist\n",
    "df_all_motifs.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb2ba9-3c17-4108-bb39-30bf011de275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#and read back in\n",
    "start_time = time.time()\n",
    "df_all_motifs = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all/onehot\").cache()\n",
    "print(\"read time all: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d2a9c-7524-45c5-9a92-b0fb45894b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#and count \n",
    "start_time = time.time()\n",
    "print(df_all_motifs.count())\n",
    "print(\"count : \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6138b-93aa-4b99-9c37-335469425750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#and count \n",
    "start_time = time.time()\n",
    "print(df_all_motifs.count())\n",
    "print(\"count : \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee5742-e0f5-44d6-bebb-9498fc1d97ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc1377-72fc-4f11-a8cd-43a8d8efe63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_cols = ['file_path_ext','image_path',\n",
    "             'parent_image_path',\n",
    "             'action','timestamp_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6a0b4-436c-4a42-a874-bb68d17d2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then run for c in list cols above, one hot encode...then write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df0ac2-31d9-4946-8820-44b2b22bf4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11b9d3-5ac9-43cd-ab5a-112cac2e0f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a50a485-30e6-4c2a-82dd-d1348f5a2f70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union: 0.20996594429016113\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_all_motifs = df_2.union(df_3).union(df_4).union(df_5).union(df_6).cache()\n",
    "print(\"union: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "667a01ad-bb51-4e83-bb85-6b024a1b1a7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 606:>             (5 + 32) / 200][Stage 607:>              (0 + 0) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/3b/temp_shuffle_e4374012-b3bd-447a-bc17-4a0116768cf3, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/0f/temp_shuffle_81f49ba8-fdc7-4642-af17-d6db3bf38dfb, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/36/temp_shuffle_753e6fba-5c74-45ae-b60e-400c5a756ff2, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/26/temp_shuffle_5fb5cda8-88ff-4047-a8d5-639bbf201182, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/28/temp_shuffle_41d027c3-34ba-42d0-ae0f-2a63d6bf15cf, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/04/temp_shuffle_42408df6-f5a2-400c-b2a2-3287a30a81c8, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/37/temp_shuffle_f23dc0cf-2149-40c9-86cb-397007914cd1, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/3e/temp_shuffle_ff58a13d-555c-4f72-873c-c7e977e1c9c2, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/39/temp_shuffle_a3f28e17-8e30-4162-a94f-3553e8e6359e, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/36/temp_shuffle_6feb1799-bc9d-419a-b77c-6236fa8b92fc, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/2b/temp_shuffle_7f02ecdb-662f-45d2-b90b-d50804a4132e, No space left on device\n",
      "23/04/06 02:37:20 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /home/ec2-user/SageMaker/tmp/blockmgr-e4790cc7-50bb-4348-ba56-1bc5e220124d/32/temp_shuffle_27ca4aba-c88c-473b-a008-7c8215e21a1a, No space left on device\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 14.0 in stage 606.0 (TID 25701)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 16.0 in stage 606.0 (TID 25703)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 9.0 in stage 606.0 (TID 25696)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 10.0 in stage 606.0 (TID 25697)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 6.0 in stage 606.0 (TID 25693)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 12.0 in stage 606.0 (TID 25699)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 5.0 in stage 606.0 (TID 25692)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 18.0 in stage 606.0 (TID 25705)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 17.0 in stage 606.0 (TID 25704)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 13.0 in stage 606.0 (TID 25700)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 15.0 in stage 606.0 (TID 25702)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:20 ERROR Executor: Exception in task 7.0 in stage 606.0 (TID 25694)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/06 02:37:21 ERROR TaskSetManager: Task 10 in stage 606.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 606:=>                                                    (5 + 19) / 200]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o876.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 606.0 failed 1 times, most recent failure: Lost task 10.0 in stage 606.0 (TID 25697) (ip-172-16-94-2.us-west-2.compute.internal executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_all_motifs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall count: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o876.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 606.0 failed 1 times, most recent failure: Lost task 10.0 in stage 606.0 (TID 25697) (ip-172-16-94-2.us-west-2.compute.internal executor driver): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:542)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(df_all_motifs.count())\n",
    "print(\"all count: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864d08ad-06ab-4bb1-9d0d-63c5cb5c2933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2.unpersist()\n",
    "df_3.unpersist()\n",
    "df_4.unpersist()\n",
    "df_5.unpersist()\n",
    "df_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911c5b4-a835-4330-888f-ba94534d5d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 606:=>                                                    (5 + 18) / 200]\r"
     ]
    }
   ],
   "source": [
    "df_all_motifs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5280d4-fb96-4dc6-8a23-13a9b4957518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a motif column to partition by motifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37729c51-faa3-4243-a297-2b000fa02626",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:======================================>            (764 + 32) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 23:35:42 ERROR Executor: Exception in task 767.0 in stage 109.0 (TID 11313)\n",
      "java.io.IOException: No space left on device\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n",
      "\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n",
      "\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n",
      "\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n",
      "\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n",
      "\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n",
      "\tat java.nio.channels.Channels.access$000(Channels.java:61)\n",
      "\tat java.nio.channels.Channels$1.write(Channels.java:174)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n",
      "\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/04 23:35:42 ERROR Executor: Exception in task 793.0 in stage 109.0 (TID 11339)\n",
      "java.io.IOException: No space left on device\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n",
      "\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n",
      "\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n",
      "\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n",
      "\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n",
      "\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n",
      "\tat java.nio.channels.Channels.access$000(Channels.java:61)\n",
      "\tat java.nio.channels.Channels$1.write(Channels.java:174)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n",
      "\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/04 23:35:42 ERROR TaskSetManager: Task 767 in stage 109.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:======================================>            (764 + 29) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 23:35:42 ERROR FileFormatWriter: Aborting job 92c80e86-3d03-47d9-9101-17470447aff8.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 767 in stage 109.0 failed 1 times, most recent failure: Lost task 767.0 in stage 109.0 (TID 11313) (ip-172-16-94-2.us-west-2.compute.internal executor driver): java.io.IOException: No space left on device\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n",
      "\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n",
      "\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n",
      "\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n",
      "\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n",
      "\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n",
      "\tat java.nio.channels.Channels.access$000(Channels.java:61)\n",
      "\tat java.nio.channels.Channels$1.write(Channels.java:174)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n",
      "\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n",
      "\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n",
      "\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n",
      "\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n",
      "\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n",
      "\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n",
      "\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n",
      "\tat java.nio.channels.Channels.access$000(Channels.java:61)\n",
      "\tat java.nio.channels.Channels$1.write(Channels.java:174)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n",
      "\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o747.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 767 in stage 109.0 failed 1 times, most recent failure: Lost task 767.0 in stage 109.0 (TID 11313) (ip-172-16-94-2.us-west-2.compute.internal executor driver): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_all_motifs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxRecordsPerFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms3_url_trusted\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/prod/graph/all_motifs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o747.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 767 in stage 109.0 failed 1 times, most recent failure: Lost task 767.0 in stage 109.0 (TID 11313) (ip-172-16-94-2.us-west-2.compute.internal executor driver): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:353)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1517)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1515)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1515)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_all_motifs.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/all_motifs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec32e5d-5b88-4459-8424-be0c4fe95a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca547b46-dce8-446e-b67b-fc0cdbd85e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_motifs.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba548b-7b97-49b6-a870-63dafc58c49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_all_motifs = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/all_motifs\").cache()\n",
    "print(\"read all_motifs: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b15cd-2b5c-4fc5-a8b4-a3e626b3be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all_motifs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74fcba-0386-4b99-a311-c9bfd89dae43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2bf713-f282-4cea-99e3-1a5c865fe0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#instantiate dictionary and return df\n",
    "dict_mapping = {}\n",
    "#list of sparse cols\n",
    "list_sparse = []\n",
    "\n",
    "df_all_motifs, dict_mapping, list_sparse = oneHotCol(df_all_motifs,'object', dict_mapping, list_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a1c06-d2cd-4285-8e66-534d7fa761c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dict_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe036b40-2ce9-49ce-b1df-44583fd0fb11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_onehot.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").partitionBy(\"hostname\").parquet(f\"{s3_url_trusted}/prod/graph/onehot_traces\")\n",
    "df_onehot.unpersist()\n",
    "print(\"write onehot: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a5e11-3120-429b-aca1-73e654fa77fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e36d5f-e27f-42bc-b411-fde8751256ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single function \n",
    "df_onehot = oneHotCol(df_all)...pivot_ind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81b21e-2a46-4c6c-8f40-4bfbfae33e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").partitionBy(\"event_day\", \"event_hour\").parquet(f\"{s3_url_trusted}/prod/graph/onehot_traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f392e-113d-43c8-abde-e76082ed2f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb95d5e-a876-4d8e-8e47-ecd8d7ea9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: see if duplicate subpaths can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048202a-f753-4aa1-abe1-f1d3520cab77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a27d4-5f83-418c-859a-4306fafb7ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
