{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd9b7fd-20e7-4aa0-965d-68c8c2e40a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d31d3498-2c26-4bb2-ace2-b75351639438;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 287ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d31d3498-2c26-4bb2-ace2-b75351639438\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 01:32:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 01:32:54 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ab99e5-c4c7-4e68-9f63-6de60d4d2b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run build_adjacency.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e63425-7456-49b1-9416-cdedaad017b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:48AM UTC on Mar 13, 2023 --- read and cache time: 1.0855274200439453 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_epm = readCheckpoint(size = \"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d41369-50fe-44eb-ae31-cdef71a91d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfb8520-fd5a-42ad-a3a7-204a870db9b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- event_minute: integer (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_eps.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f18d26-e723-473e-9745-c2ebdaf8eb2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "+-------+\n",
      "| object|\n",
      "+-------+\n",
      "|  SHELL|\n",
      "|   FLOW|\n",
      "|   FILE|\n",
      "|PROCESS|\n",
      "+-------+\n",
      "\n",
      "+---------+\n",
      "|event_day|\n",
      "+---------+\n",
      "|       23|\n",
      "+---------+\n",
      "\n",
      "+----------+\n",
      "|event_hour|\n",
      "+----------+\n",
      "|        16|\n",
      "|        15|\n",
      "|        17|\n",
      "|        18|\n",
      "|        13|\n",
      "|        14|\n",
      "|        19|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_eps.count())\n",
    "df_eps.select(\"object\").distinct().show()\n",
    "df_eps.select(\"event_day\").distinct().show()\n",
    "df_eps.select(\"event_hour\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073fb40f-444a-409f-ab88-68b61629cfcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a PySxpark DataFrame\n",
    "df = spark.createDataFrame([(0, \"category1\"), (1, \"category2\"), (2, \"category3\"), (3, \"category1\")], [\"id\", \"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff63a587-cfab-45e6-8115-36ff27b13b67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>category1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>category2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>category3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>category1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   category\n",
       "0   0  category1\n",
       "1   1  category2\n",
       "2   2  category3\n",
       "3   3  category1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d82b066-8262-4804-847e-428dade4a4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+\n",
      "| id| category|class_numeric|\n",
      "+---+---------+-------------+\n",
      "|  0|category1|          0.0|\n",
      "|  1|category2|          1.0|\n",
      "|  2|category3|          2.0|\n",
      "|  3|category1|          0.0|\n",
      "+---+---------+-------------+\n",
      "\n",
      "+---+---------+-------------+-------------+\n",
      "| id| category|class_numeric| class_onehot|\n",
      "+---+---------+-------------+-------------+\n",
      "|  0|category1|          0.0|(3,[0],[1.0])|\n",
      "|  1|category2|          1.0|(3,[1],[1.0])|\n",
      "|  2|category3|          2.0|(3,[2],[1.0])|\n",
      "|  3|category1|          0.0|(3,[0],[1.0])|\n",
      "+---+---------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#turn into numeric index before encoding\n",
    "indexer = StringIndexer(inputCol='category', outputCol='class_numeric')\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df_indexed = indexer_fitted.transform(df)\n",
    "df_indexed.show()\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['class_numeric'], outputCols=['class_onehot'],dropLast=False)\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22703a05-edb9-4ff1-b909-b25e5bcbb331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+-------------+---------------+\n",
      "| id| category|class_numeric| class_onehot|     col_onehot|\n",
      "+---+---------+-------------+-------------+---------------+\n",
      "|  0|category1|          0.0|(3,[0],[1.0])|[1.0, 0.0, 0.0]|\n",
      "|  1|category2|          1.0|(3,[1],[1.0])|[0.0, 1.0, 0.0]|\n",
      "|  2|category3|          2.0|(3,[2],[1.0])|[0.0, 0.0, 1.0]|\n",
      "|  3|category1|          0.0|(3,[0],[1.0])|[1.0, 0.0, 0.0]|\n",
      "+---+---------+-------------+-------------+---------------+\n",
      "\n",
      "+---+---------+-------------+-------------+-------------+\n",
      "| id| category|col_onehot[0]|col_onehot[1]|col_onehot[2]|\n",
      "+---+---------+-------------+-------------+-------------+\n",
      "|  0|category1|          1.0|          0.0|          0.0|\n",
      "|  1|category2|          0.0|          1.0|          0.0|\n",
      "|  2|category3|          0.0|          0.0|          1.0|\n",
      "|  3|category1|          1.0|          0.0|          0.0|\n",
      "+---+---------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now turn sparse array into columns\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "df_col_onehot = df_onehot.select('*', vector_to_array('class_onehot').alias('col_onehot'))\n",
    "df_col_onehot.show()\n",
    "\n",
    "#and then unpack vectors into columns\n",
    "num_categories = len(df_col_onehot.first()['col_onehot'])   # 3\n",
    "cols_expanded = [(col('col_onehot')[i]) for i in range(num_categories)]\n",
    "df_cols_onehot = df_col_onehot.select('id', 'category', *cols_expanded)\n",
    "df_cols_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0122624c-7d07-4460-bd8c-5455dd574ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------+---------+\n",
      "| id| category|category1|category2|category3|\n",
      "+---+---------+---------+---------+---------+\n",
      "|  0|category1|      1.0|      0.0|      0.0|\n",
      "|  1|category2|      0.0|      1.0|      0.0|\n",
      "|  2|category3|      0.0|      0.0|      1.0|\n",
      "|  3|category1|      1.0|      0.0|      0.0|\n",
      "+---+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now rename the categories\n",
    "cols_expanded = [col('col_onehot')[i].alias(f'{indexer_fitted.labels[i]}') for i in range(num_categories)]\n",
    "df_cols_onehot = df_col_onehot.select('id', 'category', *cols_expanded)\n",
    "df_cols_onehot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6656ad34-453c-416c-9d43-f3910822dc04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+-------------+--------------------+\n",
      "| id| category|class_numeric| class_onehot|      HashedFeatures|\n",
      "+---+---------+-------------+-------------+--------------------+\n",
      "|  0|category1|          0.0|(3,[0],[1.0])|[[1.0], [-4.0], [...|\n",
      "|  1|category2|          1.0|(3,[1],[1.0])|[[8.0], [-7.0], [...|\n",
      "|  2|category3|          2.0|(3,[2],[1.0])|[[4.0], [6.0], [-...|\n",
      "|  3|category1|          0.0|(3,[0],[1.0])|[[1.0], [-4.0], [...|\n",
      "+---+---------+-------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH, VectorAssembler\n",
    "\n",
    "#now create an index based on the sparse vector\n",
    "# create a BucketedRandomProjectionLSH instance to hash the sparse vector column\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"class_onehot\", outputCol=\"HashedFeatures\", numHashTables=3, bucketLength=0.1)\n",
    "# fit the transformer on the DataFrame and transform it\n",
    "brp_model = brp.fit(df_onehot)\n",
    "df_hashed = brp_model.transform(df_onehot)\n",
    "# show the resulting DataFrame\n",
    "df_hashed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d002a09f-570a-4fc4-8058-1dc917148f6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column HashedFeatures must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m vi \u001b[38;5;241m=\u001b[39m VectorIndexer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHashedFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexedFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxCategories\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# transform the DataFrame using the VectorIndexer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df_indexed \u001b[38;5;241m=\u001b[39m \u001b[43mvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_hashed\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df_hashed)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# show the resulting DataFrame\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df_indexed\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column HashedFeatures must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<struct<type:tinyint,size:int,indices:array<int>,values:array<double>>>."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# create a VectorIndexer instance to assign integers to the unique hashed values\n",
    "vi = VectorIndexer(inputCol=\"HashedFeatures\", outputCol=\"IndexedFeatures\", maxCategories=3)\n",
    "\n",
    "# transform the DataFrame using the VectorIndexer\n",
    "df_indexed = vi.fit(df_hashed).transform(df_hashed)\n",
    "\n",
    "# show the resulting DataFrame\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bfcaab9-a2a4-4811-a646-1f7519f6d46f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: The feature is not supported: literal for '[r, g, r, r]' of class java.util.ArrayList.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:283)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:125)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_hashed \u001b[38;5;241m=\u001b[39m df_hashed\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/functions.py:137\u001b[0m, in \u001b[0;36mlit\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlit\u001b[39m(col: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Creates a :class:`~pyspark.sql.Column` of literal value.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    [Row(height=5, spark_user=True)]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m col \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/functions.py:85\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     84\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: The feature is not supported: literal for '[r, g, r, r]' of class java.util.ArrayList.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:283)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:125)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df_hashed = df_hashed.withColumn('color',lit(['r','g','r','r']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3520d2e6-691c-49e8-9087-81915fe5da62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- second_color: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- second_color: string (nullable = true)\n",
      " |-- sha2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# create sample data\n",
    "data = [(0, \"red\",'g'), (1, \"green\",'blue'), (2, \"red\",'g')]\n",
    "\n",
    "# create dataframe\n",
    "df = spark.createDataFrame(data, [\"id\", \"color\",'second_color'])\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "#now create a hash\n",
    "df = df.withColumn(\"sha2\",sha2(concat_ws('||',col('color'),col('second_color')),256))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d3e4efd-cdb9-43f7-ad1c-4af8c7e51348",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------+--------------------+-------------+\n",
      "| id|color|second_color|                sha2|class_numeric|\n",
      "+---+-----+------------+--------------------+-------------+\n",
      "|  0|  red|           g|1edcfa2380771ecfb...|          0.0|\n",
      "|  1|green|        blue|50e3429807908a500...|          1.0|\n",
      "|  2|  red|           g|1edcfa2380771ecfb...|          0.0|\n",
      "+---+-----+------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#turn into numeric index before encoding\n",
    "indexer = StringIndexer(inputCol='sha2', outputCol='class_numeric')\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df_indexed = indexer_fitted.transform(df)\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f190b96-cc5a-42c5-bd1f-44115cfa4e51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:13PM UTC on Mar 10, 2023 --- read and cache time: 9.284032821655273 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#load in size medium\n",
    "df_eps = readCheckpoint(size='medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555c66d3-8998-4b5e-9eb3-5e77116579b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_mal_hosts = ['SysClient0559.systemia.com',\n",
    "'SysClient0255.systemia.com',\n",
    "'SysClient0205.systemia.com',\n",
    "'SysClient0503.systemia.com',\n",
    "'SysClient0355.systemia.com',\n",
    "'SysClient0104.systemia.com',\n",
    "'SysClient0874.systemia.com',\n",
    "'SysClient0955.systemia.com',\n",
    "'SysClient0201.systemia.com',\n",
    "'SysClient0402.systemia.com',\n",
    "'SysClient0660.systemia.com',\n",
    "'SysClient0321.systemia.com',\n",
    "'SysClient0609.systemia.com',\n",
    "'SysClient0771.systemia.com',\n",
    "'SysClient0462.systemia.com',\n",
    "'SysClient0419.systemia.com',\n",
    "'SysClient0010.systemia.com',\n",
    "'SysClient0069.systemia.com',\n",
    "'SysClient0358.systemia.com',\n",
    "'SysClient0203.systemia.com',\n",
    "'SysClient0618.systemia.com',\n",
    "'SysClient0851.systemia.com',\n",
    "'SysClient0501.systemia.com',\n",
    "'SysClient0811.systemia.com',\n",
    "'SysClient0170.systemia.com',\n",
    "'SysClient0351.systemia.com',\n",
    "'SysClient0051.systemia.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d54292-d7d9-4366-88d0-85f87a7f43ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_epm = df_eps.filter(col(\"hostname\").isin(list_mal_hosts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d65332-ebe5-4a83-b3f7-089009a126ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- event_minute: integer (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_epm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983dfc87-fbfb-408b-8c93-3e207e701cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:========================================================>(82 + 1) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|event_day|\n",
      "+---------+\n",
      "|       23|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_epm.select(\"event_day\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1387d2b6-9745-48cd-9d65-0a55873f27dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6553319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_epm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f003c393-377a-44d9-99c5-b8e916f37feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's learn about user log-on events\n",
    "df_epm.where(col('object') == 'USER_SESSION').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8476bd2f-76b8-45c6-8350-8ee5399c8d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4:20PM UTC on Mar 10, 2023 --- read and cache time: 1.71661376953125e-05 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#get a count of malicious events to compare what we have to what we need\n",
    "df_labels = readCheckpoint_bcm('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f24789-388e-4433-9cbe-f69dad0cb512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with distinct objectIDs to identify malcious ObjectIds\n",
    "df_malcious_objectIDs = df_labels.select('id').distinct()\n",
    "\n",
    "# Create a new column called 'malicious' in df_ecar to label malicious records\n",
    "df_epm = df_epm.withColumn('malicious', when(col('id').isin(df_malcious_objectIDs.rdd.flatMap(lambda x: x).collect()), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f64339a6-c33d-4c54-8f2a-8a94ea3108b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- objectID: string (nullable = true)\n",
      " |-- actorID: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- privileges: string (nullable = true)\n",
      " |-- image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- new_path: string (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- logon_id: string (nullable = true)\n",
      " |-- requesting_domain: string (nullable = true)\n",
      " |-- requesting_user: string (nullable = true)\n",
      " |-- event_minute: integer (nullable = true)\n",
      " |-- event_day: integer (nullable = true)\n",
      " |-- event_hour: integer (nullable = true)\n",
      " |-- malicious: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_epm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f293d0-445e-4209-bc0c-0a0db02435b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malicious: 71193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================================> (81 + 2) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign: 6482126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#how many malicious and how many not\n",
    "print(\"malicious: \"+ str(df_epm.where(col('malicious') == 1).count()))\n",
    "print(\"benign: \"+ str(df_epm.where(col('malicious') == 0).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f5f90a3-1ea9-472e-b7ed-0568174e6752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#so what I am going to need to do is run build adjaceny matrix against this function. \n",
    "\n",
    "#first let's pull a small section of the benign\n",
    "df_epm_benign = df_epm.where(col('malicious') == 0).limit(300000)\n",
    "df_epm_mal = df_epm.where(col('malicious') == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a49f782c-c623-4e17-aa50-fe2e498001f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#work on extracting parent time and subtracting from child time\n",
    "#get actIDs\n",
    "col_objID = df_epm_mal.select(\"objectID\").distinct().collect()\n",
    "col_actID = df_epm_mal.select(\"actorID\").distinct().collect()\n",
    "\n",
    "#put these columns into a dictionary and turn into a spark data frame\n",
    "data = [{\"actorIDs\" : [row.actorID for row in col_actID], \\\n",
    "               \"objectIDs\" : [row.objectID for row in col_objID]}]\n",
    "\n",
    "id_intersect = spark.createDataFrame(data)\n",
    "\n",
    "#compare ID pools to find actIDs that are nodes\n",
    "id_intersect = id_intersect.withColumn(\"id_int\", \\\n",
    "                                       array_intersect(col(\"actorIDs\"),col(\"objectIDs\")))\n",
    "\n",
    "#now build a list of the nodal actIDs and print the length\n",
    "actIDs = [row.id_int for row in id_intersect.select(\"id_int\").collect()][0]\n",
    "print(len(actIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7597b8e7-92c2-4c14-bacb-45bdeee3d2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#now pull time and child_event for first actor\n",
    "first_act = actIDs[1]\n",
    "birth_event = df_epm_mal.where((col(\"objectID\") == first_act) \\\n",
    "                        & (col(\"actorID\")!= first_act)).limit(3).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef522b37-2527-4699-95bb-151d2f5c4101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='e8cd79b8-6d34-4e34-8771-535c38cc5178', timestamp=datetime.datetime(2019, 9, 23, 18, 44, 59, 248000), objectID='2d02ab42-4400-4049-baad-6dfae19c3c4c', actorID='796a63d7-45d9-4a50-8d1c-94a9c4e74e54', object='PROCESS', action='CREATE', hostname='SysClient0609.systemia.com', user_name=None, privileges=None, image_path='powershell.exe', parent_image_path='\\\\Device\\\\HarddiskVolume1\\\\Windows\\\\system32\\\\wbem\\\\wmiprvse.exe', new_path=None, file_path=None, direction=None, logon_id=None, requesting_domain=None, requesting_user=None, event_minute=44, event_day=23, event_hour=18, malicious=1)\n"
     ]
    }
   ],
   "source": [
    "print(birth_event[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6a242d4-fbe2-4ef1-8acf-2a6c098cdf6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70ba4f9c-e27b-4a34-8dce-303c00c227c2:2019-09-23 18:44:53.785000:PROCESS:\\Device\\HarddiskVolume1\\Windows\\system32\\wbem\\wmiprvse.exe:powershell.exe\n"
     ]
    }
   ],
   "source": [
    "#str(row[\"objectID\"]+':'+row[\"first(id)\"])\n",
    "\n",
    "str_key = str(birth_event[0]['objectID']+\":\" \\\n",
    "              +str(birth_event[0]['timestamp'])+\":\" \\\n",
    "              +birth_event[0]['object']+\":\" \\\n",
    "              +birth_event[0]['parent_image_path']+\":\" \\\n",
    "              +birth_event[0]['image_path'])\n",
    "print(str_key)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "813f3652-6b11-43ca-b4c7-c31ddb0c1351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Device\\HarddiskVolume1\\Windows\\system32\\wbem\\wmiprvse.exe\n"
     ]
    }
   ],
   "source": [
    "for row in birth_event:\n",
    "    print(row['parent_image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6edebfc-9f85-491c-ad29-eef2c83d3d90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run build_adjacency.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d137ad1f-00db-4892-95fe-fb96a86e56f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:========================================================>(82 + 1) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|event_hour|\n",
      "+----------+\n",
      "|        16|\n",
      "|        19|\n",
      "|        15|\n",
      "|        17|\n",
      "|        18|\n",
      "|        13|\n",
      "|        14|\n",
      "|        22|\n",
      "|        21|\n",
      "|        20|\n",
      "|        12|\n",
      "|         9|\n",
      "|        23|\n",
      "|         7|\n",
      "|        10|\n",
      "|        11|\n",
      "|         3|\n",
      "|         5|\n",
      "|         4|\n",
      "|         8|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_epm.select(\"event_hour\").where(col(\"event_day\") == 23).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b0204a-8e0b-4aad-8338-21ee8d4abc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:========================================================>(82 + 1) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|event_day|\n",
      "+---------+\n",
      "|       23|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_epm.select('event_day').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d096a43-e9d0-4a6a-8fbd-68566b40e10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_epm_9_10 = df_epm.where((col('event_hour')== 9) | (col('event_hour')== 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78f63a1a-b502-4dfa-9363-b44da5a3765c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length actIDs: 1308\n",
      "elapsed time: 2.8200037479400635\n",
      "ed4dfa35-ee46-4e37-a2b6-9838128b9b50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no birth event for 83 events.\n",
      "elapsed time 2: 2158.625776529312\n"
     ]
    }
   ],
   "source": [
    "df_epm_test = build_adjaceny(df_epm_9_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d9d3c8-52a0-41fd-bf50-d641322c0580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "s3_write_loc = f\"{s3_url_trusted}/prod/adjacency-graph/0910\"\n",
    "df_epm_test.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(s3_write_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "819f53a7-7a44-4d53-a86f-02f348a412d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length actIDs: 491\n",
      "elapsed time: 178.3520474433899\n",
      "5ffe548e-f658-49b9-b448-9d5c78f7788d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no birth event for 41 events.\n",
      "elapsed time 2: 949.9309887886047\n"
     ]
    }
   ],
   "source": [
    "df_epm_11 = df_epm.where((col('event_hour')== 11))\n",
    "df_epm_11w = build_adjaceny(df_epm_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff4768e-bc82-42f1-9c60-a086005600f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df_epm_11 = build_adjaceny(df_epm_11)\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "s3_write_loc = f\"{s3_url_trusted}/prod/adjacency-graph/11\"\n",
    "df_epm_11w.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(s3_write_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
