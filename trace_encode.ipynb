{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf9097a-9fbb-43ee-b532-01f8d8abb511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c33cce-a25f-4bc3-9742-e31e262f8612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80a239c-ceb2-400c-8543-895bc146d667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ecd6f4-d70b-4b5e-bc3d-167f82edcdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def de_dupe():\n",
    "    #read in first events and then drop bi-directional events\n",
    "    window = Window.partitionBy(\"relationship\").orderBy(\"timestamp\")\n",
    "    df = readFirstEvents().sort(\"timestamp\").withColumn(\"relationship\", sort_array(array(\"actorID\", \"objectID\")) )  \\\n",
    "                .withColumn(\"row_num\", row_number().over(window)) \\\n",
    "                .filter(col(\"row_num\") == 1) \\\n",
    "                .drop(\"row_num\") \\\n",
    "                .drop(\"relationship\").cache()\n",
    "    \n",
    "    #df = df.where(col(\"event_day\") == day).cache()\n",
    "    \n",
    "    #create file extnsion columns\n",
    "    df = df.withColumn(\"file_path_ext\", regexp_extract(\"file_path\", \"\\.[0-9a-z]+$\", 0))\n",
    "    \n",
    "    #filter file extensions\n",
    "    file_ext = ['None', 'empty', '.dll', '.pyc', '.bat', '.pf', '.ini', '.exe', '.tmp']\n",
    "    df = df.withColumn(\"file_path_ext\", when(col(\"file_path_ext\").isNull(),\"None\").otherwise(col(\"file_path_ext\"))) \\\n",
    "    .withColumn(\"file_path_ext\", when(col(\"file_path_ext\") == '', 'empty').otherwise(col(\"file_path_ext\"))) \\\n",
    "    .withColumn(\"file_path_ext\", when(col(\"file_path_ext\").isin(file_ext), col('file_path_ext')).otherwise('other'))\n",
    "    \n",
    "    #TODO, incorporate get_file_udf and then filter based on updated image path and parent path \n",
    "    list_img_short = ['None','empty','.exe','.com']\n",
    "    list_img = ['svchost.exe', 'System', 'cmd.exe', 'None', \n",
    "                'conhost.exe', 'python.exe', 'csrss.exe', 'GoogleUpdate.exe', \n",
    "                'firefox.exe', 'backgroundTaskHost.exe', 'PING.EXE', 'python.EXE', \n",
    "                'geckodriver.exe', 'Explorer.EXE', 'CompatTelRunner.exe', 'taskhostw.exe']\n",
    "    list_par = ['None', 'cmd.exe', 'csrss.exe', 'svchost.exe', 'GoogleUpdate.exe', 'conhost.exe']\n",
    "    \n",
    "    #create image path columns \n",
    "    df = df.withColumn(\"image_path\", getFileUDF(col(\"image_path\")))\n",
    "    df = df.withColumn(\"parent_image_path\", getFileUDF(col(\"parent_image_path\")))\n",
    "    \n",
    "    #extract the right elements from the image and parent image path columns\n",
    "    df = df.withColumn(\"image_path\", when(col(\"image_path\").isNull(),\"None\").otherwise(col(\"image_path\"))) \\\n",
    "    .withColumn(\"image_path\", when(col(\"image_path\") == '', 'empty').otherwise(col(\"image_path\"))) \\\n",
    "    .withColumn(\"image_path\", when(col(\"image_path\").isin(list_img), col('image_path')).otherwise('other'))\n",
    "    \n",
    "    df = df.withColumn(\"parent_image_path\", when(col(\"parent_image_path\").isNull(),\"None\").otherwise(col(\"parent_image_path\"))) \\\n",
    "    .withColumn(\"parent_image_path\", when(col(\"parent_image_path\") == '', 'empty').otherwise(col(\"parent_image_path\"))) \\\n",
    "    .withColumn(\"parent_image_path\", when(col(\"parent_image_path\").isin(list_par), col('parent_image_path')).otherwise('other'))\n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"image_path_ext\", when(col(\"image_path_ext\").isNull(),\"None\").otherwise(col(\"image_path_ext\"))) \\\n",
    "    .withColumn(\"image_path_ext\", when(col(\"image_path_ext\") == '', 'empty').otherwise(col(\"image_path_ext\"))) \\\n",
    "    .withColumn(\"image_path_ext\", when(col(\"image_path_ext\").isin(list_img), col('image_path_ext')).otherwise('other')).cache()\n",
    "    \n",
    "    \n",
    "    df = df.withColumn(\"parent_path_ext\", when(col(\"parent_path_ext\").isNull(),\"None\").otherwise(col(\"parent_path_ext\"))) \\\n",
    "    .withColumn(\"parent_path_ext\", when(col(\"parent_path_ext\") == '', 'empty').otherwise(col(\"parent_path_ext\"))).cache()\n",
    "    \"\"\"\n",
    "    \n",
    "    '''#read in pagerank data <- TODO: return to this later\n",
    "    s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "    pr = spark.read.parquet(f\"{s3_url_trusted}prod/graph/pagerank\")\n",
    "    \n",
    "    #now we need to add page rank by \"union\"ing page rank based on ID\n",
    "    #now let's join\n",
    "    df = df.sort('objectID').join(pr.sort('objectID'), on = 'objectID')'''\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc5b57-7a3e-4c64-bd8e-702eb70e0f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb6e3cf-2714-4d83-817b-0e59924ab567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to generate a graphframe\n",
    "def create_graph2(df):\n",
    "    #now create graph for event traces of 2 events, 3 events\n",
    "    start_time = time.time()\n",
    "\n",
    "    #create trace matrix from malicious events for speed. \n",
    "    # Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "    src_vertices = df.selectExpr('objectID as id').distinct()\n",
    "    dst_vertices = df.selectExpr('actorID as id').distinct()\n",
    "    vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "    # Create edges by using timestamp as an edge\n",
    "    edges = df.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                              'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious',\n",
    "                         'file_path_ext')#, 'pagerank')#,'parent_path_ext','image_path_ext')\n",
    "\n",
    "    # Create GraphFrame\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c)\")\n",
    "    print(\"found connections: \"+ str(time.time() - start_time))\n",
    "    #create paths and count\n",
    "    # filter paths to only those where all edges are connected\n",
    "    connected_paths = motifs6.filter('''e1.timestamp <= e2.timestamp''')\n",
    "    print(\"filtered connections: \"+ str(time.time() - start_time))\n",
    "    \n",
    "    return connected_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4facffe4-11c4-460b-83dc-6c29ea17b51d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to generate a graphframe\n",
    "def create_graph3(df):\n",
    "    #now create graph for event traces of 2 events, 3 events\n",
    "    start_time = time.time()\n",
    "\n",
    "    #create trace matrix from malicious events for speed. \n",
    "    # Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "    src_vertices = df.selectExpr('objectID as id').distinct()\n",
    "    dst_vertices = df.selectExpr('actorID as id').distinct()\n",
    "    vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "    # Create edges by using timestamp as an edge\n",
    "    edges = df.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                              'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious',\n",
    "                         'file_path_ext')#, 'pagerank')#,'parent_path_ext','image_path_ext')\n",
    "\n",
    "    # Create GraphFrame\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "    print(\"found connections: \"+ str(time.time() - start_time))\n",
    "    #create paths and count\n",
    "    # filter paths to only those where all edges are connected\n",
    "    connected_paths = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                        #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                        #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "    #tot = connected_paths.count()\n",
    "    #print(\"event traces: \"+str(tot))\n",
    "    return connected_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85275f18-6d74-4fb6-8971-976fc3e257e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to generate a graphframe\n",
    "def create_graph4(df):\n",
    "    start_time = time.time()\n",
    "\n",
    "    #create trace matrix from malicious events for speed. \n",
    "    # Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "    src_vertices = df.selectExpr('objectID as id').distinct()\n",
    "    dst_vertices = df.selectExpr('actorID as id').distinct()\n",
    "    vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "    # Create edges by using timestamp as an edge\n",
    "    edges = df.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                              'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious',\n",
    "                         'file_path_ext')#, 'pagerank')#,'parent_path_ext','image_path_ext')\n",
    "\n",
    "    # Create GraphFrame\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d); (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "    print(\"found connections: \"+ str(time.time() - start_time))\n",
    "    #create paths and count\n",
    "    # filter paths to only those where all edges are connected\n",
    "    connected_paths = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp and \n",
    "                                        e3.timestamp <= e4.timestamp''')# and e4.timestamp <= e5.timestamp and \n",
    "                                        #e5.timestamp <= e6.timestamp''')\n",
    "    \n",
    "    #tot = connected_paths.count()\n",
    "    #print(\"event traces: \"+str(tot))\n",
    "    return connected_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b7782ad-18a5-4225-aaae-f038538dbdd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to generate a graphframe\n",
    "def create_graph5(df):\n",
    "    start_time = time.time()\n",
    "\n",
    "    #create trace matrix from malicious events for speed. \n",
    "    # Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "    src_vertices = df.selectExpr('objectID as id').distinct()\n",
    "    dst_vertices = df.selectExpr('actorID as id').distinct()\n",
    "    vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "    # Create edges by using timestamp as an edge\n",
    "    edges = df.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                              'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious',\n",
    "                         'file_path_ext')#, 'pagerank')#,'parent_path_ext','image_path_ext')\n",
    "\n",
    "    # Create GraphFrame\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    motifs5 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d); (d)-[e4]->(e); (e)-[e5]->(f)\")\n",
    "    print(\"found connections: \"+ str(time.time() - start_time))\n",
    "    #create paths and count\n",
    "    # filter paths to only those where all edges are connected\n",
    "    connected_paths = motifs5.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp and \n",
    "                                        e3.timestamp <= e4.timestamp and e4.timestamp <= e5.timestamp''')\n",
    "\n",
    "    #tot = connected_paths.count()\n",
    "    #print(\"event traces: \"+str(tot))\n",
    "    return connected_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9a08484-e39c-4d22-9163-ac8619f8d305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to generate a graphframe\n",
    "def create_graph6(df):\n",
    "    start_time = time.time()\n",
    "\n",
    "    #create trace matrix from malicious events for speed. \n",
    "    # Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "    src_vertices = df.selectExpr('objectID as id').distinct()\n",
    "    dst_vertices = df.selectExpr('actorID as id').distinct()\n",
    "    vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "    # Create edges by using timestamp as an edge\n",
    "    edges = df.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                              'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious',\n",
    "                         'file_path_ext')#, 'pagerank')#,'parent_path_ext','image_path_ext')\n",
    "\n",
    "    # Create GraphFrame\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d); (d)-[e4]->(e); (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "    print(\"found connections: \"+ str(time.time() - start_time))\n",
    "    #create paths and count\n",
    "    # filter paths to only those where all edges are connected\n",
    "    connected_paths = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp and \n",
    "                                        e3.timestamp <= e4.timestamp and e4.timestamp <= e5.timestamp and \n",
    "                                        e5.timestamp <= e6.timestamp''')\n",
    "\n",
    "    #tot = connected_paths.count()\n",
    "    #print(\"event traces: \"+str(tot))\n",
    "    return connected_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07223021-c48b-4e6a-8e20-a8ee35042c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to bin the timestamp data in the data frame\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "def ts_diff(df):\n",
    "    # Define the window specification\n",
    "    window_spec = Window.partitionBy(\"Trace\").orderBy(\"timestamp\")\n",
    "\n",
    "    # Calculate the timestamp difference between the current event and the preceding event\n",
    "    df = df.withColumn(\n",
    "        \"timestamp_difference\",\n",
    "        coalesce(\n",
    "            (col(\"timestamp\").cast(\"long\") - lag(col(\"timestamp\").cast(\"long\")).over(window_spec)) * 1000\n",
    "            + (col(\"timestamp\").cast(\"double\") % 1 - lag(col(\"timestamp\").cast(\"double\")).over(window_spec) % 1) * 1000,\n",
    "            lit(None).cast(\"double\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f42bc25-dd39-4a55-8122-a9bf61c05ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create bins\n",
    "def bin_it(df):\n",
    "    \n",
    "    df = df.drop(\"timestamp_difference\")\n",
    "    \n",
    "    df = ts_diff(df)\n",
    "    \n",
    "    # if timestamp_bins exists then drop the column\n",
    "    df = df.drop(\"timestamp_bins\")\n",
    "    \n",
    "    # create a Bucketizer instance with quantiles\n",
    "    #bins = stacked_df.approxQuantile(\"timestamp_difference\", [0.2, 0.4, .6, .8], 0)\n",
    "    \n",
    "    bins_2 = [float(\"-inf\")] + [1730209.0001106262, 8254118.999958038, \n",
    "                                21626879.999876022, 44756893.000125885] + [float(\"inf\")]\n",
    "    \n",
    "    bucketizer = Bucketizer(splits=bins_2, inputCol=\"timestamp_difference\", outputCol=\"timestamp_bins\")\n",
    "    \n",
    "    # transform the DataFrame\n",
    "    df = bucketizer.transform(df)\n",
    "\n",
    "    #drop tiimestamp diff\n",
    "    df = df.drop('timestamp_difference')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44074033-cd16-4abe-84bd-de6023571625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bin_it_pr(df):\n",
    "    \n",
    "    df = df.drop('pagerank_bins')\n",
    "    \n",
    "    #df = df.distinct().orderBy(rand()).limit(5000000).cache()\n",
    "    \n",
    "    #create a bucketizer for quintiles\n",
    "    bins = df.approxQuantile(\"pagerank\", [0.2, 0.4, .6, .8], 0)\n",
    "    \n",
    "    bins = [float(\"-inf\")] + bins + [float(\"inf\")]\n",
    "    \n",
    "    bucketizer = Bucketizer(splits=bins, inputCol=\"pagerank\", outputCol=\"pagerank_bins\")\n",
    "    \n",
    "    # transform the DataFrame\n",
    "    df = bucketizer.transform(df)\n",
    "    \n",
    "    #drop pagerank diff\n",
    "    df = df.drop('pagerank')\n",
    "    \n",
    "    return df, bins\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d4d539-fe27-43fc-b80b-cb09a14eecf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to accept an event trace data frame and then encode it. Ideally we run this twice, once on the malicious\n",
    "#traces and again on benign traces. This implies that we run graphframes on both sets of events independently. \n",
    "\n",
    "#TODO: extract only the extensions, or declare file path an attribute \n",
    "\n",
    "def oneHotCol(df, colm, dict_mapping, cols_sparse):\n",
    "    \n",
    "    #now action\n",
    "    #turn into numeric index before encoding\n",
    "    \n",
    "    num = colm+'_numeric'\n",
    "    sparse = colm+'_sparse'\n",
    "    indexer = StringIndexer(inputCol=colm, outputCol=num, handleInvalid=\"keep\")\n",
    "    indexer_fitted = indexer.fit(df)\n",
    "    df = indexer_fitted.transform(df)\n",
    "\n",
    "    encoder = OneHotEncoder(inputCol=num, outputCol=sparse,dropLast=False)\n",
    "    encoder_fit = encoder.fit(df)\n",
    "    df = encoder_fit.transform(df)\n",
    "    df = df.drop(colm, num)\n",
    "\n",
    "    #set dict to mapping\n",
    "    dict_mapping[colm] = indexer_fitted.labels\n",
    "    \n",
    "    #add column to cols_sparse list\n",
    "    cols_sparse.append(sparse)\n",
    "        \n",
    "    return df.cache(), dict_mapping, cols_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14ee82f7-ed2f-4c4a-9745-ffbb59dfebc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#udf functions\n",
    "\n",
    "# define a user-defined function to convert binary int array to string array\n",
    "def binary_to_string_array(binary_int_array):\n",
    "    string_array = []\n",
    "    for i in binary_int_array:\n",
    "        string_array.append(str(int(i)))\n",
    "    return ''.join(string_array)\n",
    "\n",
    "# register the user-defined function as a UDF\n",
    "binary_to_string_array_udf = udf(binary_to_string_array, StringType())\n",
    "\n",
    "def int_cast(num):\n",
    "    return int(num)\n",
    "\n",
    "int_cast_udf = udf(int_cast, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99dc7f04-5460-4195-abbd-8e3368c69c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transp_expl5(df):\n",
    "    #add \"trace index\" to keep track of traces. important for transposing back\n",
    "    df = df.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "    df = df.withColumn(\"Trace\",concat(col(\"Trace\"), lit(\"-5\")))\n",
    "    df = df.select(\"Trace\", *[col for col in df.columns if col != \"Trace\"])\n",
    "\n",
    "    #drop all vertices\n",
    "    df = df.drop('a','b','c','d','e','f')\n",
    "\n",
    "    #transpose rows \n",
    "    df = df.selectExpr(\n",
    "        \"Trace\", \n",
    "        \"posexplode(array(e1, e2, e3, e4, e5)) as (pos, col)\"\n",
    "    ).select(\n",
    "        \"Trace\", \n",
    "        expr('''CASE pos \n",
    "        WHEN 0 THEN 'e1' \n",
    "        WHEN 1 THEN 'e2'\n",
    "        WHEN 2 THEN 'e3'\n",
    "        WHEN 3 THEN 'e4'\n",
    "        ELSE 'e5' END''').alias(\"event\"),\n",
    "        \"col\"\n",
    "    ).orderBy(\"Trace\",\"event\")\n",
    "\n",
    "    #explode columns\n",
    "    df = df.select(*df.columns, \"col.*\").drop('col')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dfccae4-f52a-451d-a30c-d3c463fdea69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transp_expl(df):\n",
    "    #add \"trace index\" to keep track of traces. important for transposing back\n",
    "    df = df.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "    df = df.withColumn(\"Trace\",concat(col(\"Trace\"), lit(\"-6\")))\n",
    "    df = df.select(\"Trace\", *[col for col in df.columns if col != \"Trace\"])\n",
    "\n",
    "    #drop all vertices\n",
    "    df = df.drop('a','b','c','d','e','f','g')\n",
    "\n",
    "    #transpose rows \n",
    "    df = df.selectExpr(\n",
    "        \"Trace\", \n",
    "        \"posexplode(array(e1, e2, e3, e4, e5, e6)) as (pos, col)\"\n",
    "    ).select(\n",
    "        \"Trace\", \n",
    "        expr('''CASE pos \n",
    "        WHEN 0 THEN 'e1' \n",
    "        WHEN 1 THEN 'e2'\n",
    "        WHEN 2 THEN 'e3'\n",
    "        WHEN 3 THEN 'e4'\n",
    "        WHEN 4 THEN 'e5'\n",
    "        ELSE 'e6' END''').alias(\"event\"),\n",
    "        \"col\"\n",
    "    ).orderBy(\"Trace\",\"event\")\n",
    "\n",
    "    #explode columns\n",
    "    df = df.select(*df.columns, \"col.*\").drop('col')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f0c65e1-4537-4598-8157-6b45227da486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transp_expl2(df_transp):\n",
    "    #add \"trace index\" to keep track of traces. important for transposing back\n",
    "    df_transp = df_transp.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "    #add a motif identifier\n",
    "    df_transp = df_transp.withColumn(\"Trace\",concat(col(\"Trace\"), lit(\"-2\")))\n",
    "    df_transp = df_transp.select(\"Trace\", \n",
    "                                 *[col for col in df_transp.columns if col != \"Trace\"])\n",
    "\n",
    "    #drop all vertices\n",
    "    df_transp = df_transp.drop('a','b','c')\n",
    "\n",
    "    #transpose rows \n",
    "    df_transp = df_transp.selectExpr(\n",
    "        \"Trace\", \n",
    "        \"posexplode(array(e1, e2)) as (pos, col)\"\n",
    "    ).select(\n",
    "        \"Trace\", \n",
    "        expr('''CASE pos \n",
    "        WHEN 0 THEN 'e1'\n",
    "        ELSE 'e2' END''').alias(\"event\"),\n",
    "        \"col\"\n",
    "    ).orderBy(\"Trace\",\"event\").cache()\n",
    "\n",
    "    #explode columns\n",
    "    df_transp = df_transp.select(*df_transp.columns, \"col.*\").drop('col')\n",
    "    \n",
    "    return df_transp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1469e36-0314-44e4-9206-01a401904d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transp_expl3(df_transp):\n",
    "    #add \"trace index\" to keep track of traces. important for transposing back\n",
    "    df_transp = df_transp.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "    df_transp = df_transp.withColumn(\"Trace\",concat(col(\"Trace\"), lit(\"-3\")))\n",
    "    df_transp = df_transp.select(\"Trace\", \n",
    "                                 *[col for col in df_transp.columns if col != \"Trace\"])\n",
    "\n",
    "    #drop all vertices\n",
    "    df_transp = df_transp.drop('a','b','c','d')\n",
    "\n",
    "    #transpose rows \n",
    "    df_transp = df_transp.selectExpr(\n",
    "        \"Trace\", \n",
    "        \"posexplode(array(e1, e2, e3)) as (pos, col)\"\n",
    "    ).select(\n",
    "        \"Trace\", \n",
    "        expr('''CASE pos \n",
    "        WHEN 0 THEN 'e1' \n",
    "        WHEN 1 THEN 'e2'\n",
    "        ELSE 'e3' END''').alias(\"event\"),\n",
    "        \"col\"\n",
    "    ).orderBy(\"Trace\",\"event\")\n",
    "\n",
    "    #explode columns\n",
    "    df_transp = df_transp.select(*df_transp.columns, \"col.*\").drop('col')\n",
    "    \n",
    "    return df_transp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e48600d-aa1e-47e4-b940-3f903fda10f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transp_expl4(df_transp):\n",
    "    #add \"trace index\" to keep track of traces. important for transposing back\n",
    "    df_transp = df_transp.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "    df_transp = df_transp.withColumn(\"Trace\",concat(col(\"Trace\"), lit(\"-4\")))\n",
    "    df_transp = df_transp.select(\"Trace\", \n",
    "                                 *[col for col in df_transp.columns if col != \"Trace\"])\n",
    "\n",
    "    #drop all vertices\n",
    "    df_transp = df_transp.drop('a','b','c','d','e')\n",
    "\n",
    "    #transpose rows \n",
    "    df_transp = df_transp.selectExpr(\n",
    "        \"Trace\", \n",
    "        \"posexplode(array(e1, e2, e3, e4)) as (pos, col)\"\n",
    "    ).select(\n",
    "        \"Trace\", \n",
    "        expr('''CASE pos \n",
    "        WHEN 0 THEN 'e1' \n",
    "        WHEN 1 THEN 'e2'\n",
    "        WHEN 2 THEN 'e3'\n",
    "        ELSE 'e4' END''').alias(\"event\"),\n",
    "        \"col\"\n",
    "    ).orderBy(\"Trace\",\"event\")\n",
    "\n",
    "    #explode columns\n",
    "    df_transp = df_transp.select(*df_transp.columns, \"col.*\").drop('col')\n",
    "    \n",
    "    return df_transp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdc9b0de-18bc-40df-b302-a371021a11b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trace_encode(df, traces, list_cols, output = 'vec'):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if traces == 3:\n",
    "        df_graph = create_graph3(df)\n",
    "        print(\"create graph: \"+ str(time.time() - start_time))\n",
    "        df_onehot = transp_expl3(df_graph)\n",
    "        print(\"transp-explode: \"+ str(time.time() - start_time))\n",
    "    elif traces == 4:\n",
    "        df_graph = create_graph4(df)\n",
    "        print(\"create graph: \"+ str(time.time() - start_time))\n",
    "        df_onehot = transp_expl4(df_graph)\n",
    "        print(\"transp-explode: \"+ str(time.time() - start_time))\n",
    "    else: \n",
    "        df_graph = create_graph6(df)\n",
    "        print(\"create graph: \"+ str(time.time() - start_time))\n",
    "        #step one accept the event trace transpose it, and explode it.\n",
    "        df_onehot = transp_expl(df_graph)\n",
    "        print(\"transp-explode: \"+ str(time.time() - start_time))\n",
    "\n",
    "    #calculate malicious trace then check schema and how many were made \n",
    "    w = Window.partitionBy(\"Trace\")\n",
    "    df_onehot = df_onehot.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()\n",
    "    \n",
    "    #pr_bins = []\n",
    "    \n",
    "    #now develop time_diff bins, include tot for chunking \n",
    "    df_onehot = bin_it(df_onehot).cache()\n",
    "    \n",
    "    #and now pagerank\n",
    "    #df_onehot, pr_bins = bin_it_pr(df_onehot, tot)\n",
    "    \n",
    "    #instantiate dictionary and return df\n",
    "    dict_mapping = {}\n",
    "    #list of sparse cols\n",
    "    list_sparse = []\n",
    "    \n",
    "    print(\"bin time: \"+ str(time.time() - start_time))\n",
    "    \n",
    "    #think we insert binning here\n",
    "    \n",
    "    #for all columns to one hot, one hot, preserve mapping\n",
    "    for colm in list_cols+[\"timestamp_bins\"]:#,\"pagerank_bins\"]:\n",
    "        #print(colm)\n",
    "        df_onehot, dict_mapping, list_sparse = oneHotCol(df_onehot,colm, dict_mapping, list_sparse)\n",
    "    \n",
    "    #assemble vectors for all sparse columns - this might be enough for our ML algorithms\n",
    "    assembler = VectorAssembler(inputCols=list_sparse, \n",
    "                            outputCol=\"final_vec\")\n",
    "    df_onehot = assembler.transform(df_onehot).cache()\n",
    "    \n",
    "    #turn into string\n",
    "    df_onehot = df_onehot.withColumn(\"vec2string\", binary_to_string_array_udf(\"final_vec\")).cache()\n",
    "    \n",
    "    print(\"one-hot time: \"+ str(time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    #now I need to arrange the output in a column-wise dataframe with event strings or indices and the malicious tag\n",
    "    if output == 'vec':\n",
    "        \n",
    "        #Generate a list of columns to drop\n",
    "        keep_cols = ['mal_trace','Trace','event','vec2string']\n",
    "        drop_cols = [col for col in df_onehot.columns  \n",
    "                     if col not in list_cols and col not in keep_cols]\n",
    "    \n",
    "        #i want to drop any columnn not in the column list or is the malicious column\n",
    "        df_onehot = df_onehot.drop(*drop_cols).cache()\n",
    "        \n",
    "        if traces == 3: \n",
    "        \n",
    "            #first pivot aka transpose and keep all events\n",
    "            pivot_vec = df_onehot.groupBy('Trace').pivot('event')\\\n",
    "            .agg(first('mal_trace'),first('vec2string')).cache()\n",
    "            #then consolidate the columns into a single event sequence\n",
    "            df_onehot = pivot_vec.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                                      array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                               'e3_first(vec2string)').alias('event_sequence')).cache()\n",
    "        elif traces == 4:\n",
    "            \n",
    "            #first pivot aka transpose and keep all events\n",
    "            pivot_vec = df_onehot.groupBy('Trace').pivot('event')\\\n",
    "            .agg(first('mal_trace'),first('vec2string')).cache()\n",
    "            #then consolidate the columns into a single event sequence\n",
    "            df_onehot = pivot_vec.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                                      array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                               'e3_first(vec2string)','e4_first(vec2string)').alias('event_sequence')).cache()\n",
    "        else: \n",
    "            \n",
    "            #first pivot aka transpose and keep all events\n",
    "            pivot_vec = df_onehot.groupBy('Trace').pivot('event')\\\n",
    "            .agg(first('mal_trace'),first('vec2string')).cache()\n",
    "            #then consolidate the columns into a single event sequence\n",
    "            df_onehot = pivot_vec.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                                      array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                               'e3_first(vec2string)','e4_first(vec2string)', \n",
    "                               'e5_first(vec2string)', 'e6_first(vec2string)').alias('event_sequence')).cache()\n",
    "    else: \n",
    "        \n",
    "        #index\n",
    "        indexer = StringIndexer(inputCol='vec2string', outputCol='event_ind')\n",
    "        indexer_fitted = indexer.fit(df_onehot)\n",
    "        df_onehot = indexer_fitted.transform(df_onehot).cache()\n",
    "\n",
    "        #turn index into an integer\n",
    "        df_onehot = df_onehot.withColumn(\"event_index\", int_cast_udf(\"event_ind\")).cache()\n",
    "\n",
    "        print(\"indexing time: \"+ str(time.time() - start_time))\n",
    "\n",
    "        #Generate a list of columns to drop\n",
    "        keep_cols = ['mal_trace','Trace','event','vec2string',\"event_index\"]\n",
    "        drop_cols = [col for col in df_onehot.columns  \n",
    "                     if col not in list_cols and col not in keep_cols]\n",
    "    \n",
    "        #i want to drop any columnn not in the column list or is the malicious column\n",
    "        df_onehot = df_onehot.drop(*drop_cols).cache()\n",
    "        \n",
    "        #now do it for the indices\n",
    "        pivot_ind = df_onehot.groupBy('Trace').pivot('event').agg(first('mal_trace'),\n",
    "                                        first('event_index')).cache()\n",
    "        df_onehot = pivot_ind.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                                  array('e1_first(event_index)', 'e2_first(event_index)',\n",
    "                           'e3_first(event_index)','e4_first(event_index)', \n",
    "                           'e5_first(event_index)', 'e6_first(event_index)').alias('event_sequence')).cache()\n",
    "        \n",
    "    print(\"total elapsed time: \"+ str(time.time() - start_time))\n",
    "    \n",
    "    return df_onehot,dict_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b5645f9-855d-4a1d-8a46-a0c22570ce8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_after(df,day,trace,div, des):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #split the df in quarters\n",
    "    parts = df.randomSplit(div*[1.0/div], seed=42)\n",
    "    \n",
    "    print(\"split time: \" + str(time.time() - start_time))\n",
    "    \n",
    "    #now develop file name for the writes of each part to S3\n",
    "    s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "    day = str(day)\n",
    "    trace = str(trace)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for part in parts:\n",
    "        count+=1\n",
    "        cnt = str(count)\n",
    "        print(\"write: part \" + str(cnt)) \n",
    "        part.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\")\\\n",
    "        .parquet(f'''{s3_url_trusted}/prod/graph/encoded/real/\n",
    "                    {des}/{day}Sep/{trace}/part_{cnt}''')\n",
    "        \n",
    "        print(\"part \" + cnt + \" write time: \" + str(time.time() - start_time))\n",
    "        \n",
    "    print(\"total write time: \" + str(time.time() - start_time))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4313c5-1cfe-4cb0-a011-a0fd919f68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
