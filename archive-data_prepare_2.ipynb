{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861e74cd-997b-4f7f-9503-8d31085f4726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9a51c2eb-f440-4cab-b356-a760c19697eb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.2-spark3.2-s_2.12!graphframes.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (39ms)\n",
      ":: resolution report :: resolve 834ms :: artifacts dl 93ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9a51c2eb-f440-4cab-b356-a760c19697eb\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (281kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 12:47:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 12:47:11 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ddd8d-73f2-4471-8669-fdaeee193da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a82bc-e9f2-4db1-9edd-849f73909ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d6088-4df5-4448-b60d-8a91cb14cebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = df_all.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f01d3-3c2d-4803-b12c-d84b7273f0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2f125-b5cf-40c5-9f6b-129221793d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8e5c7-2199-48f6-b720-8b21e0527280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot = df_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78aaadd-a938-48d8-bc5a-d380920b4813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check days and malicious counts\n",
    "df_all.groupBy(\"event_day\", \"malicious\") \\\n",
    "              .count().cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf3a0c-4831-427d-8232-5d6a9205a6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = df_all.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76baa3-d6a6-48ee-9e45-21140f872a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#how many image paths are there\n",
    "df_all.groupBy('image_path_ext','parent_path_ext').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c0926-20bc-4a46-8bc9-e06a3ee5cd23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_img = ['None','empty','.exe','.com']\n",
    "list_par_img = ['None','empty','.exe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90e32d-879a-422e-9872-a758b0b32887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#how many new paths are there\n",
    "df_all.select('new_path_ext').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da573d-cba5-42c6-ae3c-4053d90357ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#what are the new path counts\n",
    "df_all.groupBy('new_path_ext').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327b1df-7e22-4de3-a5da-20af704d2748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#how many file paths are there\n",
    "df_all.select('file_path_ext').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541406b7-8675-4586-aa57-34866aee06c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#how many file paths are there\n",
    "ext_ratios = df_all.groupBy('file_path_ext','malicious').count().sort(col('count').desc()).withColumn('perc_of_count_total', (col('count') / tot) * 100).limit(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2dd4f-7fac-4ecb-975a-85b7cd8a928d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext_ratios.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c921200-e1e3-47fc-afa6-334464879462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a list of top 20 file extensions\n",
    "file_ext = list(df_all.groupBy('file_path_ext').count().sort(col('count').desc()).limit(9).toPandas()['file_path_ext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a5b10-5c30-4926-bdf5-6d959e78723c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = df_all.withColumn(\"image_path\", getFileUDF(col(\"image_path\"))).cache()\n",
    "df_all = df_all.withColumn(\"parent_image_path\", getFileUDF(col(\"parent_image_path\"))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40372b-c76e-4bed-9e6a-ec630c2b4798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#how many file paths are there\n",
    "ext_ratios_img = df_all.groupBy('image_path').count().sort(col('count').desc()).withColumn('perc_of_count_total', (col('count') / tot) * 100).limit(20)\n",
    "ext_ratios_parent = df_all.groupBy('parent_image_path').count().sort(col('count').desc()).withColumn('perc_of_count_total', (col('count') / tot) * 100).limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2d7de-7760-4f7d-9f15-44d4b009893f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ext_ratios_img.show()\n",
    "ext_ratios_parent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638c9c4-7f72-4329-ac0b-382fdfb34e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a list of top 20 file extensions\n",
    "img_path = list(df_all.groupBy('image_path').count().sort(col('count').desc()).limit(16).toPandas()['image_path'])\n",
    "#create a list of top 20 file extensions\n",
    "par_path = list(df_all.groupBy('parent_image_path').count().sort(col('count').desc()).limit(6).toPandas()['parent_image_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc7f1e-d247-4456-b7de-ea69314174ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(img_path)\n",
    "print(par_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2288f8-1c25-4d26-ac22-ee6b9b75ceb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23 = df_all.where(col('event_day') == 23).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52034829-befb-4a5f-88aa-78228ed7acb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_1516 = df_23.where((col('event_hour') == 16) | (col('event_hour') == 15)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d967636-a44c-422a-91f0-72f176639672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_1516.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35450811-3973-440c-ae78-fad4b6ad93b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_15 = df_23.where((col('event_hour') == 15)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d51a3-d80c-428c-8307-1adf0651c361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_15.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73835f-1078-4394-8dee-d6e6c32a9bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now create graph for event traces of 2 events, 4 events, and 6 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c)\") #; (c)-[e3]->(d); (d)-[e4]->(e); (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_2 = motifs6.filter('''e1.timestamp <= e2.timestamp''').cache()# and e2.timestamp <= e3.timestamp and \n",
    "                                    #e3.timestamp <= e4.timestamp and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_2.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530cc343-e705-4340-a80b-c9132a3bf1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now create graph for event traces of 2 events, 3 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('actorID as src', 'objectID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_3 = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                    #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_3.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcdb176-39c4-45a1-ae4a-d44e7765492e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now create graph for event traces of 2 events, 3 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('actorID as src', 'objectID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_3 = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                    #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_3.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907eb93-5169-4d18-af78-e21fc879a5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now create graph for event traces of 2 events, 3 events\n",
    "start_time = time.time()\n",
    "    \n",
    "#create trace matrix from malicious events for speed. \n",
    "# Create distinct vertices with source as actorid, destination as objectid for malicious\n",
    "src_vertices = df_23_15.selectExpr('objectID as id').distinct()\n",
    "dst_vertices = df_23_15.selectExpr('actorID as id').distinct()\n",
    "vertices = src_vertices.union(dst_vertices).distinct()\n",
    "\n",
    "# Create edges by using timestamp as an edge\n",
    "edges = df_23_15.selectExpr('objectID as src', 'actorID as dst', 'timestamp', 'object', 'action', 'hostname', 'user_name', 'privileges', 'image_path',\n",
    "                          'parent_image_path', 'new_path', 'file_path', 'direction', 'logon_id', 'requesting_domain', 'requesting_user', 'malicious')\n",
    "\n",
    "# Create GraphFrame\n",
    "g = GraphFrame(vertices, edges)\n",
    "motifs6 = g.find(\"(a)-[e1]->(b); (b)-[e2]->(c); (c)-[e3]->(d)\") #; (d)-[e4]->(e)\") #; (e)-[e5]->(f); (f)-[e6]->(g)\")\n",
    "print(\"found connections: \"+ str(time.time() - start_time))\n",
    "#create paths and count\n",
    "# filter paths to only those where all edges are connected\n",
    "cp_2315_3oa = motifs6.filter('''e1.timestamp <= e2.timestamp and e2.timestamp <= e3.timestamp''')# and \n",
    "                                    #e3.timestamp <= e4.timestamp''').cache()# and e4.timestamp <= e5.timestamp and \n",
    "                                    #e5.timestamp <= e6.timestamp''').cache()\n",
    "\n",
    "print(\"event traces: \"+str(cp_2315_3oa.count()))\n",
    "\n",
    "print(\"create graph: \"+ str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1245a6-7832-429b-8e6e-c60daf902d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transp = cp_2315_3oa.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "df_transp = df_transp.select(\"Trace\", \n",
    "                             *[col for col in df_transp.columns if col != \"Trace\"])\n",
    "\n",
    "#drop all vertices\n",
    "df_transp = df_transp.drop('a','b','c','d')\n",
    "\n",
    "#transpose rows \n",
    "stacked_df = df_transp.selectExpr(\n",
    "    \"Trace\", \n",
    "    \"posexplode(array(e1, e2, e3)) as (pos, col)\"\n",
    ").select(\n",
    "    \"Trace\", \n",
    "    expr('''CASE pos \n",
    "    WHEN 0 THEN 'e1' \n",
    "    WHEN 1 THEN 'e2'\n",
    "    ELSE 'e3' END''').alias(\"event\"),\n",
    "    \"col\"\n",
    ").orderBy(\"Trace\",\"event\")\n",
    "\n",
    "#explode columns\n",
    "df_onehot_oa = stacked_df.select(*stacked_df.columns, \"col.*\").drop('col').cache()\n",
    "\n",
    "df_transp = cp_2315_3.withColumn(\"Trace\", (monotonically_increasing_id() + 1))\n",
    "df_transp = df_transp.select(\"Trace\", \n",
    "                             *[col for col in df_transp.columns if col != \"Trace\"])\n",
    "\n",
    "#drop all vertices\n",
    "df_transp = df_transp.drop('a','b','c','d')\n",
    "\n",
    "#transpose rows \n",
    "stacked_df = df_transp.selectExpr(\n",
    "    \"Trace\", \n",
    "    \"posexplode(array(e1, e2, e3)) as (pos, col)\"\n",
    ").select(\n",
    "    \"Trace\", \n",
    "    expr('''CASE pos \n",
    "    WHEN 0 THEN 'e1' \n",
    "    WHEN 1 THEN 'e2'\n",
    "    ELSE 'e3' END''').alias(\"event\"),\n",
    "    \"col\"\n",
    ").orderBy(\"Trace\",\"event\")\n",
    "\n",
    "#explode columns\n",
    "df_onehot = stacked_df.select(*stacked_df.columns, \"col.*\").drop('col').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6237c-24e1-477c-b86a-c8d4b7cd7b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot_oa.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35f5cc-f954-40e5-9688-db72ec3681dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673cc76-b71a-4e44-8d10-b19aa1ef30d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d846f5-2544-4c91-ac00-272e1c17353f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#write both samples to parquet\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "cp_2315_2.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/test/2315_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9b087-4ee7-40fa-bcd2-34d4a1c0bc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#write both samples to parquet\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "cp_2315_4.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/test/2315_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59d1cc-2d23-4757-b951-2f627a261f30",
   "metadata": {},
   "source": [
    "## So now let's work on filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce59a6-2fe3-4c1d-8e32-4272a1d1977e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the necessary file extension filtering\n",
    "df = df_onehot_oa.withColumn(\"image_path_ext\", regexp_extract(\"image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"parent_path_ext\", regexp_extract(\"parent_image_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"new_path_ext\", regexp_extract(\"new_path\", \"\\.[0-9a-z]+$\", 0)) \\\n",
    "        .withColumn(\"file_path_ext\", regexp_extract(\"file_path\", \"\\.[0-9a-z]+$\", 0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5111bb-2e74-40b8-ba07-23c5ada1e951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd47db-fd13-43a2-8200-d8012f4b8088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupBy('file_path_ext').count().sort(col('count').desc()).withColumn('perc_of_count_total', (col('count') / tot) * 100).limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd6748d-54a7-48ea-8ff0-3015f84759b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_ext = [str(i) for i in file_ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c34539-a319-4b2e-8e0a-a26874854ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_ext = ['None', 'empty', '.dll', '.pyc', '.bat', '.pf', '.ini', '.exe', '.tmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed313e-7331-4493-a38b-f93a754d2e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"file_path_ext\", when(col(\"file_path_ext\").isNull(),\"None\").otherwise(col(\"file_path_ext\"))) \\\n",
    ".withColumn(\"file_path_ext\", when(col(\"file_path_ext\") == '', 'empty').otherwise(col(\"file_path_ext\"))) \\\n",
    ".withColumn(\"file_path_ext\", when(col(\"file_path_ext\").isin(file_ext), col('file_path_ext')).otherwise('other')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b4bf4-4d5e-49ac-9d0b-eb21245292a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_img = ['None','empty','.exe','.com']\n",
    "list_par_img = ['None','empty','.exe']\n",
    "\n",
    "df = df.withColumn(\"image_path_ext\", when(col(\"image_path_ext\").isNull(),\"None\").otherwise(col(\"image_path_ext\"))) \\\n",
    ".withColumn(\"image_path_ext\", when(col(\"image_path_ext\") == '', 'empty').otherwise(col(\"image_path_ext\"))) \\\n",
    ".withColumn(\"image_path_ext\", when(col(\"image_path_ext\").isin(list_img), col('image_path_ext')).otherwise('other')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839b21f-1fa5-4e3d-b830-1035586248c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"parent_path_ext\", when(col(\"parent_path_ext\").isNull(),\"None\").otherwise(col(\"parent_path_ext\"))) \\\n",
    ".withColumn(\"parent_path_ext\", when(col(\"parent_path_ext\") == '', 'empty').otherwise(col(\"parent_path_ext\"))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8d407-9fff-4f74-a21c-3a014f24b647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select('image_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191b9b3-94c8-4087-9265-b99420396e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select('parent_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefd101-40d5-4825-b5e8-4b08b612d47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now filter the file paths based on the 9 we selected above\n",
    "df = df.withColumn(\"file_path_ext\",\n",
    "                             when(col(\"file_path_ext\").isin(file_ext), \n",
    "                                  col('file_path_ext')).otherwise('other')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd2a10-3be3-4e1e-b0c6-fe085a35e7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check results\n",
    "df.select('file_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24d707-7a10-420e-8450-69c71d798a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select('image_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd34ac7-1369-41f5-90c7-6b762f74b12d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select('parent_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee3cd2-a269-4143-b978-f6affa4d2a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now work on labeling entire malicious trace\n",
    "# create a sample dataframe that aligns closer with what you see above. \n",
    "#I think the trick is going to be partitioning based on object action and timestamp.\n",
    "#if those three things are the same then pull the first one out\n",
    "data = [(\"A\", \"B\",1, 1,'process','create',0), \n",
    "        (\"B\", \"C\",1, 2,'process','create',0), \n",
    "        (\"C\", \"D\",1, 3,'process','open',0),\n",
    "        (\"D\", \"E\",2,4,'process','open',1),\n",
    "        (\"E\", \"F\",2, 5,'file','read',0),\n",
    "       (\"F\", \"G\",2, 6,'process','create',0), \n",
    "        (\"G\", \"H\",3, 7,'process','create',0), \n",
    "        (\"H\", \"I\",3, 8,'process','open',1),\n",
    "        (\"I\", \"J\" ,3,9,'process','open',0),\n",
    "        (\"J\", \"K\",3, 10,'file','read',1),\n",
    "       (\"K\", \"L\",4, 8,'process','open',1),\n",
    "        (\"M\", \"N\",4,9,'process','open',1),\n",
    "        (\"O\", \"P\",4, 10,'file','read',1)]\n",
    "columns = [\"src\", \"dst\",\"Trace\", \"time\",\"object\",\"action\",'malicious']\n",
    "df_samp = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6bb4f-d5a0-4172-9d09-c5bc4fff6320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a window method to label traces as malicious given one event\n",
    "w = Window.partitionBy(\"Trace\")#.orderBy(\"time\")\n",
    "\n",
    "# Apply the conditions to the window using the when function\n",
    "df_samp = df_samp.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0))\n",
    "#df_samp = df_samp.withColumn('mal_trace', sum('mal_trace').over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd7146-c877-4c79-9a21-31865e227d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_samp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394c510-ae91-4161-a4f3-69a7dad27915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df = df.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8e02e-4f99-4f31-a22b-c2df1d77925e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917983a-96fd-4dcf-970e-c0f9cc09e169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby('mal_trace').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc1d3f-2a72-427f-8cb9-e9a53e2a135b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in ['file_path_ext','image_path_ext','parent_path_ext','object','action']:\n",
    "    df, dict_mapping, cols_sparse = oneHotCol(df, c, {}, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73de53d-120b-4167-b31b-dece6b6a04a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9277cee-2acf-435e-bc98-6bc132e62637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a732199-831b-45fa-a14e-c3c0fc1c0f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df = df.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc5f91-9cbf-4c81-af59-e8353f9ba58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now adjust the timediff bins\n",
    "df = ts_diff(df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5b4e0-b920-4a37-bd35-a0b1957fc434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1de5d4-9f4a-401c-804e-0982c31ecc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#develop quantiles for timestamp diff\n",
    "bins = df.approxQuantile(\"timestamp_difference\", [0.2, 0.4, .6, .8], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c5548-c725-48fa-aeb0-110f5286e16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Bucketizer instance\n",
    "#bins = df.approxQuantile(\"timestamp_difference\", [0.2, 0.4, .6, .8], 0)\n",
    "#bins_2 = [float(\"-inf\")] + bins + [float(\"inf\")]\n",
    "#bucketizer = Bucketizer(splits=bins_2, inputCol=\"timestamp_difference\", outputCol=\"timestamp_bins_3\")\n",
    "# transform the DataFrame\n",
    "#df = bucketizer.transform(df)\n",
    "df_2 = df.withColumn(\"bin_column\", when(col(\"timestamp_difference\") <= bins[0], \"1\") \\\n",
    "              .when((col(\"timestamp_difference\") > bins[0]) & (col('timestamp_difference') <= bins[1]), \"2\") \\\n",
    "                .when((col(\"timestamp_difference\") > bins[1]) & (col('timestamp_difference') <= bins[2]), \"3\") \\\n",
    "                     .when((col(\"timestamp_difference\") > bins[2]) & (col('timestamp_difference') <= bins[3]), \"4\") \\\n",
    "                     .when((col(\"timestamp_difference\") > bins[3]), 5).otherwise('first'))\n",
    "                     \n",
    "              #((df[\"timestamp_difference\"] > percentiles[0]) & (df[\"timestamp_difference\"] <= percentiles[1]), \"2\")).otherwise(\"no\"))\n",
    "    #when((df[\"timestamp_difference\"] > percentiles[1]) & (df[\"timestamp_difference\"] <= percentiles[2]), \"3\"), \n",
    "    #when((df[\"timestamp_difference\"] > percentiles[2]) & (df[\"timestamp_difference\"] <= percentiles[3]), \"4\"),\n",
    "    #when(df[\"timestamp_difference\"] > percentiles[3], \"5\").otherwise(\"First\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a13ce-d904-4f2e-baa0-2ec8bde5a65c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f7344-134c-4a2c-b755-9ef01eaae898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = bin_it(ts_diff(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f9365-7e4d-4140-ba28-ac28e8f67d74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da942a80-6518-467b-af19-e7ad8deca7ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca266861-5eda-4bb8-b55b-cbe9816fa555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09171324-04f4-4bbb-a2d6-bf3028ed42b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72df11c-78ae-4816-825b-b99fc2a300f9",
   "metadata": {},
   "source": [
    "## Now run your function and see what happens, don't forget to update for trace 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb23403f-7d2f-4468-a1dd-c01186dd126b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adb83d-f827-4f4e-b3d0-62fe345cdaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23 = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca56f29-5d6d-4542-8853-43813bb6e9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2315 = df_23.where((col('event_hour') == 15)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbe7e6-e69b-4876-852b-bf7c540b4a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2315.select('file_path_ext').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d0368-a2fa-49dd-ad0d-10d9b3412cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2315.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9a983-5b8c-40fe-8370-88e3f41415e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_graph = create_graph3(df_2315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e19e2c-57b0-4099-8993-74484a9e62e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot = transp_expl3(df_graph).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a81e06-4a6d-4f64-8454-479a8ee931c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot.select(\"Trace\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68877afa-2da2-4d43-bef0-2f4e664d7917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df_onehot = df_onehot.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd958a-16c1-4099-bde6-f46ef0fac692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now develop time_diff bins\n",
    "df_onehot = bin_it(df_onehot).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d97f4d-2b6a-44ad-9f47-5f8a7fc6df30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot.select(\"Trace\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee3376-f4ee-4591-9240-444a4164a5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot.sort(\"Trace\").limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e45f5a-eafb-4dd7-a5f4-cbc0e20ae083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1612e31-cfbd-438a-849a-d8063b3ec2bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for all columns to one hot, one hot, preserve mapping\n",
    "for colm in ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action']+[\"timestamp_bins\"]:\n",
    "    print(colm)\n",
    "    df_onehot, dict_mapping, list_sparse = oneHotCol(df_onehot,colm, {}, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cc9a7-ebbd-43c0-83f9-08dd5cfa0f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot.select(\"Trace\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f4ea3-bdc3-4664-bc16-75d34efa84c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assemble vectors for all sparse columns - this might be enough for our ML algorithms\n",
    "assembler = VectorAssembler(inputCols=list_sparse, \n",
    "                        outputCol=\"final_vec\")\n",
    "df_onehot = assembler.transform(df_onehot).cache()\n",
    "\n",
    "list_cols = ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action']\n",
    "\n",
    "#turn into string\n",
    "df_onehot = df_onehot.withColumn(\"vec2string\", binary_to_string_array_udf(\"final_vec\")).cache()\n",
    " #Generate a list of columns to drop\n",
    "keep_cols = ['mal_trace','Trace','event','vec2string']\n",
    "drop_cols = [col for col in df_onehot.columns  \n",
    "             if col not in list_cols and col not in keep_cols]\n",
    "\n",
    "#i want to drop any columnn not in the column list or is the malicious column\n",
    "df_onehot = df_onehot.drop(*drop_cols).cache()\n",
    "#first pivot aka transpose and keep all events\n",
    "pivot_vec = df_onehot.groupBy('Trace').pivot('event')\\\n",
    ".agg(first('mal_trace'),first('vec2string')).cache()\n",
    "#then consolidate the columns into a single event sequence\n",
    "df_onehot = pivot_vec.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                          array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                   'e3_first(vec2string)').alias('event_sequence')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf338609-e96e-4ff1-8872-132cc65f11b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onehot.select(\"Trace\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4713a8-f803-4cfe-acc7-8ddc98fa2f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_enc,dict_mapping = trace_encode(df_2315, 3,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793cfee-00b9-4f0c-9e37-02bd25861a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_enc4,dict_mapping4 = trace_encode(df_2315, 4,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648555db-59ba-46bf-9a13-ca08a484a7fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read/Write examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c0abc-0f2e-45c7-b25c-cdb490e5ddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_enc.where((col('mal_trace')==1)&(col('malicious')==1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c56d2-098b-4b99-a9cc-cfd53b72d14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this math checks out \n",
    "df_enc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5cd6a-cccb-49f3-a44b-6ff63a91f944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = create_graph6(df_2315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d15788-fb12-47b4-8a56-d79d9f9cd229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_enc.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/sample3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9c5eb-0b61-4cf9-9b4a-8c904e145519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_enc4.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/sample4\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66004407-59fc-4966-8329-66d3c7a7a409",
   "metadata": {
    "tags": []
   },
   "source": [
    "#read it back in \n",
    "#now read the traces back in graph back in\n",
    "df_t_23 = spark.read.parquet(f\"{s3_url_trusted}/prod/graph/traces_23\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c2f4c-5cd3-4cf6-ae10-722b58a83ae4",
   "metadata": {},
   "source": [
    "### Create real data with just image and parent image extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673666f-8e80-41f4-8f17-c4dc95016760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23 = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cca04-8253-4599-b0b7-b7c913b26044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23 = df_23.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef6972-a888-487c-a683-d6917b9bbb61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9017b60-718e-43c0-8f22-e329915b91bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_enc_3,dict_mapping_23_3 = trace_encode(df_23, 3,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d5fdb-9941-4e33-8d54-4d874195c423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_23_enc_3.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/real/23Sep3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7360f0f-084a-4101-ae6b-a3c096858e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_mapping_23_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38d78e-f5a9-4695-ad65-f0ca25aa4d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_enc_4,dict_mapping_23_4 = trace_encode(df_23, 4,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b0f16-0f68-4cc5-a532-a690b9268bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_23_enc_6,dict_mapping_23_6 = trace_encode(df_23, 6,\n",
    "                                   ['file_path_ext','image_path_ext',\n",
    "                                    'parent_path_ext','object','action'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b10fb9-7e56-438f-8765-aadea0190a2e",
   "metadata": {},
   "source": [
    "## Prepare data with image path names and extensions and test with NB to observe any change in testing performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "118f04ea-2f1b-470a-b504-eaed879f8db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99806adc-345f-4898-8806-6f6237cf8105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:47PM UTC on Apr 01, 2023 --- read time: 6.950466871261597 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_23 = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b559c1d-c0ed-41b8-b431-8e105a9518ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:====================================>                (136 + 32) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          image_path|\n",
      "+--------------------+\n",
      "|                None|\n",
      "|          python.EXE|\n",
      "|          python.exe|\n",
      "|        Explorer.EXE|\n",
      "|       taskhostw.exe|\n",
      "|               other|\n",
      "|            PING.EXE|\n",
      "|     geckodriver.exe|\n",
      "|           csrss.exe|\n",
      "|         svchost.exe|\n",
      "|             cmd.exe|\n",
      "|         conhost.exe|\n",
      "|         firefox.exe|\n",
      "|    GoogleUpdate.exe|\n",
      "| CompatTelRunner.exe|\n",
      "|              System|\n",
      "|backgroundTaskHos...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_23.select('image_path').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a12ee383-e634-47ec-b688-b10bccc85d3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|parent_image_path|\n",
      "+-----------------+\n",
      "|             None|\n",
      "|            other|\n",
      "|        csrss.exe|\n",
      "|      svchost.exe|\n",
      "|          cmd.exe|\n",
      "|      conhost.exe|\n",
      "| GoogleUpdate.exe|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_23.select('parent_image_path').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802a96a0-4670-4976-ac8d-71355ffbae8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>(19 + 32) / 200][Stage 10:> (0 + 0) / 200][Stage 11:> (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.7195041179656982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 17373568\n"
     ]
    }
   ],
   "source": [
    "#run specific elements of trace encode with 23Sep data to get timestamp bins\n",
    "#first create the graph. This is where I will split the data later\n",
    "graph_3, tot = create_graph3(df_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc167566-0721-492f-aed5-262ea7c49adf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.3261566162109375\n"
     ]
    }
   ],
   "source": [
    "#now run transp_explode\n",
    "start_time = time.time()\n",
    "\n",
    "graph_3 = graph_3.cache()\n",
    "trace_23_3 = transp_expl3(graph_3)\n",
    "trace_23_3 = trace_23_3.cache()\n",
    "\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f95eb0e1-6ec5-4242-86d3-af324cd3990f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 312.6835699081421\n"
     ]
    }
   ],
   "source": [
    "#now bin this shit to get timestamp bins\n",
    "start_time = time.time()\n",
    "\n",
    "tf_bins = []\n",
    "trace_23_3, tf_bins = bin_it(trace_23_3,0)\n",
    "trace_23_3 = trace_23_3.cache()\n",
    "\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39ce33f-ec24-4ecf-82ce-af39ada8f5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25506.00004196167, 416052.00004577637, 2879809.000015259, 8122484.999895096]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4344987-a8d6-4bec-b8ce-abf1fa38ffdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#for all columns to one hot, one hot, preserve mapping -> doing this to get 23Sep3 tf bin mapping\n",
    "dict_mapping = {}\n",
    "list_sparse = []\n",
    "df_onehot = trace_23_3\n",
    "\n",
    "for colm in ['file_path_ext','image_path',\n",
    "                                    'parent_image_path','object','action',\"timestamp_bins\"]:#,\"pagerank_bins\"]:\n",
    "    #print(colm)\n",
    "    df_onehot, dict_mapping, list_sparse = oneHotCol(df_onehot,colm, dict_mapping, list_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c37d230d-d11a-44b6-9ac8-f4d8dbea6191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None', 'empty', '.dll', 'other', '.pyc', '.ini', '.pf', '.tmp', '.exe', '.bat']\n",
      "['GoogleUpdate.exe', 'other', 'svchost.exe', 'python.exe', 'firefox.exe', 'Explorer.EXE', 'conhost.exe', 'None', 'System', 'cmd.exe', 'csrss.exe', 'PING.EXE', 'backgroundTaskHost.exe', 'taskhostw.exe', 'geckodriver.exe', 'python.EXE', 'CompatTelRunner.exe']\n",
      "['None', 'GoogleUpdate.exe', 'other', 'svchost.exe', 'conhost.exe', 'cmd.exe', 'csrss.exe']\n",
      "['PROCESS', 'FILE', 'FLOW', 'SHELL']\n",
      "['OPEN', 'READ', 'MODIFY', 'INFO', 'CREATE', 'WRITE', 'DELETE', 'RENAME', 'COMMAND']\n",
      "['4.0', '1.0', '0.0', '2.0', '3.0']\n"
     ]
    }
   ],
   "source": [
    "for k in dict_mapping.keys():\n",
    "    print(dict_mapping[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26385fde-3a4d-479e-befe-67da80f00e49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.24323320388793945\n"
     ]
    }
   ],
   "source": [
    "#run bin it again with pre-definied bins\n",
    "start_time = time.time()\n",
    "\n",
    "#tf_bins = []\n",
    "trace_23_3 = bin_it(trace_23_3, tf_bins, 0)\n",
    "trace_23_3 = trace_23_3.cache()\n",
    "\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c91914f-5799-4133-928b-929a0226de01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.34658265113830566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 17373568\n",
      "create graph: 18.293725967407227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transposed explode and bin: 133.16281723976135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot time: 322.17062401771545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elapsed time: 345.9525034427643\n"
     ]
    }
   ],
   "source": [
    "#confirmed both image lists so now run the \n",
    "df_23_enc_3,dict_mapping_23_3 = trace_encode(df_23, 3,\n",
    "                                   ['file_path_ext','image_path',\n",
    "                                    'parent_image_path','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28c048a5-41fa-4157-95f7-30d384a707aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0]\r"
     ]
    }
   ],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_23_enc_3.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/real/23Sep3_paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381cd49-d8ac-441f-9b33-34f6f1878dfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### generate 6 event trace with pagerank and write to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c491c941-7fcc-462f-a363-1f78542cce94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c8db1011-b255-4c39-9569-9072f873ba5c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 148ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c8db1011-b255-4c39-9569-9072f873ba5c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 18:16:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 18:16:19 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/04/01 18:16:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/01 18:16:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c62a486d-226c-40d2-9755-8f5e6d498145",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:20PM UTC on Apr 01, 2023 --- read time: 0.4017181396484375 seconds ---\n",
      "found connections: 0.8320086002349854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event traces: 14384678\n",
      "create graph: 25.145016193389893\n",
      "transp-explode: 25.734282970428467\n",
      "transposed explode and bin: 26.262259244918823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot time: 120.77667164802551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elapsed time: 123.36127758026123\n"
     ]
    }
   ],
   "source": [
    "df_23 = de_dupe()\n",
    "#confirmed both image lists so now run the \n",
    "df_23_enc_6,dict_mapping_23_6 = trace_encode(df_23, 6,\n",
    "                                   ['file_path_ext','image_path',\n",
    "                                    'parent_image_path','object','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2e8c7a3-d3fc-43c0-bc15-ed6dd538a784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None', 'empty', '.dll', 'other', '.pyc', '.ini', '.tmp', '.pf', '.exe', '.bat']\n",
      "['GoogleUpdate.exe', 'other', 'svchost.exe', 'csrss.exe', 'Explorer.EXE', 'python.exe', 'conhost.exe', 'firefox.exe', 'cmd.exe', 'None', 'System', 'PING.EXE', 'backgroundTaskHost.exe', 'taskhostw.exe', 'geckodriver.exe', 'python.EXE', 'CompatTelRunner.exe']\n",
      "['GoogleUpdate.exe', 'other', 'None', 'svchost.exe', 'csrss.exe', 'conhost.exe', 'cmd.exe']\n",
      "['PROCESS', 'FILE', 'FLOW', 'SHELL']\n",
      "['OPEN', 'READ', 'MODIFY', 'INFO', 'CREATE', 'WRITE', 'DELETE', 'RENAME', 'COMMAND']\n",
      "['0.0', '1.0', '3.0', '2.0', '4.0']\n"
     ]
    }
   ],
   "source": [
    "for k in dict_mapping_23_6.keys():\n",
    "    print(dict_mapping_23_6[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8cac26f-2636-46f0-8229-923f0a5af2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_23_enc_6.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\").parquet(f\"{s3_url_trusted}/prod/graph/encoded/real/23Sep6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9707e2-ab9a-4823-918a-ef82ee64980f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now automate encoding and parquet writing for 3 and 6 event traces for 24-25Sep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71715d0c-ab90-4568-a6be-7f6ee00ccab7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b5bc2923-b725-4d43-b777-00bb90be6856;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 102ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b5bc2923-b725-4d43-b777-00bb90be6856\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/03 03:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/03 03:25:07 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/04/03 03:25:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/03 03:25:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb\n",
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98f0c8e-c5e0-4f5e-b398-cd2c4498465f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:25AM UTC on Apr 03, 2023 --- read time: 4.108601331710815 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#count number of traces in 24Sep and 25 sep\n",
    "df_24 = de_dupe(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ad1144-538b-4c33-8890-7f64050e4a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.9773728847503662\n"
     ]
    }
   ],
   "source": [
    "df_24 = create_graph3(df_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21a03c8-f710-4b08-a569-57598e518a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_24 = transp_expl3(df_24)\n",
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df_24 = df_24.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()\n",
    "#pr_bins = []\n",
    "#now develop time_diff bins, include tot for chunking \n",
    "df_24 = bin_it(df_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e8e26d-28c4-4531-908f-8a204887b660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_24_m = df_24.filter(col('mal_trace')==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c10c9e4-f89e-49bd-9fbc-bb6d7f838d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now down sample benign events \n",
    "#first extract those events\n",
    "df_24_b = df_24.filter(col('mal_trace')==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc6fe4d-17c0-43cc-8572-a37874a095f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now filter on trace number\n",
    "df_24_b = df_24_b.filter((col('Trace') <= 8235205) & \n",
    "                         (col('Trace') > 24705615))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14e588e-0e50-4898-8fc1-eb47fea53ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rejoin benign and malicious traces\n",
    "df_24 = df_24_b.union(df_24_m).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd531d0-f7d9-4ae3-b732-c03f93b60303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dict_mapping = {}\n",
    "list_sparse = []\n",
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'file_path_ext', dict_mapping, list_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f669f2e1-85b8-4ba7-8b52-90c167dd1bda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>object</th>\n",
       "      <th>action</th>\n",
       "      <th>hostname</th>\n",
       "      <th>user_name</th>\n",
       "      <th>privileges</th>\n",
       "      <th>...</th>\n",
       "      <th>new_path</th>\n",
       "      <th>file_path</th>\n",
       "      <th>direction</th>\n",
       "      <th>logon_id</th>\n",
       "      <th>requesting_domain</th>\n",
       "      <th>requesting_user</th>\n",
       "      <th>malicious</th>\n",
       "      <th>mal_trace</th>\n",
       "      <th>timestamp_bins</th>\n",
       "      <th>file_path_ext_sparse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240</td>\n",
       "      <td>e2</td>\n",
       "      <td>73d89b62-3424-41ea-9cb9-f2219035997d</td>\n",
       "      <td>59cfbe40-9d95-4ebc-a713-16d1c78a46d5</td>\n",
       "      <td>2019-09-24 20:21:29.083</td>\n",
       "      <td>PROCESS</td>\n",
       "      <td>OPEN</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2404</td>\n",
       "      <td>e1</td>\n",
       "      <td>bcd4a5c8-7f1b-4809-81db-ce4b4a9e6890</td>\n",
       "      <td>73d89b62-3424-41ea-9cb9-f2219035997d</td>\n",
       "      <td>2019-09-24 14:09:27.523</td>\n",
       "      <td>FILE</td>\n",
       "      <td>MODIFY</td>\n",
       "      <td>SysClient0419.systemia.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\System32\\WDI\\{...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trace event                                   src  \\\n",
       "0   1240    e2  73d89b62-3424-41ea-9cb9-f2219035997d   \n",
       "1   2404    e1  bcd4a5c8-7f1b-4809-81db-ce4b4a9e6890   \n",
       "\n",
       "                                    dst               timestamp   object  \\\n",
       "0  59cfbe40-9d95-4ebc-a713-16d1c78a46d5 2019-09-24 20:21:29.083  PROCESS   \n",
       "1  73d89b62-3424-41ea-9cb9-f2219035997d 2019-09-24 14:09:27.523     FILE   \n",
       "\n",
       "   action                    hostname user_name privileges  ... new_path  \\\n",
       "0    OPEN  SysClient0419.systemia.com      None       None  ...     None   \n",
       "1  MODIFY  SysClient0419.systemia.com      None       None  ...     None   \n",
       "\n",
       "                                           file_path direction logon_id  \\\n",
       "0                                               None      None     None   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\System32\\WDI\\{...      None     None   \n",
       "\n",
       "  requesting_domain requesting_user malicious mal_trace  timestamp_bins  \\\n",
       "0              None            None         0         0             0.0   \n",
       "1              None            None         0         0             NaN   \n",
       "\n",
       "                                file_path_ext_sparse  \n",
       "0  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_24.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea5fb6cc-bdeb-4377-bccb-5bd657543431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for col in ['image_path','parent_image_path']:\n",
    "    df_24, dict_mapping, list_sparse = oneHotCol(df_24,col, dict_mapping, list_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be87f4c-d101-41e4-911b-8daa3b0167cf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_24' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_24\u001b[49m\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_24' is not defined"
     ]
    }
   ],
   "source": [
    "df_24.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c30ebc8-051c-4d07-bff9-473c195a5d23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'object', dict_mapping, list_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481e8d21-6f27-42e1-8d4b-55ca5f050b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'action', dict_mapping, list_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99daac78-dc4d-42e6-a481-2536b6627cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'timestamp_bins', dict_mapping, list_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26d4f732-a740-4844-a879-dfdfd7386124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assemble vectors for all sparse columns - this might be enough for our ML algorithms\n",
    "assembler = VectorAssembler(inputCols=list_sparse, \n",
    "                        outputCol=\"final_vec\")\n",
    "\n",
    "df_24 = assembler.transform(df_24).cache()\n",
    "\n",
    "#turn into string\n",
    "df_24 = df_24.withColumn(\"vec2string\", binary_to_string_array_udf(\"final_vec\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "090133ce-0174-4f01-9fe5-4996a886011c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file_path_ext_sparse',\n",
       " 'image_path_sparse',\n",
       " 'parent_image_path_sparse',\n",
       " 'object_sparse',\n",
       " 'action_sparse',\n",
       " 'timestamp_bins_sparse']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80344772-e222-4e02-9c8b-ab34afdc7076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_cols = ['file_path_ext','image_path',\n",
    "             'parent_image_path','object',\n",
    "             'action','timestamp_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf2eaff9-3b24-43f5-a6ae-5457b7cf9b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>event</th>\n",
       "      <th>mal_trace</th>\n",
       "      <th>vec2string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158106</td>\n",
       "      <td>e1</td>\n",
       "      <td>1</td>\n",
       "      <td>010000000100000000010000000010001000000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158106</td>\n",
       "      <td>e2</td>\n",
       "      <td>1</td>\n",
       "      <td>100000000001000000000100000100010000000001000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Trace event  mal_trace                                     vec2string\n",
       "0  158106    e1          1  010000000100000000010000000010001000000000001\n",
       "1  158106    e2          1  100000000001000000000100000100010000000001000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_24.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23736ec-ea76-4b60-bb68-1e2c07c1c53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trace: long (nullable = false)\n",
      " |-- event: string (nullable = false)\n",
      " |-- mal_trace: integer (nullable = false)\n",
      " |-- vec2string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_24.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b813981-55f2-4c84-a717-c15a68a76f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keep_cols = ['mal_trace','Trace','event','vec2string']\n",
    "drop_cols = [col for col in df_24.columns  \n",
    "             if col not in list_cols and col not in keep_cols]\n",
    "\n",
    "#i want to drop any columnn not in the column list or is the malicious column\n",
    "df_24 = df_24.drop(*drop_cols).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20d7fc8f-b918-4ede-ab67-ac61286b3e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#filter df by traces again\n",
    "df_24_1 = df_24.filter(col('Trace') <= 8235205) #df_24.where(col(\"Trace\") <= 8235205) \n",
    "df_24_2 = df_24.filter((col('Trace') > 24705615) | \n",
    "                      (col(\"mal_trace\") == 1)) #df_24.where(col(\"Trace\") > 24705615) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46779db-4575-46ff-a092-08ee41dded3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Trace: bigint, mal_trace: int, event_sequence: array<string>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first pivot aka transpose and keep all events\n",
    "df_24_1 = df_24_1.groupBy('Trace').pivot('event')\\\n",
    ".agg(first('mal_trace'),first('vec2string')).cache()\n",
    "#then consolidate the columns into a single event sequence\n",
    "df_24_1 = df_24_1.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                          array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                   'e3_first(vec2string)').alias('event_sequence')).cache()\n",
    "\n",
    "#first pivot aka transpose and keep all events\n",
    "df_24_2 = df_24_2.groupBy('Trace').pivot('event')\\\n",
    ".agg(first('mal_trace'),first('vec2string')).cache()\n",
    "#then consolidate the columns into a single event sequence\n",
    "df_24_2 = df_24_2.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                          array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                   'e3_first(vec2string)').alias('event_sequence')).cache()\n",
    "\n",
    "df_24 = df_24_1.union(df_24_2)\n",
    "\n",
    "df_24_1.unpersist()\n",
    "df_24_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c18af7e-7183-49f3-8ee9-76c5353cbd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_24 = df_24.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a18743b0-309f-4c95-9782-5c8f9f41e788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trace</th>\n",
       "      <th>mal_trace</th>\n",
       "      <th>event_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158106</td>\n",
       "      <td>1</td>\n",
       "      <td>[010000000100000000010000000010001000000000001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167219</td>\n",
       "      <td>1</td>\n",
       "      <td>[100000000100000000010000000001000010000000001...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Trace  mal_trace                                     event_sequence\n",
       "0  158106          1  [010000000100000000010000000010001000000000001...\n",
       "1  167219          1  [100000000100000000010000000001000010000000001..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_24.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47dbc5bd-fef6-429b-8b19-f494e1b2d901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None', 'empty', '.dll', 'other', '.ini', '.exe', '.pf', '.tmp']\n",
      "['other', 'GoogleUpdate.exe', 'svchost.exe', 'csrss.exe', 'conhost.exe', 'cmd.exe', 'taskhostw.exe', 'PING.EXE', 'Explorer.EXE']\n",
      "['None', 'GoogleUpdate.exe', 'svchost.exe', 'other', 'csrss.exe', 'conhost.exe', 'cmd.exe']\n",
      "['PROCESS', 'FILE', 'SHELL']\n",
      "['OPEN', 'READ', 'CREATE', 'COMMAND', 'MODIFY', 'WRITE', 'DELETE']\n",
      "['4.0', '3.0', '2.0', '0.0', '1.0']\n"
     ]
    }
   ],
   "source": [
    "for k in dict_mapping.keys():\n",
    "    print(dict_mapping[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f433c249-0d9c-4e53-b0b2-d30ac0714d95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split time: 1.5885505676269531\n",
      "write: part 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 1 write time: 209.6388964653015\n",
      "write: part 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 2 write time: 423.24907088279724\n",
      "write: part 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 3 write time: 625.9810876846313\n",
      "write: part 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 4 write time: 822.8803811073303\n",
      "write: part 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 5 write time: 1022.060403585434\n",
      "write: part 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 6 write time: 1230.0164246559143\n",
      "write: part 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 7 write time: 1440.7981452941895\n",
      "write: part 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 8 write time: 1660.2410397529602\n",
      "write: part 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 9 write time: 1861.268610715866\n",
      "write: part 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 10 write time: 2066.009083032608\n",
      "write: part 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 11 write time: 2266.3800570964813\n",
      "write: part 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 12 write time: 2470.998607635498\n",
      "write: part 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 13 write time: 2684.2194674015045\n",
      "write: part 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 14 write time: 2888.9532198905945\n",
      "write: part 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 15 write time: 3103.960657596588\n",
      "write: part 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 16 write time: 3322.5822825431824\n",
      "write: part 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 17 write time: 3531.944423675537\n",
      "write: part 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 18 write time: 3737.7635962963104\n",
      "write: part 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 19 write time: 3949.1249299049377\n",
      "write: part 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 20 write time: 4162.245982885361\n",
      "total write time: 4162.246063947678\n"
     ]
    }
   ],
   "source": [
    "#now write this data to parquet\n",
    "write_after(df_24,24,3,20,\"actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2e7f1-646e-4ea6-b747-c9f42bdc0474",
   "metadata": {},
   "source": [
    "### Now create the df for 24Sep trace 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3f998c-a826-41b8-9449-c2defc54e003",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a780743d-4200-4ebc-911e-75d3c137c869;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 138ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a780743d-4200-4ebc-911e-75d3c137c869\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/03 08:33:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/03 08:33:15 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/04/03 08:33:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/03 08:33:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb\n",
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec0cec-3bc9-444d-bab4-4c191fde2f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8:33AM UTC on Apr 03, 2023 --- read time: 4.531464576721191 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df_24 = de_dupe(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e33fb2b-fe05-4730-b1ea-aeb5b263c13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 1.666179895401001\n"
     ]
    }
   ],
   "source": [
    "df_24 = create_graph6(df_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0142f373-8fa2-4a27-800e-f4875451e4c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.              (0 + 32) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m s3_url_trusted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://sapient-bucket-trusted/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[43mdf_24\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxRecordsPerFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms3_url_trusted\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/prod/graph/encoded/real/\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m                    unprocessed/24Sep6\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO: use this snippet to write graph df into S3\n",
    "s3_url_trusted = \"s3a://sapient-bucket-trusted/\"\n",
    "df_24.write.option(\"maxRecordsPerFile\", 300000).mode(\"overwrite\")\\\n",
    "        .parquet(f'''{s3_url_trusted}/prod/graph/encoded/real/\n",
    "                    unprocessed/24Sep6''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8895f4-94c5-4f2a-9a4f-5b7619e4f94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_24 = transp_expl(df_24)\n",
    "#calculate malicious trace then check schema and how many were made \n",
    "w = Window.partitionBy(\"Trace\")\n",
    "df_24 = df_24.withColumn('mal_trace', when(sum('malicious').over(w) > 0, 1).otherwise(0)).cache()\n",
    "#now develop time_diff bins, include tot for chunking \n",
    "df_24 = bin_it(df_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c7c3bb-e9a4-4209-9ffb-78489ea77799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_24_m = df_24.filter(col('mal_trace')==1)\n",
    "df_24_b = df_24.filter(col('mal_trace')==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b34a51-1d06-47f0-a70b-bdd9c02b782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now filter on trace number\n",
    "df_24_b = df_24_b.filter((col('Trace') > 8235205) & \n",
    "                         (col('Trace') <= 12352808) & \n",
    "                         (col('Trace') > 24705615)& \n",
    "                         (col('Trace') <= 28823217))\n",
    "#rejoin benign and malicious traces\n",
    "df_24 = df_24_b.union(df_24_m).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069c423-2eec-473f-949b-e2fca63b39ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===============================>                     (117 + 32) / 200]\r"
     ]
    }
   ],
   "source": [
    "dict_mapping = {}\n",
    "list_sparse = []\n",
    "start_time = time.time()\n",
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'file_path_ext', dict_mapping, list_sparse)\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cec778-a98c-436c-9c53-ec9d9bf6c016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for col in ['image_path','parent_image_path']:\n",
    "    df_24, dict_mapping, list_sparse = oneHotCol(df_24,col, dict_mapping, list_sparse)\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96615822-3fc4-485d-9f83-034401b8688f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'object', dict_mapping, list_sparse)\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d07b2-9682-4d8e-bbcf-97418b5cfc16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'action', dict_mapping, list_sparse)\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5d838-3cc5-40fd-8f90-ede68ef58784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_24, dict_mapping, list_sparse = oneHotCol(df_24,'timestamp_bins', dict_mapping, list_sparse)\n",
    "print(\"elapsed time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768dff0d-2b5c-45a6-bda0-4d19787c2a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#assemble vectors for all sparse columns - this might be enough for our ML algorithms\n",
    "assembler = VectorAssembler(inputCols=list_sparse, \n",
    "                        outputCol=\"final_vec\")\n",
    "\n",
    "df_24 = assembler.transform(df_24).cache()\n",
    "\n",
    "#turn into string\n",
    "df_24 = df_24.withColumn(\"vec2string\", binary_to_string_array_udf(\"final_vec\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a7f53-8070-4e61-a137-669807a6708d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_cols = ['file_path_ext','image_path',\n",
    "             'parent_image_path','object',\n",
    "             'action','timestamp_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec3be3-2252-4ceb-975e-ac8ac78a219f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keep_cols = ['mal_trace','Trace','event','vec2string']\n",
    "drop_cols = [col for col in df_24.columns  \n",
    "             if col not in list_cols and col not in keep_cols]\n",
    "\n",
    "#i want to drop any columnn not in the column list or is the malicious column\n",
    "df_24 = df_24.drop(*drop_cols).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032bea4-54c3-4330-943d-ede1602725b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#filter df by traces again\n",
    "df_24_1 = df_24.filter((col('Trace') > 8235205) & \n",
    "                         (col('Trace') <= 12352808)) \n",
    "df_24_2 = df_24.filter(((col('Trace') > 24705615) & \n",
    "                         (col('Trace') <= 28823217))| \n",
    "                      (col(\"mal_trace\") == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d055dfd-b341-43ad-a521-b6ec57f0d609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#first pivot aka transpose and keep all events\n",
    "df_24_1 = df_24_1.groupBy('Trace').pivot('event')\\\n",
    ".agg(first('mal_trace'),first('vec2string')).cache()\n",
    "#then consolidate the columns into a single event sequence\n",
    "df_24_1 = df_24_1.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                          array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                   'e3_first(vec2string)').alias('event_sequence')).cache()\n",
    "\n",
    "#first pivot aka transpose and keep all events\n",
    "df_24_2 = df_24_2.groupBy('Trace').pivot('event')\\\n",
    ".agg(first('mal_trace'),first('vec2string')).cache()\n",
    "#then consolidate the columns into a single event sequence\n",
    "df_24_2 = df_24_2.select('Trace',col('e1_first(mal_trace)').alias('mal_trace'),\n",
    "                          array('e1_first(vec2string)', 'e2_first(vec2string)',\n",
    "                   'e3_first(vec2string)').alias('event_sequence')).cache()\n",
    "\n",
    "df_24 = df_24_1.union(df_24_2).cache()\n",
    "\n",
    "df_24_1.unpersist()\n",
    "df_24_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e59617-a8f2-4efa-987f-5d824212bb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in dict_mapping.keys():\n",
    "    print(dict_mapping[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6594ef8-eeec-4d7e-b7bb-3dd439aaef58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now write this data to parquet\n",
    "write_after(df_24,24,6,20,\"actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc47c99-be4e-40d3-af66-deb9ec02c763",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  processing 24 Sep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:33PM UTC on Apr 01, 2023 --- read time: 136.55989837646484 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    processing 3 event traces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=============================================>       (170 + 30) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found connections: 0.6804215908050537\n",
      "create graph: 0.718543529510498\n",
      "transp-explode: 1.1523950099945068\n",
      "bin time: 1.6936383247375488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot time: 1225.5698642730713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:>                                                     (0 + 32) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 15:56:06 ERROR Executor: Exception in task 27.0 in stage 236.0 (TID 4537)\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "23/04/01 15:56:06 ERROR Executor: Exception in task 12.0 in stage 236.0 (TID 4522)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "23/04/01 15:56:09 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 12.0 in stage 236.0 (TID 4522),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "23/04/01 15:56:09 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 27.0 in stage 236.0 (TID 4537),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.reflect.Array.newInstance(Array.java:75)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "23/04/01 15:56:10 ERROR TaskSetManager: Task 27 in stage 236.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:>                                                     (0 + 32) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@23c57067 rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@6e66deb0 rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@74403b19 rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@c6134bc rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@78a2f4f0 rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@7e541bf2 rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@60f5d8fa rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@6dc13443 rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/04/01 15:56:13 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6525/192258468@51140bb9 rejected from java.util.concurrent.ThreadPoolExecutor@67deacaf[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4512]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1432.pivot.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 236.0 failed 1 times, most recent failure: Lost task 27.0 in stage 236.0 (TID 4537) (ip-172-16-26-180.us-west-2.compute.internal executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m event traces\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#encoding\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m df_enc, dict_mapping \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                               \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile_path_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparent_image_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sep-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    encoding time: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time))\n",
      "File \u001b[0;32m/tmp/ipykernel_24427/3319645323.py:73\u001b[0m, in \u001b[0;36mtrace_encode\u001b[0;34m(df, traces, list_cols, output)\u001b[0m\n\u001b[1;32m     68\u001b[0m df_onehot \u001b[38;5;241m=\u001b[39m df_onehot\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;241m*\u001b[39mdrop_cols)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m traces \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m: \n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m#first pivot aka transpose and keep all events\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     pivot_vec \u001b[38;5;241m=\u001b[39m \u001b[43mdf_onehot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;241m.\u001b[39magg(first(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmal_trace\u001b[39m\u001b[38;5;124m'\u001b[39m),first(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvec2string\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m#then consolidate the columns into a single event sequence\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     df_onehot \u001b[38;5;241m=\u001b[39m pivot_vec\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrace\u001b[39m\u001b[38;5;124m'\u001b[39m,col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me1_first(mal_trace)\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmal_trace\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     77\u001b[0m                               array(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me1_first(vec2string)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me2_first(vec2string)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     78\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me3_first(vec2string)\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mcache()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/group.py:277\u001b[0m, in \u001b[0;36mGroupedData.pivot\u001b[0;34m(self, pivot_col, values)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mPivots a column of the current :class:`DataFrame` and perform the specified aggregation.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03mThere are two versions of pivot function: one that requires the caller to specify the list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m[Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     jgd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpivot_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     jgd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd\u001b[38;5;241m.\u001b[39mpivot(pivot_col, values)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1432.pivot.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 236.0 failed 1 times, most recent failure: Lost task 27.0 in stage 236.0 (TID 4537) (ip-172-16-26-180.us-west-2.compute.internal executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.reflect.Array.newInstance(Array.java:75)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2082)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2118)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1656)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2430)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2354)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2212)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1668)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:502)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:460)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1654)\n\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:943)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1326)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary to preserve mappings for everything\n",
    "mapping = {}\n",
    "start_time = time.time()\n",
    "for d in [24,25]:\n",
    "    \n",
    "    print(f\"  processing {d} Sep\")\n",
    "    #create dataframe before breaking it down into traces\n",
    "    df = de_dupe(d)\n",
    "    \n",
    "    for trace in [3,6]: \n",
    "        \n",
    "        print(f\"    processing {trace} event traces\")\n",
    "        \n",
    "        #encoding\n",
    "        df_enc, dict_mapping = trace_encode(df, trace,\n",
    "                                       ['file_path_ext','image_path',\n",
    "                                        'parent_image_path','object','action'])\n",
    "        \n",
    "        print(f\"    {d} Sep-{trace} encoded\")\n",
    "        print(\"    encoding time: \"+str(time.time()-start_time))\n",
    "        \n",
    "        #preserve mapping\n",
    "        mapping[f\"{d}-{trace}\"] = dict_mapping\n",
    "    \n",
    "    print(f\"  {day} Sep processing time: \"+str(time.time()-start_time))\n",
    "    \n",
    "print(\"total time: \"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238db12f-33a0-4f4d-b116-02c3f2e14957",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data processing the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918c4383-3bd4-4997-9e2b-9c9d1fa73d99",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f16eb368-7fb5-498a-b7be-3caef9613bd4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.2-spark3.2-s_2.12!graphframes.jar (46ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (33ms)\n",
      ":: resolution report :: resolve 804ms :: artifacts dl 81ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f16eb368-7fb5-498a-b7be-3caef9613bd4\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (281kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 01:08:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 01:08:22 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eaf4932-a862-4cfb-8bb3-5697228f4791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run trace_encode.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b97e65-1f82-4d37-ba6e-9fb652db87cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:09AM UTC on Apr 04, 2023 --- read time: 6.179586887359619 seconds ---\n"
     ]
    }
   ],
   "source": [
    "df = de_dupe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf868087-e3aa-4e81-9b8a-39441d6b02cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17f6ba-ec6c-4828-b047-944505a2f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write using the write after function\n",
    "write_after(df_enc, day, trace)\n",
    "print(f\"    {d}-{trace} written\")\n",
    "print(\"    write time: \"+str(time.time()-start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
