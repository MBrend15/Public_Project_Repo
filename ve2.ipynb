{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e431368d-df60-47bb-9e4b-787c63fd1531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.access.key\n",
      "Warning: Ignoring non-Spark config property: fs.s3a.secret.key\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-67259536-64d4-4b2e-b700-39d94e962a7a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 445ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-67259536-64d4-4b2e-b700-39d94e962a7a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 01:14:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 01:14:14 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/03/13 01:14:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/13 01:14:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "%run read_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58538063-4347-4fee-b6ad-0ee65a4cd5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run build_adjacency.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb6f5c5-4a98-41ca-a180-f7eb6be18a20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1:25AM UTC on Mar 13, 2023 --- read and cache time: 0.3010730743408203 seconds ---\n",
      "397\n",
      "length actIDs: 125\n",
      "elapsed time: 1.6015713214874268\n",
      "8f18426d-d87e-479f-802d-d6f49a34a1e1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no birth event for 17 events.\n",
      "elapsed time 2: 253.75643801689148\n"
     ]
    }
   ],
   "source": [
    "df_epm = readCheckpoint(size = \"medium\")\n",
    "df_epm = df_epm.limit(50000)\n",
    "df_ohe_test = build_adjaceny(df_epm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4fbe99-bca2-4b75-9823-92eb4bb8b370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order: integer (nullable = false)\n",
      " |-- parent_objectid: string (nullable = true)\n",
      " |-- parent_timestamp: string (nullable = true)\n",
      " |-- parent_object: string (nullable = true)\n",
      " |-- parent_parent_image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- parent_USER: string (nullable = true)\n",
      " |-- child_objectid: string (nullable = true)\n",
      " |-- child_timestamp: string (nullable = true)\n",
      " |-- child_object: string (nullable = true)\n",
      " |-- child_parent_image_path: string (nullable = true)\n",
      " |-- child_image_path: string (nullable = true)\n",
      " |-- child_USER: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohe_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "671620c1-a669-40a6-81da-8725aaf7abeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    child_image_path|\n",
      "+--------------------+\n",
      "|\\Device\\HarddiskV...|\n",
      "|              System|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\\\?\\C:\\Program Fi...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|\\Device\\HarddiskV...|\n",
      "|            PING.EXE|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ohe_test.select(col(\"child_image_path\") == \"PING.EXE\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7af478e6-e059-446b-91fc-6608630b42dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ohe_test.select(col(\"child_image_path\") == \"PING.EXE\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa06a9f4-81d7-4cec-9a93-56cea1ad3703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|parent_object| p_obj_sparse|\n",
      "+-------------+-------------+\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "|      PROCESS|(1,[0],[1.0])|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "#turn into numeric index before encoding\n",
    "indexer = StringIndexer(inputCol='parent_object', outputCol='p_obj_numeric')\n",
    "indexer_fitted = indexer.fit(df_ohe_test)\n",
    "df_indexed = indexer_fitted.transform(df_ohe_test)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['p_obj_numeric'], outputCols=['p_obj_sparse'],dropLast=False)\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_onehot.select('parent_object','p_obj_sparse').limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c96770-f194-4ed1-9f14-3a6d33556511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order: integer (nullable = false)\n",
      " |-- parent_objectid: string (nullable = true)\n",
      " |-- parent_timestamp: string (nullable = true)\n",
      " |-- parent_object: string (nullable = true)\n",
      " |-- parent_parent_image_path: string (nullable = true)\n",
      " |-- parent_image_path: string (nullable = true)\n",
      " |-- parent_USER: string (nullable = true)\n",
      " |-- child_objectid: string (nullable = true)\n",
      " |-- child_timestamp: string (nullable = true)\n",
      " |-- child_object: string (nullable = true)\n",
      " |-- child_parent_image_path: string (nullable = true)\n",
      " |-- child_image_path: string (nullable = true)\n",
      " |-- child_USER: string (nullable = true)\n",
      " |-- p_obj_numeric: double (nullable = false)\n",
      " |-- p_obj_sparse: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_onehot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f9f726-4ddc-46c1-ab08-fc7140ba6130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "| p_obj_sparse| c_obj_sparse|\n",
      "+-------------+-------------+\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[1],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[1],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[1],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "|(1,[0],[1.0])|(2,[0],[1.0])|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create sparse vecotrs for child objects\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "#turn into numeric index before encoding\n",
    "indexer = StringIndexer(inputCol='child_object', outputCol='c_obj_numeric')\n",
    "indexer_fitted = indexer.fit(df_onehot)\n",
    "df_indexed = indexer_fitted.transform(df_onehot)\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['c_obj_numeric'], outputCols=['c_obj_sparse'],dropLast=False)\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_onehot = df_onehot.drop('c_obj_numeric', 'p_obj_numeric')\n",
    "df_onehot.select('p_obj_sparse','c_obj_sparse').limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26299d84-227f-4f49-9306-725f0fd538e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#combine sparse vectors and turn into string\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Use VectorAssembler to concatenate the two sparse vectors\n",
    "assembler = VectorAssembler(inputCols=['p_obj_sparse', 'c_obj_sparse'], outputCol=\"comb_sparse\")\n",
    "df_onehot_comb = assembler.transform(df_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e34d7b2-b38c-4a51-a332-5e8a63f8bc82",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  comb_sparse|\n",
      "+-------------+\n",
      "|[1.0,1.0,0.0]|\n",
      "|[1.0,1.0,0.0]|\n",
      "|[1.0,0.0,1.0]|\n",
      "|[1.0,1.0,0.0]|\n",
      "|[1.0,1.0,0.0]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_onehot_comb.select('comb_sparse').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38bc2ce3-b4be-42f6-8b42-f10b1c14cf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a timestamp timediff, bucketize, and then create a one hot vector, and then combine all three\n",
    "#question: does order matter\n",
    "\n",
    "df_onehot_comb = df_onehot_comb.withColumn('from_timestamp',to_timestamp(col('parent_timestamp')))\\\n",
    "  .withColumn('end_timestamp', to_timestamp(col('child_timestamp')))\\\n",
    "  .withColumn('DiffInSeconds',col(\"end_timestamp\").cast(\"long\") - col('from_timestamp').cast(\"long\")) \n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0fe0bd-18dc-48ee-9336-0a30220396ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use QuantileDiscretizer to calculate the bin boundaries\n",
    "quantile_discretizer = QuantileDiscretizer(numBuckets=5, inputCol=\"DiffInSeconds\", outputCol=\"binned_diff\")\n",
    "df_onehot_comb = quantile_discretizer.fit(df_onehot_comb).transform(df_onehot_comb)\n",
    "\n",
    "# Convert the binned column to a sparse vector -> may want to consider a different method for this.\n",
    "sparse_vectors = df_onehot_comb.rdd.map(lambda x: (x[0], Vectors.sparse(5, {x[2]: 1.0})))\n",
    "sparse_df = spark.createDataFrame(sparse_vectors, [\"id\", \"sparse_vector\"])\n",
    "\n",
    "# Show the resulting sparse vectors\n",
    "sparse_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
